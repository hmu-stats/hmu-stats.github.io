
<!----------------->
<!--- New slide --->
<!----------------->
## Von der Wahrscheinlichkeit zur Wahrscheinlichkeitsdichte


Zunächst ein Hinweis zur Nomenklatur: 

::: {.definition}
<!---  Definition--->
|||
|:-:|-|
|||
| ![](images/definition.svg){height=70px} | Wir verwenden den Begriff **relative Häufigkeiten** bei empirischen Daten und meinen damit den Anteil einer Merkmalsausprägung relativ zu allen Datenpunkten. Beispiel: in einer Stichprobe von 100 Würfelversuchen lag die relative Häufigkeit von Zahlen größer 3 bei $0.48$ oder $48 \%$. | 
|||
: {tbl-colwidths="[10, 90]"}
:::

::: {.definition}
<!---  Definition--->
|||
|:-:|-|
|||
| ![](images/definition.svg){height=70px} | Wir verwenden den Begriff **Wahrscheinlichkeit**, wenn die theoretische Häufigkeitsverteilung eines Merkmals bekannt ist, und meinen damit den Anteil einer Merkmalsausprägung laut Theorie. Beispiel: bei einem perfekten Würfel ist die Wahrscheinlichkeit einer Zahl größer 3 exakt $0.5$. | 
|||
: {tbl-colwidths="[10, 90]"}
:::

- Im Kontext von theoretischen Häufigkeitsverteilungen können wir daher von **Wahrscheinlichkeiten** sprechen.
- Klar ist auch: durch die Nomenklaturänderung *relative Häufigkeit* &rarr; *Wahrscheinlichkeit* ist noch nichts gewonnen.

## Von der Wahrscheinlichkeit zur Wahrscheinlichkeitsdichte

<div class="vspace-large"></div>

Der entscheidende Trick theoretischer Häufigkeitsverteilungen ist der **Übergang von Wahrscheinlichkeiten zu Wahrscheinlichkeitsdichten**. 

::: {.colorbox .fragment}
Ist das Merkmal $X$ eine kontinuierliche Variable (z.B. Nasenlänge in $cm$), so geben theoretische Häufigkeitsverteilungen $f(x)$ eine [**Wahrscheinlichkeitsdichte**]{color="navy"} an.
:::

<div class="vspace-large"></div>

Wie kann man sich "Wahrscheinlichkeitsdichte" vorstellen?
    
- Wir kennen das Konzept der "Dichte" bei Stoffen: z.B. ist die Dichte von Eis ist ca. $1\stackrel{g}{}\!\!\unicode{x2215}_{\!\unicode{x202f}cm^3}$, d.h. dass sich eine Masse von $1g$ in einem Kubikzentimeter ($1cm^3$) befindet.
- Eine Dichte ist also immer eine bestimmte Masse *pro* Maßeinheit.

## Von der Wahrscheinlichkeit zur Wahrscheinlichkeitsdichte

<div class="vspace-large"></div>

Wir können daher Wahrscheinlichkeitsdichte wie folgt definieren:

::: {.definition .fragment}
<!---  Definition--->
|||
|:-:|-|
|||
| ![](images/definition.svg){height=70px} | $\text{Wahrscheinlichkeitsdichte} = \text{Wahrscheinlichkeits(masse) }pro\text{ Maßeinheit}$ | 
|||
: {tbl-colwidths="[10, 90]"}
:::

<div class="vspace-large"></div>

- In Abgrenzung zur Wahrscheinlichkeits*dichte* wird die Wahrscheinlichkeit selbst tatsächlich auch als Wahrscheinlichkeits*masse* bezeichnet (engl. *probability mass*).
    - Jedoch ist Wahrscheinlichkeit bzw. Wahrscheinlichkeitsmasse im Gegensatz zur physikalischen Masse einheitslos.
- Die Einheit der Wahrscheinlichkeitsdichte wiederum ist Wahrscheinlichkeit pro Maßeinheit: Wahrscheinlichkeit *pro* Zentimeter Nasenlänge, Wahrscheinlichkeit *pro* IQ-Punkt, Wahrscheinlichkeit *pro* Fragebogenpunkt.




<!----------------->
<!--- New slide --->
<!----------------->
## Wahrscheinlichkeitsdichte

- Theoretische Häufigkeitsverteilungen $f(x)$ für kontinulierliche Merkmale $X$ werden auch als [**Wahrscheinlichkeitsdichtefunktion**]{color="navy"} bezeichnet (engl. *probability density function*).
- Wie bei Histogrammen mit relativen Häufigkeiten ist die *gesamte Wahrscheinlichkeitsmasse* von Wahrscheinlichkeitsdichtefunktionen $f(x)$ immer 1. 
- Anders gesagt: Wahrscheinlichkeitsdichtefunktion $f(x)$ sind immer so normalisiert, dass ihr Flächeninhalt den Wert 1 hat.

Beispiel Normalverteilung:
$$
f(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}
$$


<div class="vspace-medium"></div>

:::: {.columns}
::: {.column width="65%"}
Der Normalisierungsfaktor $\frac{1}{\sigma\sqrt{2\pi}}$ sorgt in diesem Fall dafür, dass die Fläche unter der Normalverteilung gleich 1 ist:
$$
\int_{-\infty}^{\infty} f(x) dx = 1
$$
:::
::: {.column width="35%"}
:::{.fragment}
![](images/integral_of_pdf_is_1.png){height=222px style="margin-top:-40px !important"}
:::
:::
::::

<!-- ```{python}

``` -->
<!-- ```{python}
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import norm

np.random.seed(0)

fontsize = 12

mu = 5
std = 1.5
x = np.linspace(0, 10, 100)
y = norm.pdf(x, mu, std)

plt.figure(figsize=(4, 2.5))
plt.plot(x, y, color='#783c00', lw=2)
plt.fill_between(x,y, where= (2 < x)&(x < 4), color="b", alpha=0.2)
plt.annotate('Wahrscheinlichkeit,\n dass sich das\n Merkmal im Bereich\n [2cm;4cm] befindet', xy=(3, 0.05), xytext=(7.5,0.12), arrowprops=dict(arrowstyle='->'))
plt.xlabel('Nasenlänge (cm)', fontsize=fontsize)
plt.ylabel('Wahrscheinlichkeitsdichte', fontsize=fontsize)
plt.xticks(bins[::2], fontsize=fontsize-2)
plt.yticks(fontsize=fontsize-2)
plt.ylim(0, 0.28)
plt.xlim(-0.5, 15)
plt.savefig('images/density_integral.png', bbox_inches='tight')
``` -->


<!----------------->
<!--- New slide --->
<!----------------->
## [Von der Wahrscheinlichkeitsdichte zurück zur Wahrscheinlichkeit]{style="font-size: 41px"}


<!-- Analogie Dichte von Eis: bei eine Dichte von Eis $0.918\frac{g}{cm^3}$ können wir ausrechnen, dass sich in einem 10x10x10cm Eiswürfel $918g$ Eis befinden: $Gewicht = Volumen \cdot Dichte = 1000cm^3 \cdot 0.918\frac{g}{cm^3}=918g$ -->

<div class="vspace-small"></div>

:::: {.columns .fragment}
::: {.column width="62%"}
:::{.nonincremental}
- Um aus einer Wahrscheinlichkeits*dichte* eine Wahrscheinlichkeit zu erhalten, muss die Dichte über einen bestimmten **Wertebereich $[x_0; x_1]$** des Merkmals summiert (integriert) werden.
- Mathematisch beschreiben wir diese Operation als ein Integral:
:::

$$
P(x_0<x<x_1) = \int_{x_0}^{x_1} f(x) dx
$$

- $P$ ist die Wahrscheinlichkeit, dass das Merkmal einen Wert zwischen $x_0$ (Untergrenze) und $x_1$ (Obergrenze) aufweist.
- Das Integral setzt die *Wahrscheinlichkeitsdichte* $f(x)$ mit der *Wahrscheinlichkeit* $P(x_0<x<x_1)$ in Verbindung.
:::
::: {.column width="38%"}
<div class="vspace-large"></div>
![Berechnung einer Wahrscheinlichkeit $P$ auf Basis einer Wahrscheinlichkeitsdichtefunktion $f(x)$ (hier der Normalverteilung).](images/density_integral.png)
:::
::::    

<!----------------->
<!--- New slide --->
<!----------------->
## Wahrscheinlichkeitsdichte: Beispiel 1

:::: {.columns}
::: {.column width="63%"}

Nehmen wir an, dass Nasenlängen in der Population *normalverteilt* sind, mit Mittelwert $\mu=5$ und Standardabweichung $\sigma=1.5$. 

**Frage:** wie hoch ist die Wahrscheinlichkeit, dass eine zufällig gezogene Nase aus der Population eine Länge zwischen $2cm$ und $4cm$ hat?
:::
::: {.column width="37%"}
![](images/density_integral.png)
:::
::::



:::{.fragment}
$$
P(2\le x\le 4) = \int_2^4 f(x)dx = \frac{1}{\sigma\sqrt{2\pi}}\int_2^4\text{exp}\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)dx = \\
= \frac{1}{1.5\sqrt{2\pi}}\int_2^4\text{exp}\left(-\frac{(x-5)^2}{2\cdot1.5^2}\right)dx \overset{(Computer!)}{\approx} 0.23
$$
:::

<!----------------->
<!--- New slide --->
<!----------------->
<!-- ## Theoretische Wahrscheinlichkeitsverteilung
- Je nach Forschungsfrage kann das Ziel einer Stichprobe sein
    1. Rückschlüsse auf die Population zu ziehen (wie gerade gesehen), oder
    2. Rückschlüsse auf die [**theoretische Wahrscheinlichkeitsverteilung**]{color="navy"} zu ziehen.
- Die theoretische Wahrscheinlichkeitsverteilung ist von Interesse, wenn wir die Ursachen oder Mechanismen eines psychologischen Phänomens beschreiben und verstehen wollen.
    - In diesem Fall ist die spezifische Population nicht das primäre Interesse &mdash; selbst die Population ist im Prinzip nur eine (großen) Stichprobe aus der Wahrscheinlichkeitsverteilung.
    - Stattdessen wollen wir mechanistisch und quantitativ verstehen, wie naturwissenschaftliche Prozesse (biologisch, psychisch, sozial) bestimmte psychologische Phänomene "generieren".
- In der Psychologie ist das primäre Interesse häufig der Mechanismus und damit die theoretische Wahrscheinlichkeitsverteilung eines Merkmals; in der Politologie, Soziologie, Ökonomie ist dagegen häufig die Verteilung in der Population relevant (z.B. "Sonntagsfrage").
- Auch wenn diese Unterscheidung konzeptionell wichtig ist, ist sie in der Praxis häufig nicht relevant, da Populationen i.d.R. so groß sind, dass sie nahezu perfekt mit der theoretischen Wahrscheinlichkeitsverteilung übereinstimmen. -->



<!-- ```{python}

``` -->
<!-- ```{python}
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import norm

np.random.seed(0)

fontsize = 12

mu = 5
std = 1.5
x = np.linspace(0, 10, 100)
y = 0.1*np.ones(100)

plt.figure(figsize=(4, 2.5))
plt.plot(x, y, color='#783c00', lw=2)
plt.plot([0, 0], [0, 0.1], color='#783c00', lw=2)
plt.plot([10, 10], [0, 0.1], color='#783c00', lw=2)
plt.fill_between(x,y, where= (2 < x)&(x < 4), color="b", alpha=0.2)
plt.annotate('Wahrscheinlichkeit,\n dass sich das\n Merkmal im Bereich\n [2cm;4cm] befindet', xy=(3, 0.05), xytext=(7.5,0.16), arrowprops=dict(arrowstyle='->'))
plt.xlabel('Nasenlänge (cm)', fontsize=fontsize)
plt.ylabel('Wahrscheinlichkeitsdichte', fontsize=fontsize)
plt.xticks(range(0, 11,), fontsize=fontsize-2)
plt.yticks(fontsize=fontsize-2)
plt.ylim(0, 0.28)
plt.xlim(0, 15)
plt.savefig('images/density_integral_uniform.png', bbox_inches='tight')
``` -->

<!----------------->
<!--- New slide --->
<!----------------->
## Wahrscheinlichkeitsdichte: Beispiel 2


:::: {.columns}
::: {.column width="63%"}

Nehmen wir nun an, dass Nasenlängen in der Population *uniform zwischen 0 und 10 cm verteilt* sind.

**Gleiche Frage:** wie hoch ist die Wahrscheinlichkeit, dass eine zufällig gezogene Nase aus der Population eine Länge zwischen $2cm$ und $4cm$ hat?
:::
::: {.column width="37%"}
![](images/density_integral_uniform.png)
:::
::::

Wir wissen: die Fläche unter der Verteilung muss 1 sein. Daher muss die Wahrscheinlichkeitsdichte für jeden Wert zwischen $0cm$ und $10cm$ gleich $0.1cm^{-1}$ betragen ($10cm\cdot 0.1cm^{-1} = 1$).

Die Berechnung des Flächeninhalts im Intervall $[2cm; 4cm]$ geht in diesem Fall ohne Integration, denn er entspricht einfach der Fläche eines Rechteckes mit Breite $2cm$ und Höhe $0.1cm^{-1}$. Es gilt:

$$
\begin{aligned}
Wahrscheinlichkeit&=Intervallbreite \cdot Wahrscheinlichkeitsdichte =\\
&= 2cm \cdot 0.1cm^{-1} = 0.2
\end{aligned}
$$

