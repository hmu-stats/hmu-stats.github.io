---
title: "Vorlesung 05: Zusammenhänge"
---

## {.blackslide .center}

::: {.content-hidden when-format="pdf"}
![](images/paradoxia_group_of_scientists.png){.hcenter-image height=300px}
<!-- Source: Midjourney -->
<div class="vspace-small"></div>
:::

Die bisherige Auswertung der Beobachtungsstudie hat Evidenz sowohl für Hypothese 1 (mehr Zeit auf TikTok) als auch Hypothese 2 (erhöhte Entzündungswerte) erbracht. So einen richtigen Reim können Sie sich noch nicht auf das Ergebnis machen.

Ihre Neugier ist aber geweckt und Sie fragen sich: hängen vielleicht Ihre beiden abhängigen Variablen (TikTok-Zeit & Entzündungswerte) selbst miteinander zusammen? Steigen die Entzündungswerte mit zunehmender Online-Zeit auf der Plattform TikTok? 

<div class="vspace-small"></div>

![](images/paradoxia_zusammenhang.png){height=110px}


<!-- ## Der Forschungsprozess {.hcenter-slide}

```yaml { .animate src="images/scientific_process.svg"}
setup:
    - element: "#results"
      modifier: function() { this.node.style.fill = 'green'; }
    - element: "#resultsbg"
      modifier: function() { this.node.style.fill = '#d8ffe2';}
``` -->

## Was sind Zusammenhänge?

:::: {.columns}
::: {.column width="70%"}
- Ein [**Zusammenhang**]{color="navy"} beschreibt zu welchem Grad zwei Variablen (Merkmale) systematisch miteinander in Verbindung stehen
- Zusammenhänge bilden die Essenz der psychologischen Forschung&mdash;durch sie versuchen wir die Mechanik der menschlichen Psyche zu verstehen:
    - Fördert **Ausdauersport** das **psychische Wohlbefinden**?
    - Helfen **Psychotherapiestunden** bei der Überwindung einer **Depression**?
    - Wirkt sich **Bildschirmzeit** nachteilig auf die **Schlafqualität** aus?
    - Steigt durch **kindliche Frühförderung** die Wahrscheinlichkeit für einen **akadamischen Bildungsabschluss**?
- Arten von Zusammenhängen:
    - Linearer Zusammenhang (Pearson-Korrelation)
    - Rangkorrelation
    - Lineare Regression (Unterscheidung von UV und AV)
    - Kontingenzkoeffizient
<!-- - Fast jede wissenschaftliche Hypothese lässt sich als Zusammenhang formuliern &ndash; selbst Unterschiede!
    - [Formulierung als Unterschied]{.underline}: unterscheiden sich Männer und Frauen in ihren verbalen Fähigkeiten?
    - [Formulierung als Zusammenhang]{.underline}: steht die kategorische Variable **Geschlecht** in Zusammenhang mit der metrischen Variable **verbale Fähigkeiten**? -->
:::
<!-- Begin second column -->
::: {.column width="30%"}
![](images/relationship.png){width=200px}
:::
::::

## Zusammenhänge versus Unterschiede

:::: {.columns}
::: {.column width="56%"}

- **Zusammenhänge:**
    - Zugrunde liegen [zwei Variablen]{.underline} $X$ und $Y$ in [einer Gruppe]{.underline}.
    - Verglichen werden [Einzelwerte]{.underline} der Variablen $X$ und $Y$.
- **Unterschiede:**
    - Zugrunde liegt [eine Variable]{.underline} $X$ in [zwei Gruppen/Bedingungen]{.underline} &rArr; $X_A$, $X_B$.
    - Verglichen werden [statistische Maße]{.underline}, die für jede Gruppe/Bedingung auf Basis der Variable berechnet wurden (z.B. Mittelwerte $\bar{x}_A \text{ vs. } \bar{x}_B$).

:::
::: {.column width="44%"}

![](images/zusammenhaenge_unterschiede.png)

:::
::::

- Aber: häufig lassen sich **Unterschiedshypothesen** in **Zusammenhangshypothesen** überführen:
    - Unterscheidet sich die akademische Leistung von Rauchern und Nichtrauchern?<br>&rArr; Zusammenhang **Zahl der Zigaretten pro Tag** und **akademische Leistung**
    - Unterscheidet sich Medienkonsum von Depressiven und Kontrollen?<br>&rArr; Zusammenhang **Depressivität** und **Medienkonsum**
- Zusammenhangshypothesen sind häufig das "schärfere statistische Schwert", da eine willkürliche und verlustbehaftete Einteilung einer Variablen in Kategorien vermieden wird.


<!---  Definition--->
<!-- ::: {.definition .fragment}
|||
|:-:|-|
|||
| ![](images/definition.svg){height=70px} | Ein [**Zusammenhang**]{color="navy"} beschreibt zu welchem Grad die **Variation zweier metrischer Variablen** miteinander in Verbindung steht. | 
|||
: {tbl-colwidths="[10, 90]"}
:::

- Durch die Eingrenzung auf **metrische Variablen** (diskret oder kontinuerlich) stellt etwa die Verbindung von Geschlecht und verbalen Fähigkeiten keinen Zusammenhang im engeren Sinn dar, da Geschlecht eine kategorische Variable ist.
- In den meisten Fällen sind Zusammenhänge das "schärfere statistische Schwert" als Unterschiede und es lohnt sich oft, Forschungsfragen entsprechend anzupassen:
    - Unterscheidet sich die akademische Leistung von Rauchern und Nichtrauchern? &rArr; Zusammenhang **Zahl der Zigaretten pro Tag** und **akademische Leistung**
    - Unterscheidet sich der Medienkonsum von Depressiven und Kontrollen? &rArr; Zusammenhang **Depressivität** und **Medienkonsum**
    - Unterscheidet sich das Risiko von Alkoholsucht zwischen Nord- und Süddeutschland? &rArr; Zusammenhang **geographischer Breitengrad** und **Alkoholsuchtrisiko** -->


# Kovarianz

## Kovarianz

- [Ziel:]{.underline} mathematische Größe, die zum Ausdruck bringt, wie stark die Variation zweier Variablen miteinander in Zusammenhang steht.
- Wir haben bereits eine Größe für die Variation *einer* Variable &mdash; die Varianz:

:::{.fragment}
$$
\hat{\sigma}_{\mkern-2mu\scriptscriptstyle{X}}^2 = \frac{1}{n-1}\sum_{i=1}^n\big(x_i-\bar{x}\big)^2 = \frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})\color{darkred}{(x_i-\bar{x})}
$$
:::


- Die Varianz gibt an, wie stark eine Variable $X$ um ihren Mittelwert $\bar{x}$ schwankt.
- Analog berechnet die [**Kovarianz**]{color="navy"}, wie stark die *gemeinsame Schwankung zweier Variablen um ihren jeweiligen Mittelwert* ist:

:::{.fragment}
$$
\text{Kovarianz:}\quad \hat{Cov}(X,Y) = \hat{\sigma}_{\mkern-2mu\scriptscriptstyle{XY}} = \frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})\color{darkred}{(y_i-\bar{y})}
$$
:::

- Im Zentrum der Kovarianz steht die Erkenntnis, dass das **mathematische Produkt** zweier Abweichungsvariablen &mdash; hier die Abweichungen $(x_i-\bar{x})$ und $(y_i-\bar{y})$ vom Mittelwert &mdash; angibt, wie stark die beiden Abweichungen gleichsinnig variieren (ko-variieren).

## Kovarianz {data-fragment-index=0}

![](images/covariance-examples.png){height=370px}

[Intuition für positive Kovarianz]{.underline}:

- Sind zwei zusammengehörige Datenpunkte $x_i$ und $y_i$ **größer als der Mittelwert**, sind sowohl $(x_i-\bar{x})$ als auch $(y_i-\bar{y})$ **positiv**, und damit auch das Produkt $(x_i-\bar{x})(y_i-\bar{y})$ **positiv**.

- Sind zwei zusammengehörige Datenpunkte $x_i$ und $y_i$ **geringer als der Mittelwert**, sind sowohl $(x_i-\bar{x})$ als auch $(y_i-\bar{y})$ **negativ**, und damit das Produkt $(x_i-\bar{x})(y_i-\bar{y})$ wieder **positiv**.

<div class="vspace-small"></div>

:::{.fragment}
Die Intuition für eine negative Kovarianz funktioniert ähnlich.
:::


## Kovarianz: Größe-Gewicht-Beispiel


<div class="vspace-small"></div>

:::: {.columns}
::: {.column width="34%"}
![](images/covariance_height_weight.png)
:::
::: {.column width="66%"}

:::{.fragment}
$$
\scriptsize{
\begin{aligned}
&\hat{Cov}(X, Y) = \frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y}) = \\
          &= \frac{1}{2}\big[(160-170)(60-70)+(170-170)(70-70)+(180-170)(80-70)\big] = \\
          &= \frac{1}{2}\big[(-10)\cdot (-10)+0\cdot 0+10\cdot 10\big] = \\
          &= \frac{1}{2}\cdot 200 = 100[\color{red}{cm\cdot kg}]
\end{aligned}
}
$$
:::

:::
::::

- Problem: die Kovarianz hängt von den Einheiten ab!
- Wird im Beispiel die Körpergröße $X$ in der Einheit *Meter* angegeben, so lautet die Kovarianz:

:::{.fragment}
$$
\scriptsize{
\begin{aligned}
\hat{Cov}(X, Y) &= \frac{1}{2}\big[(1{.}60-1{.}70)(60-70)+(1{.}70-1{.}70)(70-70)+(1{,}80-1{.}70)(80-70)\big] = \\
          &= \frac{1}{2}\big[(-0{.}10)\cdot (-10)+0\cdot 0+0{.}10\cdot 10\big] = \frac{1}{2}\cdot 2 = 1[\color{red}{m\cdot kg}]
\end{aligned}
}
$$
:::

- Kovarianzen sind also **nicht vergleichbar, wenn sich Einheiten unterscheiden**, und erst recht nicht, wenn sich die Variablen unterscheiden.


# Pearson-Korrelation

## Pearson-Korrelation

:::: {.columns}
::: {.column width="70%"}
- Die [**Pearson-Korrelation**]{color="navy"} schafft Abhilfe für das Problem der mangelnden Vergleichbarkeit.
- Der Schlüssel: die Kovarianz wird mit den [Standardabweichungen]{color="darkgreen"}<br>$\color{darkgreen}{\hat{\sigma}_{\mkern-2mu\scriptscriptstyle{X}}}$ und $\color{darkgreen}{\hat{\sigma}_{\mkern-2mu\scriptscriptstyle{Y}}}$ beider Variablen normalisiert.
- Formel der Pearson-Korrelation:

:::{.fragment}
$$
\small{
\hat{\rho} = \frac{Cov(X,Y)}{\color{darkgreen}{\sigma_{\mkern-2mu\scriptscriptstyle{X}}\,\sigma_{\mkern-2mu\scriptscriptstyle{Y}}}} = \cdot\cdot\cdot = \frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\color{darkgreen}{\sqrt{\sum_{i=1}^n(x_i-\bar{x})\sum_{i=1}^n(y_i-\bar{y})}}}
}
$$
:::

:::{style="margin-top: -10px"}
- Durch das Teilen durch die Standardabweichungen $\color{darkgreen}{\sigma_{\mkern-2mu\scriptscriptstyle{X}}}$ und $\color{darkgreen}{\sigma_{\mkern-2mu\scriptscriptstyle{Y}}}$ werden die Einheiten herausgekürzt &mdash; die Korrelation ist also eine **einheitslose Größe**.
:::


:::
<!-- Begin second column -->
::: {.column width="30%"}

![Der britische Mathematiker Karl Pearson in seinem Büro im Jahr 1910. Neben dem Korrelationskoeffizienten verdanken wir Pearson viele andere statistische Konzepte wie die Hauptkomponentenanalyse oder den p-Wert. Später wurden seine Ansichten zu Eugenik kritisch hinterfragt. Bildnachweis^[http://www.learn-stat.com/life-of-karl-pearson/]](images/karl_pearson.png)

:::
::::


:::{style="margin-top: -15px"}
- Die Korrelation kann Werte zwischen $-1$ (perfekter negativer Zusammenhang) und $+1$ (perfekter positiver Zusammenhang) annehmen.
:::

:::{style="margin-top: -15px"}
- [Hinweise]{.underline}:
    - Wir verwenden in der Korrelationsformel Größen ohne Besselkorrektur (d.h. ohne ^).
    - In wissenschaftlichen Publikationen wird der Korrelationskoeffizient $\hat{\rho}$ häufig vereinfacht mit $r$ bezeichnet (z.B. $r=0.32$). Grund: die Formeln von inferentieller und nicht-inferentieller Korrelation sind identisch.
:::



<!-- ```{python}
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import pearsonr, linregress

np.random.seed(0)
data = np.random.multivariate_normal([0, 0], [[1, 0.5], [0.5, 1]], 30) + 3
x, y = data[:, 0]/2, data[:, 1]
fontsize = 15

r, p = pearsonr(x, y)
reg = linregress(x, y)
xrange = np.array([0.1, 2.7])

plt.figure(figsize=(4, 3))
plt.scatter(x, y, s=50, marker='o', color='blue', edgecolors='w')
plt.plot(xrange, reg.slope*xrange+reg.intercept, color='k', lw=2)
plt.xlabel('Schokoladenkonsum [kg/Tag]', fontsize=fontsize)
plt.ylabel('Zufriedenheit', fontsize=fontsize)
plt.xticks(fontsize=fontsize-2)
plt.yticks(fontsize=fontsize-2)
plt.ylim(0, 6.5)
plt.text(0.05, 0.9, fr'$r={r:.3f}$ ($p={p:.4f}$)'.replace('.', '{,}'), color='k', transform=plt.gca().transAxes, fontsize=fontsize-2)
plt.title('Streudiagramm', fontsize=fontsize+2, fontstyle='italic')
plt.savefig('images/streudiagramm.png', bbox_inches="tight")
``` -->

<!----------------->
<!--- New slide --->
<!----------------->
## Das Streudiagramm

- Zusammenhänge zweier Variablen werden häufig in Form eines [**Streudiagramms**]{color="navy"} (engl. *scatter plot*) dargestellt.
- Bei der Korrelation ist es dabei willkürlich, welche Variable auf der x- und y-Achse liegt.
- Häufig wird zusätzlich zur "Punktwolke" auch eine **Regressionsgerade** dargestellt, sowie die Stärke des Zusammenhangs (r=.., p=..).

:::{.fragment}
![](images/streudiagramm.png){height=400px}
:::


## Pearson-Korrelation: Größe-Gewicht-Beispiel

<div class="vspace-small"></div>

:::: {.columns}
::: {.column width="39%"}
![](images/covariance_height_weight.png)
:::
::: {.column width="61%"}
Kovarianz (ohne Besselkorrektur):

$$
\small{
\begin{aligned}
Cov(X, Y) = \frac{1}{n}\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y}) \overset{\text{(prüfe nach)}}{=} \frac{200}{3}[\color{red}{cm\cdot kg}]
\end{aligned}
}
$$

:::{.fragment}
Und berechnen nun die Korrelation $\;\;\hat{\rho} = \frac{Cov(X,Y)}{\sigma_{\mkern-2mu\scriptscriptstyle{X}} \sigma_{\mkern-2mu\scriptscriptstyle{Y}}}$

$\scriptsize{\sigma_{\mkern-2mu\scriptscriptstyle{X}}=\sqrt{\frac{1}{n}\sum_{i=1}^n\big(x_i-\bar{x}\big)^2}=\sqrt{\frac{1}{3}[(-10)^2+0^2+10^2]}=\sqrt{\frac{200}{3}}[\color{red}{cm}]}$

$\scriptsize{\sigma_{\mkern-2mu\scriptscriptstyle{Y}}=\sqrt{\frac{1}{n}\sum_{i=1}^n\big(y_i-\bar{y}\big)^2}=\sqrt{\frac{1}{3}[(-10)^2+0^2+10^2]}=\sqrt{\frac{200}{3}}[\color{red}{kg}]}$

$$
\small{
\hat{\rho} = \frac{\frac{200}{3}[\color{red}{cm\cdot kg}]}{\sqrt{\frac{200}{3}}[\color{red}{cm}]\cdot\sqrt{\frac{200}{3}}[\color{red}{kg}]}=1
}
$$
:::

:::
::::

- Die Einheiten kürzen sich beim Korrelationskoeffizienten $\hat{\rho}$ also raus! Wird die Körpergröße statt in der Einheit *Zentimeter* etwa in *Meter* angeben, bleibt der Korrelationskoeffizient $\hat{\rho}$ unverändert.

:::{.fragment}
**Die Normalisierung mit der Standardabweichung sorgt dafür, dass die Korrelation unabhängig von der Einheit ist!**
:::


<!----------------->
<!--- New slide --->
<!----------------->
## Interpretation der Pearson-Korrelation

- Die Pearson-Korrelation zeigt an, **wie linear** der Zusammenhang zweier Variablen ausgeprägt ist.
- Die Pearson-Korrelation ist dabei [nicht]{.underline} von der Steigung einer gedachten Gerade abhängig.

:::{.fragment}
![Korrelationskoeffizienten für verschiedene hypothetische Streudiagramme](images/correlation_examples.png){width=1030px}
:::

- Ein Zusammenhang zweier Variablen kann extrem schwach sein (z.B. so, dass eine Verdopplung von X nur einer 0.1%-Steigerung von Y entspricht) und dennoch kann die Korrelation stark sein (= nah an $\pm 1$), wenn die Punkte exakt auf einer Geraden liegen.
- Auf einen Satz gemünzt kann man sagen: 

::: {.fragment style="font-size: 26px; margin-top: -10px"}
> Die Pearson-Korrelation misst, wie gut bivariate Daten durch eine Gerade abgebildet werden können.
:::


<!----------------->
<!--- New slide --->
<!----------------->
:::{.content-hidden when-format="pdf"}
## [https://www.guessthecorrelation.com/]{style="font-size: 37px"}

<iframe width=100% height="100%" src="https://www.guessthecorrelation.com/"></iframe>
:::

## Voraussetzungen für das Berechnen der Pearson-Korrelation

- Die Pearson-Korrelation *kann* *immer* berechnet werden, solange beide Variablen aus Zahlenwerten bestehen.
- Es gibt jedoch weitere Kriterien, die für die Sinnhaftigkeit und Interpretierbarkeit der Pearson-Korrelation wichtig sind:

:::{.fragment}
<!---  Table --->
|Kriterium|Falls Kriterium nicht erfüllt? | Beispiel | Mögliche Abhilfe? |
|-|-|-|-|
| Daten haben mindestens Intervallskalenniveau | ▪&numsp;Keine Aussage über Linearität des untersuchten Zusammenhangs möglich \
▪&numsp;Korrelation nicht interpretierbar | ![](images/example_ordinal.png) | Rangkorrelation |
| Keine Ausreißer | Korrelationskoeffizient kann massiv verzerrt sein | ![](images/outlier_example.png) | Ausreißer entfernen oder Rangkorrelation | 
| Zusammenhang der Daten wird *nicht* durch *nicht-linearen* Anteil dominiert | ▪&numsp;Pearson-Korrelation falsches Modell \
▪&numsp;Linearität der Daten wird verzerrt wiedergegeben, da die Korrelation vom nicht-linearen Teil beeinflusst wird  | ![](images/nonlinear.png) | Komplexeres Modell, das den nicht-linearen Anteil berücksichtigt |

: {tbl-colwidths="[24, 36, 20, 20]"}
:::

## Mythen zur Pearson-Korrelation

- In vielen Quellen finden sich darüber hinaus **unzutreffende Behauptungen** zur Pearson-Korrelation:

:::{.fragment style="margin-top: 5px"}
<!---  Table --->
||Behauptung| Fact |
|-|-|-|
| **Mythos 1** | Die Variablen müssen kontinuierlich sein | **Pearson-Korrelation ist valide für diskrete Daten**, solange diese mindestens Intervallskalenniveau aufweisen. Tatsächlich gibt es sogar eine Variante der Pearson-Korrelation, bei der beide Variablen binär sind (Phi-Koeffizient). |
| **Mythos 2** | Die Variablen müssen einen linearen Zusammenhang aufweisen | Gegenbeispiel: wenn Daten aus zufälligem Rauschen basieren, sind sie mit Sicherheit nicht linear verbunden &mdash; und dennoch gibt der Pearson-Koeffizient korrekterweise an, dass die Korrelation ungefähr 0 ist. Zusammenhänge in der Psychologie sind *sehr selten* eindeutig linear, dennoch kann es sinnvoll sein, die Pearson-Korrelation anzuwenden. Besser ist daher zu sagen (s. vorherige Folie), dass der Zusammenhang **nicht zu stark durch einen nicht-linearen Anteil dominiert** werden sollten. |
| **Mythos 3** | Die beiden Variablen müssen normalverteilt sein. | Der **Pearson-Korrelationskoeffizient per se erfordert keine Normalverteilung** der Variablen. [Korrekt ist aber, dass die Daten für die **Berechnung eines p-Wertes auf Basis des t-Tests annähernd normalverteilt** (ganz korrekt: *bivariat normalverteilt*) sein sollten. Wenn Normalverteilung nicht gegeben ist, können andere Signifikanztests (Permutation, Bootstrap) verwendet werden.]{color="darkred"}
| [**Mythos 4**]{color="darkred"} | [Die Variablen müssen varianzhomogen sein]{color="darkred"} | [Varianzhomogenität (auch Homoskedastizität) meint, dass Y-Werte ähnliche Varianz in verschiedenen Abschnitten der X-Achse haben und umgekehrt. Hier gilt das gleiche wie bei Mythos 3: **Varianzhomogenität ist keine Voraussetzung für die Anwendung der Pearson-Korrelation per se, wohl aber für die Anwendung des t-Tests.**]{color="darkred"}

: {tbl-colwidths="[10, 18, 72]"}
:::


<!-- ```{python}
from string import ascii_letters
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

sns.set_theme(style="white")
fontsize = 36

data = [
    [1, 0.13, 0.35, 0.23, -0.2],
    [0, 1, 0.37, 0.1, -0.4],
    [0, 0, 1, 0.01, -0.26],
    [0, 0, 0, 1, -0.1],
    [0, 0, 0, 0, 1]
]

annot = [
    ['1', '0.13', '0.35', '0.23', '-0.2'],
    ['', '1', '0.37', '0.1', '-0.4'],
    ['', '', '1', '0.01', '-0.26'],
    ['', '', '', '1', '-0.1'],
    ['', '', '', '', '1']
]

columns = ['O', 'C', 'E', 'A', 'n']

corr = pd.DataFrame(data, columns=columns, index=pd.Index(columns))

# Generate a mask for the upper triangle
# mask = np.tril(np.ones_like(corr, dtype=bool))
mask = np.zeros_like(corr, dtype=bool)
np.fill_diagonal(mask, True)

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(230, 20, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
hm = sns.heatmap(corr, annot=annot, fmt='', mask=mask, cmap=cmap, vmax=.3, center=0,
                 square=True, linewidths=.5, cbar_kws=dict(shrink=1, label='Korrelation'), annot_kws=dict(size=fontsize))
plt.gca().collections[0].colorbar.ax.tick_params(labelsize=28)
hm.figure.axes[-1].set_ylabel('Korrelation', size=fontsize)
hm.tick_params(labelsize=fontsize)
plt.yticks(rotation=0) 
hm.patch.set_facecolor('#777')
plt.tight_layout()
plt.savefig(f'images/corrmatrix.png', bbox_inches="tight")
```-->


<!----------------->
<!--- New slide --->
<!----------------->
## Korrelationsmatrix

- Eine **Korrelation** bestimmt immer den Zusammenhang zwischen **zwei Variablen**.
- Gibt es mehr als zwei Variablen (z.B. die "Big Five"), bietet sich eine Darstellung aller paarweisen Korrelationen an &ndash; die [**Korrelationsmatrix**]{color="navy"}.

:::: {.columns .vcenter-column style="margin-top:-15px"}
::: {.column width="60%"}
:::{.fragment}
![Tabellarische Korrelationsmatrix. Sterne kennzeichnen häufig das Signifikanzniveau.](images/corrmatrix_table.png)
:::
:::
::: {.column width="40%"}
:::{.fragment}
![Korrelationsmatrix als "Heatmap" &ndash; die Einfärbung ist ein visuelles Hilfsmittel zur intuitiven und schnellen Erfassung der Korrelationsstruktur von mehreren Variablen.](images/corrmatrix.png)
:::
:::
::::
- Die [Nebendiagonalelemente]{color="darkgreen"} sind der interessante Teil der Korrelationsmatrix, sie geben die Korrelationen verschiedener Variablen an.
- Die [Diagonalelemente]{color="darkred"}, also die Korrelationen von Variablen mit sich selbst, sind immer 1.



<!----------------->
<!--- New slide --->
<!----------------->
## Kovarianzmatrix

:::: {.columns}
::: {.column width="75%"}
- Eine analoge Matrix-Darstellung gibt es auch für die **Kovarianz**.
- Im Unterschied zur Korrelationsmatrix sind die Diagonalelemente der Kovarianzmatrix nicht 1, sondern geben die **Varianz** der Variable an.
- Sei $\mathbf{X}$ (beachte Fettschrift) ein Vektor von $n$ Variablen $X_1 .. X_n$, so ist die zugehörige Kovarianzematrix $\hat{Cov}(\mathbf{X})$:

:::{.fragment}
$$
\small{
\begin{aligned}
\hat{Cov}(\mathbf{X}) & = 
\begin{pmatrix}\hat{Var}(X_1) & \hat{Cov}(X_1,X_2) & \cdots & \hat{Cov}(X_1,X_n) \\ \\
 \hat{Cov}(X_2,X_1)  & \hat{Var}(X_2) & \cdots & \hat{Cov}(X_2,X_n) \\ \\
 \vdots & \vdots & \ddots & \vdots \\ \\
\hat{Cov}(X_n,X_1) & \hat{Cov}(X_n,X_2) & \cdots & \hat{Var}(X_n)
\end{pmatrix}
\end{aligned}
}
$$
:::

:::
::: {.column width="25%"}
:::{.fragment}
![](images/covariance_matrix.png)
:::
:::
::::

- Im Gegensatz zur Korrelationsmatrix ist die Kovarianzmatrix selten das "Endprodukt" einer Analyse, sondern meist ein Zwischenschritt in fortgeschritteneren statistischen Analysen wie der **Hauptkomponentenanalyse**.
