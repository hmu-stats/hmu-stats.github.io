# Rangkorrelationen

## Was ist eine Rangkorrelation?

:::: {.columns}
::: {.column width="72%"}

- Eine Alternative zur Pearson-Korrelation sind [**Rangkorrelationsmaße**]{color="navy"}, die nicht die Linearität eines Zusammenhangs, sondern die **Monotonie des Zusammenhangs** bemessen.
- Rangkorrelationen werden in der Regel aus zwei Gründen angewendet:
    - **Theoretischer Grund:** Man ist tatsächlich an der Monotonie &mdash; und nicht der Linearität &mdash; eines Zusammenhangs interessiert. Dies ist häufig der Fall, wenn die Daten tatsächlich als ordinalskalierte Ränge &ndash; d.h. ohne interpretierbare Abstände &ndash; vorliegen.
    - **Praktischer Grund:** Eigentlich ist die Linearität das primäre Interesse, aber es sind entweder die Annahmen der Pearson-Korrelation verletzt (Daten nicht intervallskaliert) oder die Pearson-Korrelation ist aus anderen Gründen ungeeignet (Ausreißer). Die Rangkorrelation ist die "Notfalloption".
- Für Rangkorrelationsmaße spielt lediglich die Reihenfolge ("Rang") der Daten eine Rolle und nicht die spezifischen Werte.
    - Ähnlich wie beim Median ist es genau diese Eigenschaft, die Rangkorrelationsmaße unanfällig für Ausreißer macht.
<!-- - Im Folgenden werden zwei die in der Praxis häufigsten Rangkorrelationsmaße behandelt:
    - [**Spearman-Korrelation**]{color="navy"}
    - [**Kendall'sches Tau**]{color="navy"} -->
:::
::: {.column width="28%"}
![](images/rankorder_example.png)

<div class="vspace-medium"></div>

![](images/rankorder_example2.png)
:::
::::


<!----------------->
<!--- New slide --->
<!----------------->
## Spearman-Korrelation
- Das mutmaßlich häufigste Rangkorrelationsmaß ist die [**Spearman-Korrelation** $\hat{\rho}_s$]{color='navy'}. 
- Die Spearman-Korrelation ist identisch zur Pearson-Korrelation, wenn die Variablen $X$ und $Y$ als Ränge $R(X)$ und $R(Y)$ vorliegen:

:::{.fragment}
$$
\hat{\rho}_s = \frac{Cov(R(X),R(Y))}{\sigma_{\mkern-2mu\scriptscriptstyle{R(X)}} \sigma_{\mkern-2mu\scriptscriptstyle{R(Y)}}}
$$
&numsp;&numsp;wobei $\sigma_{\mkern-2mu\scriptscriptstyle{R(X)}}$ und $\sigma_{\mkern-2mu\scriptscriptstyle{R(Y)}}$ die Standardabweichungen der Ränge von $X$ und $Y$ sind.
:::

:::: {.columns}
::: {.column width="60%"}
- Wie die Pearson-Korrelation nimmt die Spearman-Korrelation Werte zwischen $-1$ und $+1$ an:
    - Ein positiver Wert impliziert eine positiven monotonen Zusammenhang
    - Ein negativer Wert impliziert eine negativen monotonen Zusammenhang
    - Ein Wert nahe bei 0 impliziert einen schwachen (oder keinen) monotonen Zusammenhang
:::
::: {.column width="40%"}
:::{.fragment}
![](images/monotonie.png)
:::
:::
::::



## Berechnung von Rängen

- Liegen die Variablen $X$ oder $Y$ nicht als Ränge vor, müssen sie zunächst in Ränge $R(X)$ bzw. $R(Y)$ umgewandelt werden:

:::{style="font-size: 23px; margin-left:40px; margin-top: -12px"}
1. Werte der Variable sortieren
2. (Unnormierte) Ränge zuordnen
3. Normierung: Gleiche Werte erhalten den Mittelwert ihrer Ränge
4. (Optional) Variablen in ihre ursprüngliche Reihenfolge bringen
:::

:::{.fragment}
![](images/rangbildung.png){height=333px}
:::

- Die Tabelle gibt die Rangberechnung *einer* Variablen an (z.B. X) &mdash; für die andere Variable muss das analoge Prozedere durchgeführt werden.




<!----------------->
<!--- New slide --->
<!----------------->
## Kendalls Tau

:::: {.columns}
::: {.column width="69%"}
- Eine Alternative Rangkorrelation zu Spearman ist [**Kendalls Tau**]{color="navy"} $\hat{\tau}$.
- Kendalls Tau vergleicht inwieweit die Rangfolge *aller* Paare ($x_i$, $x_j$) mit der Rangfolge *aller* Paare ($y_i$, $y_j$) übereinstimmt.
- Dazu wird die Zahl der konkordanten (übereinstimmenden) und diskordanten (nicht übereinstimmenden) Paare gezählt.
:::
::: {.column width="31%"}
<div class="vspace-large"></div>
:::{.fragment}
![](images/Kendalls_Tau_Gleichung.png)
:::
:::
::::


:::: {.columns}
::: {.column width="30%"}

:::{.fragment}
::: {.greybox}
**Beispiel**
:::


$$
\small{
X=\begin{pmatrix} 9\\ 3\\ 7\\ 5 \end{pmatrix}
Y=\begin{pmatrix} 18\\ 7\\ 8\\ 21 \end{pmatrix}
}
$$
:::
:::
::: {.column width="70%"}
- Die Paare von X sind: $\small{(9, 3), (9, 7), (9, 5), (3, 7), (3, 5), (7, 5)}$
- Die Paare von Y sind: $\small{(18, 7), (18, 8), (18, 21), (7, 8), (7, 21), (8, 21)}$
- Die Paare $(x_1=9, x_3=7)$ und $(y_1=18, y_3=8)$ wären **konkordant**, da die Rangfolge des X-Paares ($x_1 > x_3$) gleich der Rangfolge des entsprechenden Y-Paares ($y_1 > y_3$) ist.
- Die Paare$(x_1=9, x_4=5)$ und $(y_1=18, y_4=21)$ wären **diskonkordant**, da die Rangfolge des X-Paares ($x_1 > x_4$) ungleich der Rangfolge des entsprechenden Y-Paares ($y_1 < y_4$) ist.
:::
::::

:::{.fragment}
[Insgesamt gibt es im Beispiel 4 konkordante Paare und 2 diskordante Paare (prüfe nach!), daher gilt:]{style="margin-top:-12px; display: block"}
:::

:::{.fragment}
$$
\hat{\tau} = \frac{K-D}{K+D} = \frac{4-2}{4+2} = \frac{2}{6} = 0.333..
$$
:::



<!-- ```{python}
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import pearsonr, spearmanr, linregress
np.random.seed(13)
fontsize=15

data = np.random.multivariate_normal([0, 0], [[1, 0.1], [0.1, 1]], 20) + 3
data = np.vstack((data, [5, 12]))
x, y = data[:, 0], data[:, 1]

reg = linregress(x, y)
reg2 = linregress(x[:-1], y[:-1])
r, p = pearsonr(x, y)
r2, p2 = pearsonr(x[:-1], y[:-1])
xrange = np.array([1.1, 5.2])

plt.figure(figsize=(11, 4))
plt.subplot(121)
plt.scatter(x[:-1], y[:-1], c='#888')
plt.scatter(x[-1], y[-1], c='#d80000')
plt.plot(xrange, reg.slope*xrange+reg.intercept, color='#d80000')
plt.plot(xrange, reg2.slope*xrange+reg2.intercept, color='#888')
plt.xticks(fontsize=fontsize-2)
plt.yticks(fontsize=fontsize-2)
plt.xlabel('X', fontsize=fontsize)
plt.ylabel('Y', fontsize=fontsize)
plt.text(1, 10.25, r'Pearson (mit Ausreißer): $\hat{\rho} =' + f' {r:.3f}$', fontsize=fontsize-2, color='#d80000')
plt.text(1, 11.4, r'Pearson (ohne Ausreißer): $\hat{\rho} =' + f' {r2:.3f}$', fontsize=fontsize-2, color='#888')
plt.text(4.53, 11, 'Ausreißer', fontsize=fontsize-2, color='#d80000')

xr, yr = data[:, 0].argsort().argsort()+1, data[:, 1].argsort().argsort()+1
print(data[:, 0], data[:, 1])
print(xr, yr)
rr, pr = spearmanr(xr, yr)
rr2, pr2 = spearmanr(xr[:-1], yr[:-1])
regr = linregress(xr, yr)
regr2 = linregress(xr[:-1], yr[:-1])
xrange = np.array([1, 22])

plt.subplot(122)
plt.scatter(xr[:-1], yr[:-1], c='#888')
plt.scatter(xr[-1], yr[-1], c='#d80000')
plt.plot(xrange, regr.slope*xrange+regr.intercept, color='#d80000')
plt.plot(xrange, regr2.slope*xrange+regr2.intercept, color='#888')
plt.xticks(range(0, 21, 5), fontsize=fontsize-2)
plt.yticks(fontsize=fontsize-2)
plt.xlabel('Rang(X)', fontsize=fontsize)
plt.ylabel('Rang(Y)', fontsize=fontsize)
plt.text(0.5, 23.5, r'Spearman (mit Ausreißer): $\hat{\rho}_s =' + f'{rr:.3f}$', fontsize=fontsize-2, color='#d80000')
plt.text(0.5, 26, r'Spearman (ohne Ausreißer): $\hat{\rho}_s =' + f'{rr2:.3f}$', fontsize=fontsize-2, color='#888')
plt.ylim(0, 29)

plt.tight_layout()

plt.savefig('images/pearson_outlier.png', bbox_inches="tight")
``` -->




<!----------------->
<!--- New slide --->
<!----------------->
## Ausreißer: Der Storch bringt die Babys zur Welt(p = 0.008)

:::: {.columns}
::: {.column width="50%"}
![](images/geburtenrate_stoerche.png){height=300px}
:::
::: {.column width="50%"}
![](images/geburtenrate_stoerche_tabelle.png){height=400px}
:::
::::

[http://www3.math.uni-paderborn.de/~agbiehler/sis/sisonline/struktur/jahrgang21-2001/heft2/Langfassungen/2001-2_Matth.pdf]{style="font-size:18px"}


- Im gezeigten Beispiel wird der Wert der **Pearson-Korrelation** fast ausschließlich durch zwei Ausreißer (Türkei und Polen) dominiert. Der Zusammenhang wird dadurch überschätzt.
- Ein Rangkorrelationsmaß wie die **Spearman-Korrelation** wäre hier eine sinnvolle Alternative &mdash; obwohl eigentlich die Linearität des Zusammenhangs von primärem Interesse ist.

<!----------------->
<!--- New slide --->
<!----------------->
## Rangkorrelationen: Robust gegen Ausreißer

- Beispiel Pearson vs. Spearman:

:::{.fragment}
![](images/pearson_outlier.png){height=350px}
:::

- Ein einziger Ausreißer verändert die Pearson-Korrelation im Beispiel von $\hat{\rho}=-0.046$ nach $\hat{\rho}=0.410$.
- Demgegnüber ist die Spearman-Korrelation "robuster" gegenüber dem Ausreißer &mdash; sie verändert sich "lediglich" von $\hat{\rho}_s=-0.030$ nach $\hat{\rho}_s=0.110$.
- Intution: während der *Wert* es Ausreißers deutlich über dem zweithöchsten Y-Wert liegt, ist der *Rang* nur um 1 höher.



## Der Phi-Koeffizient

- Spezialfall: **beide Variablen haben nur zwei Ausprägungen** (sind also *dichotom*).
- Darstellbar in der [**Vierfeldertafel**]{color="navy"}:

:::: {.columns}
::: {.column width="40%"}
:::{.fragment}
![](images/vierfeldertafel.png)
:::
:::
::: {.column width="55%"}
:::{style="margin-top: -25px"}
- In die vier Felder werden die absoluten Häufigkeiten (idR Anzahl Versuchspersonen) der jeweiligen Variablen-Kombination eingetragen.
- Optional: In der letzten Zeile/Spalte die Summe.
- $a+b+c+d$ muss sich zur Stichprobengröße $n$ addieren.
:::
:::
::::

:::{style="margin-top: -15px"}
- Frage: hängen Raucherstatus und Geschlecht zusammen? Die Antwort liefert der [**Phi-Koeffizient**]{color="navy"}:
:::

:::{.fragment}
$$
\small{
\text{Phi-Koeffizient:}\quad \hat{\phi} = \frac{ad-bc}{\sqrt{(a+b)(c+d)(a+c)(b+d)}}
}
$$
:::

- Wichtig: das Vorzeichen hängt davon ab, in welcher Reihenfolge die beiden Variablen in die Vierfeldertafel eingetragen werden.
    - Übung für zu Hause: das Vorzeichen von $\phi$ dreht sich um, wenn die Spalten male/female vertauscht werden, und ebenso, wenn die Zeilen smoker/nonsmoker vertauscht werden. Warum? 
- Wie alle Korrelationskoeffizienten hat auch der Phi-Koeffizient einen Wertebereich von $[-1; 1]$.

## Der Phi-Koeffizient

- Der Phi-Koeffizient ist identisch mit der Pearson-Korrelation und der Spearman-Korrelation, wenn jeweils beide dichotomen Variablen $X$ und $Y$ mit den Werten 0/1 kodiert werden.

:::{.fragment}
$$
\hat{\phi} = \frac{Cov(X,Y)}{\sigma_{\mkern-2mu\scriptscriptstyle{X}} \sigma_{\mkern-2mu\scriptscriptstyle{Y}}}\qquad\text{mit}\quad X=\{0, 1\}, Y=\{0, 1\}
$$
:::

- Hier ist für das Vorzeichen entscheidend, welche Ausprägung als 0 und welche als 1 definiert wird (analog der Eintrage-Reihenfolge in die Vierfeldertafel).
- In der Praxis findet der Phi-Koeffizient wenig Anwendung, da der Zusammenhang zumeist intuitiver in Form von Häufigkeiten berichtet wird (z.B. Raucherhäufigkeit bei Männern versus Frauen).
- Zwei Vorteile hat der Phi-Koeffizient jedoch:
    - Es muss keine Festlegung erfolgen, ob die Raucherhäufigkeit zwischen den Geschlechtern, oder die Geschlechterhäufigkeit zwischen Rauchern und Nichtrauchern berichtet wird.
    - Der Phi-Koeffizient kann in Metaanalysen mit anderen Studien verglichen werden, die eine ähnliche Fragestellung mit einer linearen Pearson-Korrelation bemessen haben.
- Zudem macht der Phi-Koeffizient in konzeptioneller Hinsicht den Punkt zu Beginn der Vorlesung deutlich, dass jeder Unterschied (z.B. Raucherhäufigkeit bei Männern versus Frauen) auch als Zusammenhang (Zusammenhang von Geschlecht und Raucherstatus) formuliert werden kann.


<!----------------->
<!--- New slide --->
<!----------------->
## Phi-Koeffizient: Beispiel

<div class="vspace-medium"></div>

![](images/vierfeldertafel_beispiel.png){height=220px}

<div class="vspace-large"></div>

$$
\begin{align}
\hat{\phi} &= \frac{ad-bc}{\sqrt{(a+b)(c+d)(a+c)(b+d)}} = \frac{2\cdot11-8\cdot12}{\sqrt{(2+8)(12+11)(2+12)(8+11)}} = \\
    &= \frac{22-96}{\sqrt{10\cdot23\cdot14\cdot19}} = \frac{-74}{\sqrt{61180}} = -0.299
\end{align}
$$

<div class="vspace-medium"></div>
<!-- 
::: {.notabene}
:::: {.columns}
::: {.column width="7%"}
::: {style="margin-top: 10px"}
![](images/notabene2.png){height="65px"}
:::
:::
::: {.column width="93%"}
**Achtung:** Der Phi-Koeffizient beschreibt einen *Zusammenhang im weiteren Sinne* und kann somit auch als Unterschied konzeptionalisiert werden (Beispiel: *unterscheidet* sich die relative Häufigkeit des Haustierbesitzes zwischen Männern und Frauen?).
:::
::::
::: -->