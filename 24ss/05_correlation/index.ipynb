{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Vorlesung 05: Zusammenhänge\"\n",
        "---"
      ],
      "id": "56735014"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## {.blackslide .center}\n",
        "\n",
        "::: {.content-hidden when-format=\"pdf\"}\n",
        "![](images/paradoxia_group_of_scientists.png){.hcenter-image height=300px}\n",
        "<!-- Source: Midjourney -->\n",
        "<div class=\"vspace-small\"></div>\n",
        ":::\n",
        "\n",
        "Die bisherige Auswertung der Beobachtungsstudie hat Evidenz sowohl für Hypothese 1 (mehr Zeit auf TikTok) als auch Hypothese 2 (erhöhte Entzündungswerte) erbracht. So einen richtigen Reim können Sie sich noch nicht auf das Ergebnis machen.\n",
        "\n",
        "Ihre Neugier ist aber geweckt und Sie fragen sich: hängen vielleicht Ihre beiden abhängigen Variablen (TikTok-Zeit & Entzündungswerte) selbst miteinander zusammen? Steigen die Entzündungswerte mit zunehmender Online-Zeit auf der Plattform TikTok? \n",
        "\n",
        "<div class=\"vspace-small\"></div>\n",
        "\n",
        "![](images/paradoxia_zusammenhang.png){height=110px}\n",
        "\n",
        "\n",
        "<!-- ## Der Forschungsprozess {.hcenter-slide}\n",
        "\n",
        "```yaml { .animate src=\"images/scientific_process.svg\"}\n",
        "setup:\n",
        "    - element: \"#results\"\n",
        "      modifier: function() { this.node.style.fill = 'green'; }\n",
        "    - element: \"#resultsbg\"\n",
        "      modifier: function() { this.node.style.fill = '#d8ffe2';}\n",
        "``` -->\n",
        "\n",
        "## Was sind Zusammenhänge?\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"70%\"}\n",
        "- Ein [**Zusammenhang**]{color=\"navy\"} beschreibt zu welchem Grad zwei Variablen (Merkmale) systematisch miteinander in Verbindung stehen\n",
        "- Zusammenhänge bilden die Essenz der psychologischen Forschung&mdash;durch sie versuchen wir die Mechanik der menschlichen Psyche zu verstehen:\n",
        "    - Fördert **Ausdauersport** das **psychische Wohlbefinden**?\n",
        "    - Helfen **Psychotherapiestunden** bei der Überwindung einer **Depression**?\n",
        "    - Wirkt sich **Bildschirmzeit** nachteilig auf die **Schlafqualität** aus?\n",
        "    - Steigt durch **kindliche Frühförderung** die Wahrscheinlichkeit für einen **akadamischen Bildungsabschluss**?\n",
        "- Arten von Zusammenhängen:\n",
        "    - Linearer Zusammenhang (Pearson-Korrelation)\n",
        "    - Rangkorrelation\n",
        "    - Lineare Regression (Unterscheidung von UV und AV)\n",
        "    - Kontingenzkoeffizient\n",
        "<!-- - Fast jede wissenschaftliche Hypothese lässt sich als Zusammenhang formuliern &ndash; selbst Unterschiede!\n",
        "    - [Formulierung als Unterschied]{.underline}: unterscheiden sich Männer und Frauen in ihren verbalen Fähigkeiten?\n",
        "    - [Formulierung als Zusammenhang]{.underline}: steht die kategorische Variable **Geschlecht** in Zusammenhang mit der metrischen Variable **verbale Fähigkeiten**? -->\n",
        ":::\n",
        "<!-- Begin second column -->\n",
        "::: {.column width=\"30%\"}\n",
        "![](images/relationship.png){width=200px}\n",
        ":::\n",
        "::::\n",
        "\n",
        "## Zusammenhänge versus Unterschiede\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"56%\"}\n",
        "\n",
        "- **Zusammenhänge:**\n",
        "    - Zugrunde liegen [zwei Variablen]{.underline} $X$ und $Y$ in [einer Gruppe]{.underline}.\n",
        "    - Verglichen werden [Einzelwerte]{.underline} der Variablen $X$ und $Y$.\n",
        "- **Unterschiede:**\n",
        "    - Zugrunde liegt [eine Variable]{.underline} $X$ in [zwei Gruppen/Bedingungen]{.underline} &rArr; $X_A$, $X_B$.\n",
        "    - Verglichen werden [statistische Maße]{.underline}, die für jede Gruppe/Bedingung auf Basis der Variable berechnet wurden (z.B. Mittelwerte $\\bar{x}_A \\text{ vs. } \\bar{x}_B$).\n",
        "\n",
        ":::\n",
        "::: {.column width=\"44%\"}\n",
        "\n",
        "![](images/zusammenhaenge_unterschiede.png)\n",
        "\n",
        ":::\n",
        "::::\n",
        "\n",
        "- Aber: häufig lassen sich **Unterschiedshypothesen** in **Zusammenhangshypothesen** überführen:\n",
        "    - Unterscheidet sich die akademische Leistung von Rauchern und Nichtrauchern?<br>&rArr; Zusammenhang **Zahl der Zigaretten pro Tag** und **akademische Leistung**\n",
        "    - Unterscheidet sich Medienkonsum von Depressiven und Kontrollen?<br>&rArr; Zusammenhang **Depressivität** und **Medienkonsum**\n",
        "- Zusammenhangshypothesen sind häufig das \"schärfere statistische Schwert\", da eine willkürliche und verlustbehaftete Einteilung einer Variablen in Kategorien vermieden wird.\n",
        "\n",
        "\n",
        "<!---  Definition--->\n",
        "<!-- ::: {.definition .fragment}\n",
        "|||\n",
        "|:-:|-|\n",
        "|||\n",
        "| ![](images/definition.svg){height=70px} | Ein [**Zusammenhang**]{color=\"navy\"} beschreibt zu welchem Grad die **Variation zweier metrischer Variablen** miteinander in Verbindung steht. | \n",
        "|||\n",
        ": {tbl-colwidths=\"[10, 90]\"}\n",
        ":::\n",
        "\n",
        "- Durch die Eingrenzung auf **metrische Variablen** (diskret oder kontinuerlich) stellt etwa die Verbindung von Geschlecht und verbalen Fähigkeiten keinen Zusammenhang im engeren Sinn dar, da Geschlecht eine kategorische Variable ist.\n",
        "- In den meisten Fällen sind Zusammenhänge das \"schärfere statistische Schwert\" als Unterschiede und es lohnt sich oft, Forschungsfragen entsprechend anzupassen:\n",
        "    - Unterscheidet sich die akademische Leistung von Rauchern und Nichtrauchern? &rArr; Zusammenhang **Zahl der Zigaretten pro Tag** und **akademische Leistung**\n",
        "    - Unterscheidet sich der Medienkonsum von Depressiven und Kontrollen? &rArr; Zusammenhang **Depressivität** und **Medienkonsum**\n",
        "    - Unterscheidet sich das Risiko von Alkoholsucht zwischen Nord- und Süddeutschland? &rArr; Zusammenhang **geographischer Breitengrad** und **Alkoholsuchtrisiko** -->\n",
        "\n",
        "\n",
        "# Kovarianz\n",
        "\n",
        "## Kovarianz\n",
        "\n",
        "- [Ziel:]{.underline} mathematische Größe, die zum Ausdruck bringt, wie stark die Variation zweier Variablen miteinander in Zusammenhang steht.\n",
        "- Wir haben bereits eine Größe für die Variation *einer* Variable &mdash; die Varianz:\n",
        "\n",
        ":::{.fragment}\n",
        "$$\n",
        "\\hat{\\sigma}_{\\mkern-2mu\\scriptscriptstyle{X}}^2 = \\frac{1}{n-1}\\sum_{i=1}^n\\big(x_i-\\bar{x}\\big)^2 = \\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})\\color{darkred}{(x_i-\\bar{x})}\n",
        "$$\n",
        ":::\n",
        "\n",
        "\n",
        "- Die Varianz gibt an, wie stark eine Variable $X$ um ihren Mittelwert $\\bar{x}$ schwankt.\n",
        "- Analog berechnet die [**Kovarianz**]{color=\"navy\"}, wie stark die *gemeinsame Schwankung zweier Variablen um ihren jeweiligen Mittelwert* ist:\n",
        "\n",
        ":::{.fragment}\n",
        "$$\n",
        "\\text{Kovarianz:}\\quad \\hat{Cov}(X,Y) = \\hat{\\sigma}_{\\mkern-2mu\\scriptscriptstyle{XY}} = \\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})\\color{darkred}{(y_i-\\bar{y})}\n",
        "$$\n",
        ":::\n",
        "\n",
        "- Im Zentrum der Kovarianz steht die Erkenntnis, dass das **mathematische Produkt** zweier Abweichungsvariablen &mdash; hier die Abweichungen $(x_i-\\bar{x})$ und $(y_i-\\bar{y})$ vom Mittelwert &mdash; angibt, wie stark die beiden Abweichungen gleichsinnig variieren (ko-variieren).\n",
        "\n",
        "## Kovarianz {data-fragment-index=0}\n",
        "\n",
        "![](images/covariance-examples.png){height=370px}\n",
        "\n",
        "[Intuition für positive Kovarianz]{.underline}:\n",
        "\n",
        "- Sind zwei zusammengehörige Datenpunkte $x_i$ und $y_i$ **größer als der Mittelwert**, sind sowohl $(x_i-\\bar{x})$ als auch $(y_i-\\bar{y})$ **positiv**, und damit auch das Produkt $(x_i-\\bar{x})(y_i-\\bar{y})$ **positiv**.\n",
        "\n",
        "- Sind zwei zusammengehörige Datenpunkte $x_i$ und $y_i$ **geringer als der Mittelwert**, sind sowohl $(x_i-\\bar{x})$ als auch $(y_i-\\bar{y})$ **negativ**, und damit das Produkt $(x_i-\\bar{x})(y_i-\\bar{y})$ wieder **positiv**.\n",
        "\n",
        "<div class=\"vspace-small\"></div>\n",
        "\n",
        ":::{.fragment}\n",
        "Die Intuition für eine negative Kovarianz funktioniert ähnlich.\n",
        ":::\n",
        "\n",
        "\n",
        "## Kovarianz: Größe-Gewicht-Beispiel\n",
        "\n",
        "\n",
        "<div class=\"vspace-small\"></div>\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"34%\"}\n",
        "![](images/covariance_height_weight.png)\n",
        ":::\n",
        "::: {.column width=\"66%\"}\n",
        "\n",
        ":::{.fragment}\n",
        "$$\n",
        "\\scriptsize{\n",
        "\\begin{aligned}\n",
        "&\\hat{Cov}(X, Y) = \\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y}) = \\\\\n",
        "          &= \\frac{1}{2}\\big[(160-170)(60-70)+(170-170)(70-70)+(180-170)(80-70)\\big] = \\\\\n",
        "          &= \\frac{1}{2}\\big[(-10)\\cdot (-10)+0\\cdot 0+10\\cdot 10\\big] = \\\\\n",
        "          &= \\frac{1}{2}\\cdot 200 = 100[\\color{red}{cm\\cdot kg}]\n",
        "\\end{aligned}\n",
        "}\n",
        "$$\n",
        ":::\n",
        "\n",
        ":::\n",
        "::::\n",
        "\n",
        "- Problem: die Kovarianz hängt von den Einheiten ab!\n",
        "- Wird im Beispiel die Körpergröße $X$ in der Einheit *Meter* angegeben, so lautet die Kovarianz:\n",
        "\n",
        ":::{.fragment}\n",
        "$$\n",
        "\\scriptsize{\n",
        "\\begin{aligned}\n",
        "\\hat{Cov}(X, Y) &= \\frac{1}{2}\\big[(1{.}60-1{.}70)(60-70)+(1{.}70-1{.}70)(70-70)+(1{,}80-1{.}70)(80-70)\\big] = \\\\\n",
        "          &= \\frac{1}{2}\\big[(-0{.}10)\\cdot (-10)+0\\cdot 0+0{.}10\\cdot 10\\big] = \\frac{1}{2}\\cdot 2 = 1[\\color{red}{m\\cdot kg}]\n",
        "\\end{aligned}\n",
        "}\n",
        "$$\n",
        ":::\n",
        "\n",
        "- Kovarianzen sind also **nicht vergleichbar, wenn sich Einheiten unterscheiden**, und erst recht nicht, wenn sich die Variablen unterscheiden.\n",
        "\n",
        "\n",
        "# Pearson-Korrelation\n",
        "\n",
        "## Pearson-Korrelation\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"70%\"}\n",
        "- Die [**Pearson-Korrelation**]{color=\"navy\"} schafft Abhilfe für das Problem der mangelnden Vergleichbarkeit.\n",
        "- Der Schlüssel: die Kovarianz wird mit den [Standardabweichungen]{color=\"darkgreen\"}<br>$\\color{darkgreen}{\\hat{\\sigma}_{\\mkern-2mu\\scriptscriptstyle{X}}}$ und $\\color{darkgreen}{\\hat{\\sigma}_{\\mkern-2mu\\scriptscriptstyle{Y}}}$ beider Variablen normalisiert.\n",
        "- Formel der Pearson-Korrelation:\n",
        "\n",
        ":::{.fragment}\n",
        "$$\n",
        "\\small{\n",
        "\\hat{\\rho} = \\frac{\\hat{Cov}(X,Y)}{\\color{darkgreen}{\\hat{\\sigma}_{\\mkern-2mu\\scriptscriptstyle{X}}\\,\\hat{\\sigma}_{\\mkern-2mu\\scriptscriptstyle{Y}}}} = \\cdot\\cdot\\cdot = \\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\color{darkgreen}{\\sqrt{\\sum_{i=1}^n(x_i-\\bar{x})\\sum_{i=1}^n(y_i-\\bar{y})}}}\n",
        "}\n",
        "$$\n",
        ":::\n",
        "\n",
        ":::{style=\"margin-top: -10px\"}\n",
        "- Durch das Teilen durch die Standardabweichungen $\\color{darkgreen}{\\hat{\\sigma}_{\\mkern-2mu\\scriptscriptstyle{X}}}$ und $\\color{darkgreen}{\\hat{\\sigma}_{\\mkern-2mu\\scriptscriptstyle{Y}}}$ werden die Einheiten herausgekürzt &mdash; die Korrelation ist also eine **einheitslose Größe**.\n",
        "- Die Korrelation kann Werte zwischen $-1$ (perfekter negativer Zusammenhang) und $+1$ (perfekter positiver Zusammenhang) annehmen.\n",
        ":::\n",
        "\n",
        ":::\n",
        "<!-- Begin second column -->\n",
        "::: {.column width=\"30%\"}\n",
        "\n",
        "![Der britische Mathematiker Karl Pearson in seinem Büro im Jahr 1910. Neben dem Korrelationskoeffizienten verdanken wir Pearson viele andere statistische Konzepte wie die Hauptkomponentenanalyse oder den p-Wert. Später wurden seine Ansichten zu Eugenik kritisch hinterfragt. Bildnachweis^[http://www.learn-stat.com/life-of-karl-pearson/]](images/karl_pearson.png)\n",
        "\n",
        ":::\n",
        "::::\n",
        "\n",
        ":::{style=\"margin-top: -15px\"}\n",
        "- [Hinweis:]{.underline} in wissenschaftlichen Publikationen und Abbildungen wird der Korrelationskoeffizient $\\hat{\\rho}$ der Einfachheit halber häufig mit $r$ bezeichnet (z.B. $r=0.32$). Grund: die Formeln von inferentieller und nicht-inferentieller Korrelation sind identisch.\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "<!-- ```{python}\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import pearsonr, linregress\n",
        "\n",
        "np.random.seed(0)\n",
        "data = np.random.multivariate_normal([0, 0], [[1, 0.5], [0.5, 1]], 30) + 3\n",
        "x, y = data[:, 0]/2, data[:, 1]\n",
        "fontsize = 15\n",
        "\n",
        "r, p = pearsonr(x, y)\n",
        "reg = linregress(x, y)\n",
        "xrange = np.array([0.1, 2.7])\n",
        "\n",
        "plt.figure(figsize=(4, 3))\n",
        "plt.scatter(x, y, s=50, marker='o', color='blue', edgecolors='w')\n",
        "plt.plot(xrange, reg.slope*xrange+reg.intercept, color='k', lw=2)\n",
        "plt.xlabel('Schokoladenkonsum [kg/Tag]', fontsize=fontsize)\n",
        "plt.ylabel('Zufriedenheit', fontsize=fontsize)\n",
        "plt.xticks(fontsize=fontsize-2)\n",
        "plt.yticks(fontsize=fontsize-2)\n",
        "plt.ylim(0, 6.5)\n",
        "plt.text(0.05, 0.9, fr'$r={r:.3f}$ ($p={p:.4f}$)'.replace('.', '{,}'), color='k', transform=plt.gca().transAxes, fontsize=fontsize-2)\n",
        "plt.title('Streudiagramm', fontsize=fontsize+2, fontstyle='italic')\n",
        "plt.savefig('images/streudiagramm.png', bbox_inches=\"tight\")\n",
        "``` -->\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "## Das Streudiagramm\n",
        "\n",
        "- Zusammenhänge zweier Variablen werden häufig in Form eines [**Streudiagramms**]{color=\"navy\"} (engl. *scatter plot*) dargestellt.\n",
        "- Bei der Korrelation ist es dabei willkürlich, welche Variable auf der x- und y-Achse liegt.\n",
        "- Häufig wird zusätzlich zur \"Punktwolke\" auch eine **Regressionsgerade** dargestellt, sowie die Stärke des Zusammenhangs (r=.., p=..).\n",
        "\n",
        ":::{.fragment}\n",
        "![](images/streudiagramm.png){height=400px}\n",
        ":::\n",
        "\n",
        "\n",
        "## Pearson-Korrelation: Größe-Gewicht-Beispiel\n",
        "\n",
        "<div class=\"vspace-small\"></div>\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"39%\"}\n",
        "![](images/covariance_height_weight.png)\n",
        ":::\n",
        "::: {.column width=\"61%\"}\n",
        "Wir hatten:\n",
        "\n",
        "$$\n",
        "\\small{\n",
        "\\begin{aligned}\n",
        "&\\hat{Cov}(X, Y) = \\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y}) = 100[\\color{red}{cm\\cdot kg}]\n",
        "\\end{aligned}\n",
        "}\n",
        "$$\n",
        "\n",
        ":::{.fragment}\n",
        "Und berechnen nun die Korrelation $\\;\\;\\hat{\\rho} = \\frac{Cov(X,Y)}{\\hat{\\sigma}_{\\mkern-2mu\\scriptscriptstyle{X}} \\hat{\\sigma}_{\\mkern-2mu\\scriptscriptstyle{Y}}}$\n",
        "\n",
        "$\\scriptsize{\\hat{\\sigma}_{\\mkern-2mu\\scriptscriptstyle{X}}=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n\\big(x_i-\\bar{x}\\big)^2}=\\sqrt{\\frac{1}{2}[(-10)^2+0^2+10^2]}=\\sqrt{100}=10[\\color{red}{cm}]}$\n",
        "\n",
        "$\\scriptsize{\\hat{\\sigma}_{\\mkern-2mu\\scriptscriptstyle{Y}}=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n\\big(y_i-\\bar{y}\\big)^2}=\\sqrt{\\frac{1}{2}[(-10)^2+0^2+10^2]}=\\sqrt{100}=10[\\color{red}{kg}]}$\n",
        "\n",
        "$$\n",
        "\\small{\n",
        "\\hat{\\rho} = \\frac{100[\\color{red}{cm\\cdot kg}]}{10[\\color{red}{cm}]\\cdot10[\\color{red}{kg}]}=1\n",
        "}\n",
        "$$\n",
        ":::\n",
        "\n",
        ":::\n",
        "::::\n",
        "\n",
        "- Wird nun die Körpergröße statt in der Einheit *Zentimeter* in *Meter* angeben, bleibt der Korrelationskoeffizient $\\hat{\\rho}$ unverändert:\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"43%\"}\n",
        ":::{.fragment}\n",
        "$$\n",
        "\\small{\n",
        "\\hat{\\rho} = \\frac{100[\\color{red}{cm\\cdot kg}]}{10[\\color{red}{cm}]\\cdot10[\\color{red}{kg}]}=\\frac{1[\\color{red}{m\\cdot kg}]}{0.1[\\color{red}{m}]\\cdot10[\\color{red}{kg}]}=1\n",
        "}\n",
        "$$\n",
        ":::\n",
        ":::\n",
        "::: {.column width=\"57%\"}\n",
        ":::{.fragment}\n",
        "**Die Normalisierung mit der Standardabweichung sorgt dafür, dass die Korrelation unabhängig von der Einheit ist!**\n",
        ":::\n",
        ":::\n",
        "::::\n",
        "\n",
        "\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "## Interpretation der Pearson-Korrelation\n",
        "\n",
        "- Die Pearson-Korrelation zeigt an, **wie linear** der Zusammenhang zweier Variablen ausgeprägt ist.\n",
        "- Die Pearson-Korrelation ist dabei [nicht]{.underline} von der Steigung einer gedachten Gerade abhängig.\n",
        "\n",
        ":::{.fragment}\n",
        "![Korrelationskoeffizienten für verschiedene hypothetische Streudiagramme](images/correlation_examples.png){width=1030px}\n",
        ":::\n",
        "\n",
        "- Ein Zusammenhang zweier Variablen kann extrem schwach sein (z.B. so, dass eine Verdopplung von X nur einer 0.1%-Steigerung von Y entspricht) und dennoch kann die Korrelation stark sein (= nah an $\\pm 1$), wenn die Punkte exakt auf einer Geraden liegen.\n",
        "- Auf einen Satz gemünzt kann man sagen: \n",
        "\n",
        "::: {.fragment style=\"font-size: 26px; margin-top: -10px\"}\n",
        "> Die Pearson-Korrelation misst, wie gut bivariate Daten durch eine Gerade abgebildet werden können.\n",
        ":::\n",
        "\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        ":::{.content-hidden when-format=\"pdf\"}\n",
        "## [https://www.guessthecorrelation.com/]{style=\"font-size: 37px\"}\n",
        "\n",
        "<iframe width=100% height=\"100%\" src=\"https://www.guessthecorrelation.com/\"></iframe>\n",
        ":::\n",
        "\n",
        "## Voraussetzungen für das Berechnen der Pearson-Korrelation\n",
        "\n",
        "- Die Pearson-Korrelation *kann* *immer* berechnet werden, solange beide Variablen aus Zahlenwerten bestehen.\n",
        "- Es gibt jedoch weitere Kriterien, die für die Sinnhaftigkeit und Interpretierbarkeit der Pearson-Korrelation wichtig sind:\n",
        "\n",
        ":::{.fragment}\n",
        "<!---  Table --->\n",
        "|Kriterium|Falls Kriterium nicht erfüllt? | Beispiel | Mögliche Abhilfe? |\n",
        "|-|-|-|-|\n",
        "| Daten haben mindestens Intervallskalenniveau | ▪&numsp;Keine Aussage über Linearität des untersuchten Zusammenhangs möglich \\\n",
        "▪&numsp;Korrelation nicht interpretierbar | ![](images/example_ordinal.png) | Rangkorrelation |\n",
        "| Keine Ausreißer | Korrelationskoeffizient kann massiv verzerrt sein | ![](images/outlier_example.png) | Ausreißer entfernen oder Rangkorrelation | \n",
        "| Zusammenhang der Daten wird *nicht* durch *nicht-linearen* Anteil dominiert | ▪&numsp;Pearson-Korrelation falsches Modell \\\n",
        "▪&numsp;Linearität der Daten wird verzerrt wiedergegeben, da die Korrelation vom nicht-linearen Teil beeinflusst wird  | ![](images/nonlinear.png) | Komplexeres Modell, das den nicht-linearen Anteil berücksichtigt |\n",
        "\n",
        ": {tbl-colwidths=\"[24, 36, 20, 20]\"}\n",
        ":::\n",
        "\n",
        "## Mythen zur Pearson-Korrelation\n",
        "\n",
        "- In vielen Quellen finden sich darüber hinaus **unzutreffende Behauptungen** zur Pearson-Korrelation:\n",
        "\n",
        ":::{.fragment style=\"margin-top: 5px\"}\n",
        "<!---  Table --->\n",
        "||Behauptung| Fact |\n",
        "|-|-|-|\n",
        "| **Mythos 1** | Die Variablen müssen kontinuierlich sein | **Pearson-Korrelation ist valide für diskrete Daten**, solange diese mindestens Intervallskalenniveau aufweisen. Tatsächlich gibt es sogar eine Variante der Pearson-Korrelation, bei der beide Variablen binär sind (Phi-Koeffizient). |\n",
        "| **Mythos 2** | Die Variablen müssen einen linearen Zusammenhang aufweisen | Gegenbeispiel: wenn Daten aus zufälligem Rauschen basieren, sind sie mit Sicherheit nicht linear verbunden &mdash; und dennoch gibt der Pearson-Koeffizient korrekterweise an, dass die Korrelation ungefähr 0 ist. Zusammenhänge in der Psychologie sind *sehr selten* eindeutig linear, dennoch kann es sinnvoll sein, die Pearson-Korrelation anzuwenden. Besser ist daher zu sagen (s. vorherige Folie), dass der Zusammenhang **nicht zu stark durch einen nicht-linearen Anteil dominiert** werden sollten. |\n",
        "| **Mythos 3** | Die beiden Variablen müssen normalverteilt sein. | Der **Pearson-Korrelationskoeffizient per se erfordert keine Normalverteilung** der Variablen. [Korrekt ist aber, dass die Daten für die **Berechnung eines p-Wertes auf Basis des t-Tests annähernd normalverteilt** (ganz korrekt: *bivariat normalverteilt*) sein sollten. Wenn Normalverteilung nicht gegeben ist, können andere Signifikanztests (Permutation, Bootstrap) verwendet werden.]{color=\"darkred\"}\n",
        "| [**Mythos 4**]{color=\"darkred\"} | [Die Variablen müssen varianzhomogen sein]{color=\"darkred\"} | [Varianzhomogenität (auch Homoskedastizität) meint, dass Y-Werte ähnliche Varianz in verschiedenen Abschnitten der X-Achse haben und umgekehrt. Hier gilt das gleiche wie bei Mythos 3: **Varianzhomogenität ist keine Voraussetzung für die Anwendung der Pearson-Korrelation per se, wohl aber für die Anwendung des t-Tests.**]{color=\"darkred\"}\n",
        "\n",
        ": {tbl-colwidths=\"[10, 18, 72]\"}\n",
        ":::\n",
        "\n",
        "\n",
        "<!-- ```{python}\n",
        "from string import ascii_letters\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.set_theme(style=\"white\")\n",
        "fontsize = 36\n",
        "\n",
        "data = [\n",
        "    [1, 0.13, 0.35, 0.23, -0.2],\n",
        "    [0, 1, 0.37, 0.1, -0.4],\n",
        "    [0, 0, 1, 0.01, -0.26],\n",
        "    [0, 0, 0, 1, -0.1],\n",
        "    [0, 0, 0, 0, 1]\n",
        "]\n",
        "\n",
        "annot = [\n",
        "    ['1', '0.13', '0.35', '0.23', '-0.2'],\n",
        "    ['', '1', '0.37', '0.1', '-0.4'],\n",
        "    ['', '', '1', '0.01', '-0.26'],\n",
        "    ['', '', '', '1', '-0.1'],\n",
        "    ['', '', '', '', '1']\n",
        "]\n",
        "\n",
        "columns = ['O', 'C', 'E', 'A', 'n']\n",
        "\n",
        "corr = pd.DataFrame(data, columns=columns, index=pd.Index(columns))\n",
        "\n",
        "# Generate a mask for the upper triangle\n",
        "# mask = np.tril(np.ones_like(corr, dtype=bool))\n",
        "mask = np.zeros_like(corr, dtype=bool)\n",
        "np.fill_diagonal(mask, True)\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "f, ax = plt.subplots(figsize=(11, 9))\n",
        "\n",
        "# Generate a custom diverging colormap\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "\n",
        "# Draw the heatmap with the mask and correct aspect ratio\n",
        "hm = sns.heatmap(corr, annot=annot, fmt='', mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "                 square=True, linewidths=.5, cbar_kws=dict(shrink=1, label='Korrelation'), annot_kws=dict(size=fontsize))\n",
        "plt.gca().collections[0].colorbar.ax.tick_params(labelsize=28)\n",
        "hm.figure.axes[-1].set_ylabel('Korrelation', size=fontsize)\n",
        "hm.tick_params(labelsize=fontsize)\n",
        "plt.yticks(rotation=0) \n",
        "hm.patch.set_facecolor('#777')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'images/corrmatrix.png', bbox_inches=\"tight\")\n",
        "```-->\n",
        "\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "## Korrelationsmatrix\n",
        "\n",
        "- Eine **Korrelation** bestimmt immer den Zusammenhang zwischen **zwei Variablen**.\n",
        "- Gibt es mehr als zwei Variablen (z.B. die \"Big Five\"), bietet sich eine Darstellung aller paarweisen Korrelationen an &ndash; die [**Korrelationsmatrix**]{color=\"navy\"}.\n",
        "\n",
        ":::: {.columns .vcenter-column style=\"margin-top:-15px\"}\n",
        "::: {.column width=\"60%\"}\n",
        ":::{.fragment}\n",
        "![Tabellarische Korrelationsmatrix. Sterne kennzeichnen häufig das Signifikanzniveau.](images/corrmatrix_table.png)\n",
        ":::\n",
        ":::\n",
        "::: {.column width=\"40%\"}\n",
        ":::{.fragment}\n",
        "![Korrelationsmatrix als \"Heatmap\" &ndash; die Einfärbung ist ein visuelles Hilfsmittel zur intuitiven und schnellen Erfassung der Korrelationsstruktur von mehreren Variablen.](images/corrmatrix.png)\n",
        ":::\n",
        ":::\n",
        "::::\n",
        "- Die [Nebendiagonalelemente]{color=\"darkgreen\"} sind der interessante Teil der Korrelationsmatrix, sie geben die Korrelationen verschiedener Variablen an.\n",
        "- Die [Diagonalelemente]{color=\"darkred\"}, also die Korrelationen von Variablen mit sich selbst, sind immer 1.\n",
        "\n",
        "\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "## Kovarianzmatrix\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"75%\"}\n",
        "- Eine analoge Matrix-Darstellung gibt es auch für die **Kovarianz**.\n",
        "- Im Unterschied zur Korrelationsmatrix sind die Diagonalelemente der Kovarianzmatrix nicht 1, sondern geben die **Varianz** der Variable an.\n",
        "- Sei $\\mathbf{X}$ (beachte Fettschrift) ein Vektor von $n$ Variablen $X_1 .. X_n$, so ist die zugehörige Kovarianzematrix $\\operatorname{Cov}(\\mathbf{X})$:\n",
        "\n",
        ":::{.fragment}\n",
        "$$\n",
        "\\small{\n",
        "\\begin{aligned}\n",
        "\\operatorname{Cov}(\\mathbf{X}) & = \n",
        "\\begin{pmatrix}\\operatorname{Var}(X_1) & \\operatorname{Cov}(X_1,X_2) & \\cdots & \\operatorname{Cov}(X_1,X_n) \\\\ \\\\\n",
        " \\operatorname{Cov}(X_2,X_1)  & \\operatorname{Var}(X_2) & \\cdots & \\operatorname{Cov}(X_2,X_n) \\\\ \\\\\n",
        " \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\\\\n",
        "\\operatorname{Cov}(X_n,X_1) & \\operatorname{Cov}(X_n,X_2) & \\cdots & \\operatorname{Var}(X_n)\n",
        "\\end{pmatrix}\n",
        "\\end{aligned}\n",
        "}\n",
        "$$\n",
        ":::\n",
        "\n",
        ":::\n",
        "::: {.column width=\"25%\"}\n",
        ":::{.fragment}\n",
        "![](images/covariance_matrix.png)\n",
        ":::\n",
        ":::\n",
        "::::\n",
        "\n",
        "- Im Gegensatz zur Korrelationsmatrix ist die Kovarianzmatrix selten das \"Endprodukt\" einer Analyse, sondern meist ein Zwischenschritt in fortgeschritteneren statistischen Analysen wie der **Hauptkomponentenanalyse**.\n",
        "\n",
        "# Rangkorrelationen\n",
        "\n",
        "## Was ist eine Rangkorrelation?\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"72%\"}\n",
        "\n",
        "- Eine Alternative zur Pearson-Korrelation sind [**Rangkorrelationsmaße**]{color=\"navy\"}, die nicht die Linearität eines Zusammenhangs, sondern die **Monotonie des Zusammenhangs** bemessen.\n",
        "- Rangkorrelationen werden in der Regel aus zwei Gründen angewendet:\n",
        "    - **Theoretischer Grund:** Man ist tatsächlich an der Monotonie &mdash; und nicht der Linearität &mdash; eines Zusammenhangs interessiert. Dies ist häufig der Fall, wenn die Daten tatsächlich als ordinalskalierte Ränge &ndash; d.h. ohne interpretierbare Abstände &ndash; vorliegen.\n",
        "    - **Praktischer Grund:** Eigentlich ist die Linearität das primäre Interesse, aber es sind entweder die Annahmen der Pearson-Korrelation verletzt (Daten nicht intervallskaliert) oder die Pearson-Korrelation ist aus anderen Gründen ungeeignet (Ausreißer). Die Rangkorrelation ist die \"Notfalloption\".\n",
        "- Für Rangkorrelationsmaße spielt lediglich die Reihenfolge (\"Rang\") der Daten eine Rolle und nicht die spezifischen Werte.\n",
        "    - Ähnlich wie beim Median ist es genau diese Eigenschaft, die Rangkorrelationsmaße unanfällig für Ausreißer macht.\n",
        "<!-- - Im Folgenden werden zwei die in der Praxis häufigsten Rangkorrelationsmaße behandelt:\n",
        "    - [**Spearman-Korrelation**]{color=\"navy\"}\n",
        "    - [**Kendall'sches Tau**]{color=\"navy\"} -->\n",
        ":::\n",
        "::: {.column width=\"28%\"}\n",
        "![](images/rankorder_example.png)\n",
        "\n",
        "<div class=\"vspace-medium\"></div>\n",
        "\n",
        "![](images/rankorder_example2.png)\n",
        ":::\n",
        "::::\n",
        "\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "## Spearman-Korrelation\n",
        "- Das mutmaßlich häufigste Rangkorrelationsmaß ist die [**Spearman-Korrelation** $\\hat{\\rho}_s$]{color='navy'}. \n",
        "- Die Spearman-Korrelation ist identisch zur Pearson-Korrelation, wenn die Variablen $X$ und $Y$ als Ränge $R(X)$ und $R(Y)$ vorliegen:\n",
        "\n",
        ":::{.fragment}\n",
        "$$\n",
        "\\hat{\\rho}_s = \\frac{\\hat{Cov}(R(X),R(Y))}{\\hat{\\sigma}_{\\mkern-2mu\\scriptscriptstyle{R(X)}} \\hat{\\sigma}_{\\mkern-2mu\\scriptscriptstyle{R(Y)}}}\n",
        "$$\n",
        "&numsp;&numsp;wobei $\\hat{\\sigma}_{\\mkern-2mu\\scriptscriptstyle{R(X)}}$ und $\\hat{\\sigma}_{\\mkern-2mu\\scriptscriptstyle{R(Y)}}$ die Standardabweichungen der Ränge von $X$ und $Y$ sind.\n",
        ":::\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"60%\"}\n",
        "- Wie die Pearson-Korrelation nimmt die Spearman-Korrelation Werte zwischen $-1$ und $+1$ an:\n",
        "    - Ein positiver Wert impliziert eine positiven monotonen Zusammenhang\n",
        "    - Ein negativer Wert impliziert eine negativen monotonen Zusammenhang\n",
        "    - Ein Wert nahe bei 0 impliziert einen schwachen (oder keinen) monotonen Zusammenhang\n",
        ":::\n",
        "::: {.column width=\"40%\"}\n",
        ":::{.fragment}\n",
        "![](images/monotonie.png)\n",
        ":::\n",
        ":::\n",
        "::::\n",
        "\n",
        "\n",
        "\n",
        "## Berechnung von Rängen\n",
        "\n",
        "- Liegen die Variablen $X$ oder $Y$ nicht als Ränge vor, müssen sie zunächst in Ränge $R(X)$ bzw. $R(Y)$ umgewandelt werden:\n",
        "\n",
        ":::{style=\"font-size: 23px; margin-left:40px; margin-top: -12px\"}\n",
        "1. Werte der Variable sortieren\n",
        "2. (Unnormierte) Ränge zuordnen\n",
        "3. Normierung: Gleiche Werte erhalten den Mittelwert ihrer Ränge\n",
        "4. (Optional) Variablen in ihre ursprüngliche Reihenfolge bringen\n",
        ":::\n",
        "\n",
        ":::{.fragment}\n",
        "![](images/rangbildung.png){height=333px}\n",
        ":::\n",
        "\n",
        "- Die Tabelle gibt die Rangberechnung *einer* Variablen an (z.B. X) &mdash; für die andere Variable muss das analoge Prozedere durchgeführt werden.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "## Kendalls Tau\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"69%\"}\n",
        "- Eine Alternative Rangkorrelation zu Spearman ist [**Kendalls Tau**]{color=\"navy\"} $\\hat{\\tau}$.\n",
        "- Kendalls Tau vergleicht inwieweit die Rangfolge *aller* Paare ($x_i$, $x_j$) mit der Rangfolge *aller* Paare ($y_i$, $y_j$) übereinstimmt.\n",
        "- Dazu wird die Zahl der konkordanten (übereinstimmenden) und diskordanten (nicht übereinstimmenden) Paare gezählt.\n",
        ":::\n",
        "::: {.column width=\"31%\"}\n",
        "<div class=\"vspace-large\"></div>\n",
        ":::{.fragment}\n",
        "![](images/Kendalls_Tau_Gleichung.png)\n",
        ":::\n",
        ":::\n",
        "::::\n",
        "\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"30%\"}\n",
        "\n",
        ":::{.fragment}\n",
        "::: {.greybox}\n",
        "**Beispiel**\n",
        ":::\n",
        "\n",
        "\n",
        "$$\n",
        "\\small{\n",
        "X=\\begin{pmatrix} 9\\\\ 3\\\\ 7\\\\ 5 \\end{pmatrix}\n",
        "Y=\\begin{pmatrix} 18\\\\ 7\\\\ 8\\\\ 21 \\end{pmatrix}\n",
        "}\n",
        "$$\n",
        ":::\n",
        ":::\n",
        "::: {.column width=\"70%\"}\n",
        "- Die Paare von X sind: $\\small{(9, 3), (9, 7), (9, 5), (3, 7), (3, 5), (7, 5)}$\n",
        "- Die Paare von Y sind: $\\small{(18, 7), (18, 8), (18, 21), (7, 8), (7, 21), (8, 21)}$\n",
        "- Die Paare $(x_1=9, x_3=7)$ und $(y_1=18, y_3=8)$ wären **konkordant**, da die Rangfolge des X-Paares ($x_1 > x_3$) gleich der Rangfolge des entsprechenden Y-Paares ($y_1 > y_3$) ist.\n",
        "- Die Paare$(x_1=9, x_4=5)$ und $(y_1=18, y_4=21)$ wären **diskonkordant**, da die Rangfolge des X-Paares ($x_1 > x_4$) ungleich der Rangfolge des entsprechenden Y-Paares ($y_1 < y_4$) ist.\n",
        ":::\n",
        "::::\n",
        "\n",
        ":::{.fragment}\n",
        "[Insgesamt gibt es im Beispiel 4 konkordante Paare und 2 diskordante Paare (prüfe nach!), daher gilt:]{style=\"margin-top:-12px; display: block\"}\n",
        ":::\n",
        "\n",
        ":::{.fragment}\n",
        "$$\n",
        "\\hat{\\tau} = \\frac{K-D}{K+D} = \\frac{4-2}{4+2} = \\frac{2}{6} = 0.333..\n",
        "$$\n",
        ":::\n"
      ],
      "id": "b9eb30ad"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr, spearmanr, linregress\n",
        "np.random.seed(13)\n",
        "fontsize=15\n",
        "\n",
        "data = np.random.multivariate_normal([0, 0], [[1, 0.1], [0.1, 1]], 20) + 3\n",
        "data = np.vstack((data, [5, 12]))\n",
        "x, y = data[:, 0], data[:, 1]\n",
        "\n",
        "reg = linregress(x, y)\n",
        "reg2 = linregress(x[:-1], y[:-1])\n",
        "r, p = pearsonr(x, y)\n",
        "r2, p2 = pearsonr(x[:-1], y[:-1])\n",
        "xrange = np.array([1.1, 5.2])\n",
        "\n",
        "plt.figure(figsize=(11, 4))\n",
        "plt.subplot(121)\n",
        "plt.scatter(x[:-1], y[:-1], c='#888')\n",
        "plt.scatter(x[-1], y[-1], c='#d80000')\n",
        "plt.plot(xrange, reg.slope*xrange+reg.intercept, color='#d80000')\n",
        "plt.plot(xrange, reg2.slope*xrange+reg2.intercept, color='#888')\n",
        "plt.xticks(fontsize=fontsize-2)\n",
        "plt.yticks(fontsize=fontsize-2)\n",
        "plt.xlabel('X', fontsize=fontsize)\n",
        "plt.ylabel('Y', fontsize=fontsize)\n",
        "plt.text(1, 10.25, r'Pearson (mit Ausreißer): $\\hat{\\rho} =' + f' {r:.3f}$', fontsize=fontsize-2, color='#d80000')\n",
        "plt.text(1, 11.4, r'Pearson (ohne Ausreißer): $\\hat{\\rho} =' + f' {r2:.3f}$', fontsize=fontsize-2, color='#888')\n",
        "plt.text(4.53, 11, 'Ausreißer', fontsize=fontsize-2, color='#d80000')\n",
        "\n",
        "xr, yr = data[:, 0].argsort().argsort()+1, data[:, 1].argsort().argsort()+1\n",
        "print(data[:, 0], data[:, 1])\n",
        "print(xr, yr)\n",
        "rr, pr = spearmanr(xr, yr)\n",
        "rr2, pr2 = spearmanr(xr[:-1], yr[:-1])\n",
        "regr = linregress(xr, yr)\n",
        "regr2 = linregress(xr[:-1], yr[:-1])\n",
        "xrange = np.array([1, 22])\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.scatter(xr[:-1], yr[:-1], c='#888')\n",
        "plt.scatter(xr[-1], yr[-1], c='#d80000')\n",
        "plt.plot(xrange, regr.slope*xrange+regr.intercept, color='#d80000')\n",
        "plt.plot(xrange, regr2.slope*xrange+regr2.intercept, color='#888')\n",
        "plt.xticks(range(0, 21, 5), fontsize=fontsize-2)\n",
        "plt.yticks(fontsize=fontsize-2)\n",
        "plt.xlabel('Rang(X)', fontsize=fontsize)\n",
        "plt.ylabel('Rang(Y)', fontsize=fontsize)\n",
        "plt.text(0.5, 23.5, 'Spearman (mit Ausreißer): $\\hat{\\rho}_s =' + f'{rr:.3f}$', fontsize=fontsize-2, color='#d80000')\n",
        "plt.text(0.5, 26, 'Spearman (ohne Ausreißer): $\\hat{\\rho}_s =' + f'{rr2:.3f}$', fontsize=fontsize-2, color='#888')\n",
        "plt.ylim(0, 29)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig('images/pearson_outlier.png', bbox_inches=\"tight\")"
      ],
      "id": "e001a706",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "## Ausreißer: Der Storch bringt die Babys zur Welt(p = 0.008)\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/geburtenrate_stoerche.png){height=300px}\n",
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/geburtenrate_stoerche_tabelle.png){height=400px}\n",
        ":::\n",
        "::::\n",
        "\n",
        "[http://www3.math.uni-paderborn.de/~agbiehler/sis/sisonline/struktur/jahrgang21-2001/heft2/Langfassungen/2001-2_Matth.pdf]{style=\"font-size:18px\"}\n",
        "\n",
        "\n",
        "- Im gezeigten Beispiel wird der Wert der **Pearson-Korrelation** fast ausschließlich durch zwei Ausreißer (Türkei und Polen) dominiert. Der Zusammenhang wird dadurch überschätzt.\n",
        "- Ein Rangkorrelationsmaß wie die **Spearman-Korrelation** wäre hier eine sinnvolle Alternative &mdash; obwohl eigentlich die Linearität des Zusammenhangs von primärem Interesse ist.\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "## Rangkorrelationen: Robust gegen Ausreißer\n",
        "\n",
        "- Beispiel Pearson vs. Spearman:\n",
        "\n",
        ":::{.fragment}\n",
        "![](images/pearson_outlier.png)\n",
        ":::\n",
        "\n",
        "- Ein einziger Ausreißer verändert die Pearson-Korrelation im Beispiel von $r=-0{,}046$ nach $r=0{,}410$\n",
        "- Demgegnüber ist die Spearman-Korrelation \"robuster\" gegenüber dem Ausreißer &mdash; sie verändert sich \"lediglich\" von $\\rho=-0{,}030$ nach $r_s=0{,}110$.\n",
        "- Intution: während der *Wert* es Ausreißers deutlich über dem zweithöchsten Y-Wert liegt, ist der *Rang* nur um 1 höher.\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "## [Wann Spearman und wann Kendall?]{color=\"darkred\"}\n",
        "\n",
        "![](images/kein_klausurstoff.png){.absolute bottom=0 right=0 height=130px}\n",
        "\n",
        "- Beide Rangkorrelationskoeffizienten bestimmen die Monotonie eines Zusammenhangs.\n",
        "- **Kendall ist robuster bei kleinen Stichproben** und ist in diesen Fällen bevorzugt.\n",
        "- **Spearman ist etwas weniger sensitiv gegenüber Rangbindungen** (also wenn zwei Werte den gleichen Rang haben) und ist daher bevorzugt, wenn es viele Rangbindungen gibt^[Puth M-T, Neuhäuser M, Ruxton GD (2015) Effective use of Spearman’s and Kendall’s correlation coefficients for association between two measured traits. Animal Behaviour 102:77–84.].\n",
        "    - Beachte: auch bei den Kendall'schen Paaren gibt es Rangbindungen, und zwar dann, wenn die verglichenen *Paare* zwischen $X$ und $Y$ genau identisch sind.\n",
        "    - Diese Paare sind weder konkordant noch diskordant und es gibt verschiedene Algorithmen diese Fälle zu berücksichtigen (hier nicht behandelt).\n",
        "- **In Abwesenheit von Rangbindungen liefert Kendall präzisere Schätzungen** und ist in diesem Fall zu bevorzugen.^[Puth M-T, Neuhäuser M, Ruxton GD (2015) Effective use of Spearman’s and Kendall’s correlation coefficients for association between two measured traits. Animal Behaviour 102:77–84.].\n",
        "- In der Statistik wird Kendall häufig als \"Default\"-Rangkorrelation empfohlen^[DC Howell (2012). Statistical Methods for Psychology. Wadsworth.]:\n",
        "    - idR präzisere Schätzung des Populationsparameters\n",
        "    - Standardfehler ist bekannt (für Spearman gibt es lediglich Approximationen^[https://stats.stackexchange.com/questions/18887/how-to-calculate-a-confidence-interval-for-spearmans-rank-correlation])\n",
        "- In der Praxis ist aber Spearman der weitaus verbreitetere Korrelationskoeffizient &mdash;<br> womöglich weil er idR größer als der Kendall-Koeffizient ist &#128521;.\n",
        "\n",
        "<!-- https://www.researchgate.net/post/Does-Spearmans-rho-have-any-advantage-over-Kendalls-tau     -->\n",
        "\n",
        "\n",
        "## Der Phi-Koeffizient\n",
        "\n",
        "- Spezialfall: **beide Variablen haben nur zwei Ausprägungen** (sind also *dichotom*).\n",
        "- Darstellbar in der [**Vierfeldertafel**]{color=\"navy\"}:\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"40%\"}\n",
        ":::{.fragment}\n",
        "![](images/vierfeldertafel.png)\n",
        ":::\n",
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "- In die vier Felder werden die Häufigkeiten der jeweiligen Variablen-Kombination eingetragen.\n",
        "- Optional: In der letzten Zeile/Spalte die Summe.\n",
        "- $a+b+c+d$ muss sich zur Stichprobengröße $n$ addieren.\n",
        ":::\n",
        "::::\n",
        "\n",
        "- Frage: hängen Raucherstatus und Geschlecht zusammen? Die Antwort liefert der [**Phi-Koeffizient**]{color=\"navy\"}:\n",
        "\n",
        ":::{.fragment}\n",
        "$$\n",
        "\\small{\n",
        "\\text{Phi-Koeffizient:}\\quad \\phi = \\frac{ad-bc}{\\sqrt{(a+b)(c+d)(a+c)(b+d)}}\n",
        "}\n",
        "$$\n",
        ":::\n",
        "\n",
        "- Wichtig: das Vorzeichen hängt davon ab, in welcher Reihenfolge die beiden Variablen in die Vierfeldertafel eingetragen werden.\n",
        "    - Übung für zu Hause: das Vorzeichen von $\\phi$ dreht sich um, wenn die Spalten male/female vertauscht werden, und ebenso, wenn die Zeilen smoker/nonsmoker vertauscht werden. Warum? \n",
        "- Wie alle Korrelationskoeffizienten hat auch der Phi-Koeffizient einen Wertebereich von $[-1; 1]$.\n",
        "\n",
        "## Der Phi-Koeffizient\n",
        "\n",
        "- Der Phi-Koeffizient ist identisch mit der Pearson-Korrelation und der Spearman-Korrelation, wenn beide dichotomen Variablen mit 0 und 1 kodiert werden.\n",
        "\n",
        ":::{.fragment}\n",
        "$$\n",
        "\\phi = \\frac{Cov(X,Y)}{s_X s_Y}\\qquad\\text{mit}\\quad X=\\{0, 1\\}, Y=\\{0, 1\\}\n",
        "$$\n",
        ":::\n",
        "\n",
        "- Hier ist für das Vorzeichen entscheidend, welche Ausprägung als 0 und welche als 1 definiert wird (analog der Eintrage-Reihenfolge in die Vierfeldertafel).\n",
        "- In der Praxis findet der Phi-Koeffizient wenig Anwendung, da der Zusammenhang zumeist intuitiver in Form von Häufigkeiten berichtet wird (z.B. Raucherhäufigkeit bei Männern versus Frauen).\n",
        "- Zwei Vorteile hat der Phi-Koeffizient jedoch:\n",
        "    - Es muss keine Festlegung erfolgen, ob die Raucherhäufigkeit zwischen den Geschlechtern, oder die Geschlechterhäufigkeit zwischen Rauchern und Nichtrauchern berichtet wird.\n",
        "    - Der Phi-Koeffizient kann in Metaanalysen mit anderen Studien verglichen werden, die eine ähnliche Fragestellung mit einer linearen Pearson-Korrelation bemessen haben.\n",
        "- Zudem macht der Phi-Koeffizient in konzeptioneller Hinsicht den Punkt zu Beginn der Vorlesung deutlich, dass jeder Unterschied (z.B. Raucherhäufigkeit bei Männern versus Frauen) auch als Zusammenhang (Zusammenhang von Geschlecht und Raucherstatus) formuliert werden kann.\n",
        "\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "## Phi-Koeffizient: Beispiel\n",
        "\n",
        "<div class=\"vspace-medium\"></div>\n",
        "\n",
        "![](images/vierfeldertafel_beispiel.png){height=220px}\n",
        "\n",
        "\\begin{align}\n",
        "\\phi &= \\frac{ad-bc}{\\sqrt{(a+b)(c+d)(a+c)(b+d)}} = \\\\\n",
        "    &= \\frac{2\\cdot11-8\\cdot12}{\\sqrt{(2+8)(12+11)(2+12)(8+11)}} = \\\\\n",
        "    &= \\frac{22-96}{\\sqrt{10\\cdot23\\cdot14\\cdot19}} = \\frac{-74}{\\sqrt{61180}} = -0.299\n",
        "\\end{align}\n",
        "\n",
        "<div class=\"vspace-medium\"></div>\n",
        "\n",
        "::: {.notabene}\n",
        ":::: {.columns}\n",
        "::: {.column width=\"7%\"}\n",
        "::: {style=\"margin-top: 10px\"}\n",
        "![](images/notabene2.png){height=\"65px\"}\n",
        ":::\n",
        ":::\n",
        "::: {.column width=\"93%\"}\n",
        "**Achtung:** Der Phi-Koeffizient beschreibt einen *Zusammenhang im weiteren Sinne* und kann somit auch als Unterschied konzeptionalisiert werden (Beispiel: *unterscheidet* sich die relative Häufigkeit des Haustierbesitzes zwischen Männern und Frauen?).\n",
        ":::\n",
        "::::\n",
        ":::\n",
        "\n",
        "\n",
        "# Interpretation von Korrelationen\n",
        "\n",
        "<div class=\"vspace-large\"></div>\n",
        "\n",
        "![Bildnachweis^[https://www.patheos.com/blogs/tippling/2017/10/31/spooky-punkins-and-statistical-correlation/]](images/Inferring-Causation-from-Correlation.jpg){height=300px}\n",
        "\n",
        "\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "## Korrelation und Kausalität\n",
        "\n",
        "- Eine positive oder negative Korrelation zeigt an, dass zwei Variablen zusammenhängen &mdash; und auch, dass man *eine Variable aus der anderen (zu einem gewissen Grad) vorhersagen kann*.\n",
        "- Dies **bedeutet jedoch nicht, dass sich die Variablen auch kausal bedingen**\n",
        "- Einerseits **können Korrelationen zufällig zustande kommen** (die Wahrscheinlichkeit von solchen irrtümlichen Befunden untersuchen wir noch genauer beim Thema Signifikanztestung).\n",
        "- .. andererseits können Korrelationen durch dritte Variablen (**Störvariablen**) verursacht sein.\n",
        "- Umgekehrt können Störvariablen auch tatsächlich vorhandene Zusammenhänge unterdrücken &mdash; man spricht dann von **Suppression**; der Korrelationskoeffizient unterschätzt in diesem Fall die tatsächliche Stärke des Zusammenhangs.\n",
        "- Trotz aller Fallstricke bei der Interpretation gilt: eine Korrelation KANN ein **Indiz für Kausalität** sein (wo Kausalität da auch Korrelation, sofern diese nicht durch Störvariablen unterdrückt ist).\n",
        "<!-- <div class=\"vspace-medium\"></div> -->\n",
        "\n",
        ":::{.fragment}\n",
        "![](images/correlation_confounder.png){height=210px}\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "## Korrelation und Kausalität\n",
        "\n",
        "<div class=\"vspace-xlarge\">&nbsp;</div>\n",
        "\n",
        "![](images/correlation_nicolascage.png)\n",
        "\n",
        "![](images/tylergiven.png){.absolute bottom=0 left=0 height=150px}\n",
        "\n",
        "\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "## Korrelation und Kausalität\n",
        "\n",
        "<div class=\"vspace-xlarge\"></div>\n",
        "\n",
        "![](images/correlation_confounder_sharks.png)\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "## Wann ist ein Korrelationskoeffizient groß oder klein?\n",
        "- Pauschal schwer zu beantworten\n",
        "- In der psychologischen Literatur hat sich folgende Konvention/Nomenklatur nach Jacob Cohen^[Cohen J. (1988). Statistical Power Analysis for the Behavioral Sciences. New York, NY: Routledge Academic] eingebürgert:\n",
        "\n",
        ":::{.fragment}\n",
        "<!---  Table --->\n",
        "| Korrelation (r) | Konvention |\n",
        "|-|-|\n",
        "| 0.1 bis 0.3  | \"kleiner\" Effekt |\n",
        "| 0.3 bis 0.5 | \"mittlerer\" Effekt |\n",
        "| > 0.5 | \"großer\" Effekt |\n",
        "\n",
        ": {tbl-colwidths=\"[20, 20]\"}\n",
        ":::\n",
        "\n",
        "- Allerdings fügt Cohen im gleichen Artikel hinzu:\n",
        "\n",
        ":::{.fragment}\n",
        "> These proposed conventions were set forth throughout with much diffidence [Zurückhaltung], qualifications [Bedingungen], and invitations **not to employ** them if possible.\n",
        ":::\n",
        "\n",
        "- Bei der Beurteilung sollte der Korrelationskoeffizient, wie jede Effektgröße, immer in Relation zu typischen Werten im jeweiligen Forschungsfeld gesetzt werden.\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "## [Wann ist ein Korrelationskoeffizient groß oder klein?]{color=\"darkred\"}\n",
        "\n",
        "![](images/kein_klausurstoff.png){.absolute bottom=0 right=0 height=120px}\n",
        "\n",
        "- In einer kürzlichen Metaanalyse von Lovakov Agadullina (2021)^[Lovakov A, Agadullina ER (2021) Empirically derived guidelines for effect size interpretation in social psychology. Eur J Soc Psychol 51:485–504.] wurden typische Effektstärken in der Sozialpsychologie untersucht:\n",
        "\n",
        ":::{.fragment}\n",
        "![](images/lovakov_2021.png){height=400px}\n",
        ":::\n",
        "\n",
        "- Erkenntnis: \"mittlere\" und \"starke\" Korrelationen (definiert als das 50%- und 75%- Quantil)<br> sind in der Realität kleiner als von Cohen angenommen\n",
        "\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "##\n",
        ":::: {.columns}\n",
        "::: {.column width=\"9%\"}\n",
        "::: {style=\"margin-top:-15px\"}\n",
        "![](images/summary.png){width=60px}\n",
        ":::\n",
        ":::\n",
        "::: {.column width=\"91%\"}\n",
        "::: {.summary}\n",
        "- Zusammenhänge werden mit **Kovarianz** und **Korrelation** untersucht.\n",
        "- Die **Kovarianz** ist die Basis der Pearson-Korrelation &mdash; aber sie ist abhängig von den gewählten Einheiten.\n",
        "- Die **Pearson-Korrelation** misst die Linearität eines Zusammenhangs und gilt für intervallskalierte Daten.\n",
        "- Für ordinalskalierte Variablen eignen sich **Rangkorrelationen** wie **Spearmans Rho** oder **Kendalls Tau**: sie messen die Monotonie eines Zusammenhangs.\n",
        "- Sind beide Variablen binär, greift der **Phi-Koeffizient**.\n",
        "- Correlation does not imply causation, Correlation does not imply causation, Correlation does not imply causation, Correlation does not...\n",
        ":::\n",
        ":::\n",
        "::::\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "<!-- ## Korrelation und Kausalität {.blackslide}\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"59%\"}\n",
        "Die Task Force ist immer noch einigermaßen perblex, ob des Zusammenhangs von TikTok-Onlinezeit und Entzündungswerten.\n",
        "\n",
        "- Als erfahrenen Statistiker:innen kennen Sie die **erste Regel der Korrelation**:\n",
        "\n",
        "::: {style=\"color:lightblue; margin: -20px 0 5px 5px; font-family: monospace; padding-left: 14px\"}\n",
        "**Korrelation ist nicht gleich Kausalität**\n",
        ":::\n",
        "::: {style=\"text-align: left\"}\n",
        "(Oder falls Sie den Lateiner heraushängen lassen wollen: *Cum hoc ergo propter hoc*)\n",
        ":::\n",
        ":::\n",
        "::: {.column width=\"41%\"}\n",
        "![](images/streudiagramm.png)\n",
        ":::\n",
        "::::\n",
        "\n",
        "- Es scheint unmöglich, dass virtuelle TikTok-Zeit einen [**kausalen Einfluss**]{color=\"lightblue\"} auf biologische Entzündungswerte hat.\n",
        "- Sie wissen, dass ein häufiger alternativer Grund für eine Korrelation eine dritte Variable ist, die wiederum Einfluss auf beide Variablen der Korrelation hat&mdash; eine [**Störvariable**]{color=\"lightblue\"}.\n",
        "- In diesem Fall vermuten Sie, dass die Störvariable eine vorhandene Paradoxie-Erkrankung selbst ist: die Erkrankung führt einerseits zu höheren Entzündungswerten und andererseits zu vermehrter TikTok-Zeit (um Trost bei Leidensgenossen zu erfahren?) -->\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " <!-- ```{python}\n",
        "\n",
        "```   -->\n",
        "\n",
        " <!-- ```{python}\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from scipy.stats import linregress, pearsonr\n",
        "\n",
        "np.random.seed(0)\n",
        "root = '/home/matteo/OneDrive/lehre/Statistik/stats1_lecture/book/'\n",
        "df = pd.read_csv(os.path.join(root, 'data', 'paradoxia.csv'))\n",
        "data_tiktok_paradoxia = df[df.group == 2]['hours_tiktok_per_day'].values\n",
        "data_inflam_paradoxia = df[df.group == 2]['inflammation'].values\n",
        "\n",
        "linrg = linregress(data_tiktok_paradoxia, data_inflam_paradoxia)\n",
        "r, p = pearsonr(data_tiktok_paradoxia, data_inflam_paradoxia)\n",
        "print(linrg)\n",
        "\n",
        "fontsize = 15\n",
        "\n",
        "plt.style.use('dark_background')\n",
        "plt.figure(figsize=(3, 2.5))\n",
        "plt.xticks(fontsize=fontsize-2)\n",
        "plt.yticks(fontsize=fontsize-2)\n",
        "plt.xlabel('Stunden TikTok / 24h', fontsize=fontsize)\n",
        "plt.ylabel('Entzündungswerte', fontsize=fontsize)\n",
        "plt.scatter(data_tiktok_paradoxia, data_inflam_paradoxia)\n",
        "plt.plot([0.1, 3.1], linrg.intercept + linrg.slope * np.array([0.1, 3.1]), color='#ff00ff', lw=2)\n",
        "plt.text(0.2, 0.3, r'$\\bf{r=' + f'{r:.3f}' + r'}$', color='#ff00ff', fontsize=fontsize-2)\n",
        "plt.xlim(0, 3.2)\n",
        "plt.ylim(0, 0.34)\n",
        "# plt.legend()\n",
        "plt.savefig('images/paradoxia_histogram_correlation_paradoxiker.png', bbox_inches='tight')\n",
        "```  -->\n",
        "\n",
        "## {.blackslide .center}\n",
        "\n",
        "<div class=\"vspace-medium\"></div>\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"65%\"}\n",
        "Zurück zum Zusammenhang zwischen TikTok-Online-Zeit und Entzündungswerten. Beides sind kontinuierliche intervallskalierte Variablen, Sie können also den Pearson-Korrelationskoeffizienten anwenden. Sie stellen den Zusammenhang in Form eines [**Streudiagramms**]{.navy-paradoxia} dar:\n",
        ":::\n",
        "::: {.column width=\"35%\"}\n",
        "::: {.content-hidden when-format=\"pdf\"}\n",
        "![](images/paradoxia_group_of_scientists.png){.hcenter-image height=200px style=\"margin-top:-100px !important\"}\n",
        "<!-- Source: Midjourney -->\n",
        ":::\n",
        ":::\n",
        "::::\n",
        "\n",
        "<div class=\"vspace-large\"></div>\n",
        "![](images/paradoxia_histogram_correlation_paradoxiker.png){height=400px}\n",
        "\n",
        "Faszinierend. Es gibt tatsächlich einen Zusammenhang beider Variablen. Kann das Ergebnis so interpretiert werden, dass TikTok sich auf physiologische Entzündungswerte auswirkt?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "## [Warum ist die Korrelation auf &minus;1 bis 1 beschränkt?]{color=\"darkred\"}\n",
        "\n",
        "![](images/kein_klausurstoff.png){.absolute bottom=0 right=0 height=110px}\n",
        "\n",
        "Darstellung der Korrelation nur mit (Ko)Varianzen:\n",
        "\n",
        "$$\n",
        "r_{XY} = \\frac{Cov(X,Y)}{s_X s_Y} = \\frac{Cov(X,Y)}{\\sqrt{Var(X)} \\sqrt{Var(Y)}}\n",
        "$$\n",
        "\n",
        "Die Korrelation sollte maximal ($r=1$) sein, wenn $X$ mit sich selbst korreliert wird ($Y=X$). Zu berücksichtigen ist, dass die Kovarianz *von X mit sich selbst* gleich der Varianz ist:\n",
        "\n",
        "$$\n",
        "r_{XX} = \\frac{Cov(X,X)}{\\sqrt{Var(X)} \\sqrt{Var(X)}} = \\frac{Var(X)}{\\sqrt{Var(X)} \\sqrt{Var(X)}} = \\frac{Var(X)}{Var(X)} = 1\n",
        "$$\n",
        "\n",
        "\n",
        "Umgekehrt sollte die Korrelation maximal negativ sein ($r=-1$), wenn $Y$ genau das Inverse von $X$ ist, also $Y=-X$. Unter Berücksichtigung von $Var(X) = Var(-X)$ gilt:\n",
        "$$\n",
        "r_{X(-X)} = \\frac{Cov(X,-X)}{\\sqrt{Var(X)} \\sqrt{Var(-X)}} = \\frac{-Cov(X,X)}{\\sqrt{Var(X)} \\sqrt{Var(X)}} = \\frac{-Var(X)}{Var(X)} = -1\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "## [Punktbiseriale und biseriale Korrelation]{color=\"darkred\"}\n",
        "\n",
        "![](images/kein_klausurstoff.png){.absolute top=40 right=30 height=110px}\n",
        "\n",
        "- Korrelationskoeffizienten können auch berechnet werden, wenn eine<br>der beiden Variablen binär ist. Zwei Fälle werden unterschieden.\n",
        "\n",
        "\n",
        ":::{.fragment}\n",
        "**Fall 1: die binäre Variable liegt natürlicherweise in dichotomer Form vor (z.B. männl./weibl.)**\n",
        ":::\n",
        "\n",
        ":::{.fragment}\n",
        "In diesem Fall haben die beiden Werte der binären Variable *keine natürliche Ordnung* und es wird der [**punktbiseriale Korrelationskoeffizient**]{color=\"navy\"} verwendet.\n",
        ":::\n",
        "\n",
        ":::{.fragment}\n",
        "Sei $X$ die binäre Variable und $Y$ die kontinuierliche Variable. Wir teilen die Stichprobe in Gruppe $A$ (für die $X$ den einen Wert einnimmt) und Gruppe $B$ (für die $X$ den anderen Wert einnimmt). Es gilt:\n",
        "\n",
        ":::{style=\"margin-top: 0px\"}\n",
        "$$\n",
        "r_\\text{population} = \\frac{\\left(\\bar{y}_A - \\bar{y}_B\\right)}{\\sigma_Y}\\sqrt{p_A p_B} \\qquad\\qquad r_\\text{sample} = \\frac{\\left(\\bar{y}_A - \\bar{y}_B\\right)}{s_Y}\\sqrt{\\frac{n p_A p_B}{n-1}}\n",
        "$$\n",
        ":::\n",
        "\n",
        "wobei $\\bar{y}_{A/B}$ der Mittelwert von $Y$ und $p_{A/B}$ der Anteil (proportion) der Versuchspersonen in Gruppe $A$/$B$ ist. $\\sigma_Y$/$s_Y$ ist die Standardabweichung von $Y$ in der Population/Stichprobe.\n",
        ":::\n",
        "\n",
        "<div class=\"vspace-medium\"></div>\n",
        "\n",
        "::: {.merke .fragment}\n",
        ":::: {.columns}\n",
        "::: {.column width=\"5%\"}\n",
        "::: {style=\"margin-top: 18px\"}\n",
        "![](images/merke.png){height=\"55px\"}\n",
        ":::\n",
        ":::\n",
        "::: {.column width=\"95%\"}\n",
        "**Beachte:** der punktbiserielle Korrelationskoeffizient leitet sich direkt aus dem Pearson-Korrelationskoeffizienten ab (ist äquivalent), d.h. die Pearsonkorrelation einschlägiger Statistiksoftware kann verwendet werden!\n",
        ":::\n",
        "::::\n",
        ":::\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "## [Punktbiseriale und biseriale Korrelation]{color=\"darkred\"}\n",
        "\n",
        "![](images/kein_klausurstoff.png){.absolute bottom=0 right=0 height=110px}\n",
        "\n",
        "**Fall 2: die binäre Variable resultiert aus der Dichotomisierung einer kontinuierlichen Variable**\n",
        "\n",
        ":::{.fragment}\n",
        "In diesem Fall unterschätzt die punktbiseriale Korrelation den wahren Wert und es sollte der [**biseriale Korrelationskoeffizient**]{color=\"navy\"} verwendet:\n",
        "\n",
        ":::{style=\"margin-left: -30px\"}\n",
        "$$\n",
        "r_\\text{population} = \\frac{\\left(\\bar{y}_A - \\bar{y}_B\\right)}{f(z_p)\\sigma_Y}p_A p_B \\qquad\\qquad r_\\text{sample} = \\frac{\\left(\\bar{y}_A - \\bar{y}_B\\right)}{f(z_p)s_Y}\\frac{n p_A p_B}{n-1}\n",
        "$$\n",
        ":::\n",
        "\n",
        "wobei $f(z_p)$ der Wert der Standardnormalverteilung bei $z_p$ ist und $z_p$ der Wert, bei dem die Fläche rechts unter der Standardnormalverteilung gleich $p$ ist ($P(Z>z_p)=p$).\n",
        "\n",
        "<!-- Antwort auf \n",
        "https://real-statistics.com/correlation/biserial-correlation/comment-page-1/?unapproved=1505005&moderation-hash=fba191d84ae6d8b2252c6b13efb49ba1#comment-1505005\n",
        "checken! -->\n",
        "\n",
        "\n",
        "Quellen: ^[https://real-statistics.com/correlation/biserial-correlation/] ^[https://www.ncss.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Point-Biserial_and_Biserial_Correlations.pdf] ^[Jacobs P, Viechtbauer W (2017) Estimation of the biserial correlation and its sampling variance for use in meta-analysis: Biserial Correlation. Res Syn Meth 8:161–180.]\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "::: {.merke .fragment}\n",
        ":::: {.columns}\n",
        "::: {.column width=\"5%\"}\n",
        "::: {style=\"margin-top: 18px\"}\n",
        "![](images/merke.png){height=\"55px\"}\n",
        ":::\n",
        ":::\n",
        "::: {.column width=\"95%\"}\n",
        "**Beachte:** der biserielle Korrelationskoeffizient ist [nicht]{.underline} äquivalent mit dem Pearson-Korrelationskoeffizienten und sollte manuell berechnet werden.\n",
        ":::\n",
        "::::\n",
        ":::\n"
      ],
      "id": "8df301d7"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}