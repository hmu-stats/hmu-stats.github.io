---
title: "Vorlesung 06: Regression"
---


## {.blackslide .center}

<div class="vspace-medium"></div>

:::: {.columns}
::: {.column width="65%"}
Kurze Erinnerung: beim letzten Mal fanden wir einen Zusammenhang von TikTok-Online-Zeit und Entzündungsparametern:
:::
::: {.column width="35%"}
::: {.content-hidden when-format="pdf"}
![](images/paradoxia_group_of_scientists.png){.hcenter-image height=200px style="margin-top:-100px !important"}
<!-- Source: Midjourney -->
:::
:::
::::

<div class="vspace-large"></div>
![](images/paradoxia_histogram_correlation_paradoxiker.png){height=300px}

Bei der Interpretation stellt sich einerseits die Kausalitätsfrage, andererseits, wie stark der Zusammenhang tatsächlich ist. Da die Pearson-Korrelation lediglich den **Grad der Linearität** beurteilt, fragen Sie sich: um wie viel erhöhen sich die Entzündungsparameter pro Stunde zusätzliche Zeit auf TikTok? Oder umgekehrt: um wie viel erhöht sich die Zeit auf TikTok, wenn die Entzündungswerte um einen Wert x ansteigen?


<!-- 
## Messwiederholungen: Liniendiagramme{.blackslide}

Gerade als sich die Hinweise auf die Hypothese des viralen Ursprungs von Paradoxia verdichten, wird ein Blogpost des Chaos Computer Club (CCC) in der Öffentlichkeit bekannt. Anonymen Hackern gelang es, auf die letzten 12 Monate Tik-Tok-Historie von 1800 Personen zuzugreifen &ndash; darunter viele Paradoxiker!

<div class="vspace-medium"></div>

![Die Abbildung zeigt ein Liniendiagramm mit Fehlerbalken. Bei wiederholten Messungen Personen bietet es sich an, die einzelnen Messzeitpunkte mit Linien zu verbinden, um den zeitlichen Zusammenhang zu unterstreichen. Überlegen Sie sich: welche Aussage macht ein einzelner Fehlerbalken in dieser Abbildung?](images/paradoxia_lineplot.png) -->

<!-- ```{python}
import numpy as np
import matplotlib.pyplot as plt
fontsize=15
np.random.seed(0)

plt.style.use('dark_background')
plt.figure()

x = np.arange(12)
y_paradoxia = 0.2 * x - 0.3 + 1*np.random.rand(12)
yerr_paradoxia = 0.1*np.random.rand(12)+0.1
y_control = 0.03 * x + 0.15 + 0.2*np.random.rand(12)
yerr_control = 0.1*np.random.rand(12)+0.1

plt.plot(x, y_paradoxia, color='#beddff', lw=2)
plt.errorbar(x, y_paradoxia, yerr=yerr_paradoxia, fmt='o', markersize=5, capsize=10, capthick=2, elinewidth=2, ecolor='#beddff', mfc='#beddff', mec='#beddff', label='Paradoxia')
plt.plot(x, y_control, color='#f5f5b9', lw=2)
plt.errorbar(x, y_control, yerr=yerr_control, fmt='o', markersize=5, capsize=10, capthick=2, elinewidth=2, ecolor='#f5f5b9', mfc='#f5f5b9', mec='#f5f5b9', label='Kontroll')
plt.xlabel('Monate', fontsize=fontsize)
plt.xticks(x, x+1, fontsize=fontsize-2)
plt.yticks(fontsize=fontsize-2)
plt.ylabel('Stunden Tik Tok / 24h', fontsize=fontsize)
# plt.xlim(xlim)
# plt.ylim(ylim)
plt.legend(fontsize=fontsize)
plt.savefig('images/paradoxia_lineplot.png', bbox_inches="tight",
            pad_inches=0)
``` -->


<!----------------->
<!--- New slide --->
<!----------------->
<!-- ## {.blackslide .center}

::: {.content-hidden when-format="pdf"}
:::: {.columns}
::: {.column width="50%"}
![](images/paradoxia_group_of_scientists.png){.hcenter-image height=250px}
:::
::: {.column width="50%"}
![](images/paradoxia_lineplot.png)
:::
::::
:::
<div class='vspace-small'></div>

Die Daten des CCC geben der alternativen Hypothese neue Nahrung, nach der es sich bei Paradoxia um ein soziales Tik-Tok-Phänomen handelt. Leider ist nicht bekannt, zu welchem Zeitpunkt die Paradoxiker in dem Datensatz erkrankt sind. 
Die erste Frage an Sie als Task Force lautet: welche Möglichkeiten von Ursache und Wirkung könnten plausibel sein? -->


# Regression

![Bildnachweis^[https://flowingdata.com/2014/06/25/duck-vs-rabbit-plot/]](images/rabbit_duck.jpg)


<!-- ## Der Forschungsprozess {.hcenter-slide}

```yaml { .animate src="images/scientific_process.svg"}
setup:
    - element: "#results"
      modifier: function() { this.node.style.fill = 'green'; }
    - element: "#resultsbg"
      modifier: function() { this.node.style.fill = '#d8ffe2';}
``` -->


## Woher kommt der Ausdruck "Regression"?

- Lateinisch »regredi« = „umkehren, zurückgehen“
- Psychoanalyse: Regression = Zurückfallen in kindliche Verhaltensmuster

:::{.fragment}
> Wir heißen es **Regression**, wenn sich im Traum die Vorstellung in das sinnliche Bild zurückverwandelt, aus dem sie irgend einmal hervorgegangen ist.

[Sigmund Freud (1900). "Traumdeutung".]{.quote-citation}
:::

:::: {.columns}
::: {.column width="70%"}
- In die Statistik wird der Ausdruck "Regression" klassischerweise auf **Francis Galton** (Cousin von Charles Darwin) attribuiert, der bereits 1885 ein Phänomen beschrieb, das er *regression toward mediocrity* (**Regression zur Mitte**) taufte
- Das Phänomen bestand darin, dass Nachfahren großer Eltern dazu tendieren, selbst nur durchschnittlich groß zu werden
- Neuere Forschung zeigt allerdings, dass sich Galton selbst wohl noch nicht des statistischen Ursprungs dieses Phänomens bewusst war und eine biologische Erklärung favorisierte^[Krashniak A, Lamm E (2021) Francis Galton’s regression towards mediocrity and the stability of types. Studies in History and Philosophy of Science Part A 86:6–19.].
:::
::: {.column width="30%"}
:::{.fragment}
![](images/galton.png)
:::
:::
::::

## Regression

- Dem Wortsinn nach ist Ziel der [**Regression**]{color="navy"} eine abhängige Variable auf eine oder mehrere unabhängige Variablen zurückzuführen (auf diese zu *regredieren*).
- Eingängiger ist aber die umgekehrte Formulierung: Ziel der Regression ist es, auf Basis der unabhängigen Variablen die abhängige Variable **vorherzusagen** oder **zu erklären**:
    - **Unabhängige Variable(n) = vorher*sagende* oder erklär*ende* Variable(n)** ("**U**rsache").
    - **Abhängige Variable = vorher*gesagte* oder erklär*te* Variable** ("**A**uswirkung").

:::{.fragment}
![](images/regression_basics_dach.png){height=300px}
:::

::: {style="margin-top: -5px"}
- Beispiel: Studie untersucht Zusammenhang von Lebenszufriedenheit und sportlicher Aktivität.
    - **Lebenszufriedenheit:** unabhängige/vorhersagende/erklärende Variable.
    - **Sportliche Aktivität:** abhängige/vorhergesagte/erklärte Variable.
:::

## Regression

:::: {.columns}
::: {.column width="70%"}
- Im Gegensatz zur Korrelation bestimmt die Regression nicht die Linearität des Zusammenhangs (vielmehr wird dies vorausgesetzt), sondern die **Steigung** des Zusammenhangs.
- Aus diesem Grund ist die Regression (wieder im Gegensatz zur Korrelation) nicht symmetrisch &ndash; die Steigung ist abhängig davon welche Variable als abhängig und unabhängig deklariert wird.
    - Wie wir noch sehen werden, ist es auch nicht gestattet, die Regressionsgleichung zu invertieren ( $x_i = \frac{1}{\hat{b}_1}\hat{y}_i-\frac{\hat{b}_0}{\hat{b}_1}$) &mdash; im Allgemeinen ist $\frac{1}{\hat{b}_1}$ *nicht* die Steigung, wenn die Rollen von $X$ und $Y$ vertauscht werden.
:::
::: {.column width="30%"}
![](images/regression_basics_plot_dach.png)
:::
::::

:::: {.columns}
::: {.column width="60%"}
- Die Vorhersage/Erklärung von $X$ durch $Y$ geschieht durch eine Gleichung &ndash; die **Regressionsgleichung** &ndash; die im Streudiagramm als Gerade eingezeichnet werden kann.
:::
::: {.column width="40%"}
::: {style="margin-top: -25px"}
:::{.fragment}
![](images/regression_sports.png)
:::
:::
:::
::::



## [Bestimmung der Regressionsgerade: Methode der kleinsten Quadrate]{style="font-size: 37px"}
:::: {.columns}
::: {.column width="58%"}
- Ziel der Regression ist es, die Gerade zu finden, die die Datenpunkte möglichst gut abbildet &mdash; es gibt jedoch verschiedene Definitionen dessen, was  "möglichst gut" heißt.
- Die häufigste Variante ist die [**Methode der kleinsten Quadrate**]{color="navy"}, bei der die Gerade so gewählt wird, dass die Summe der quadrierten [**senkrechten Abstände**]{color="darkmagenta"} jedes Datenpunktes zur Geraden minimal ist.
    - Engl. *ordinary least square*
:::
::: {.column width="42%"}
![](images/regression_sports_distances_dach.png)
:::
::::
- Die **einfache Regression** mit nur einer unabhängigen Variablen hat zwei freie Parameter, um die Gerade an die Datenpunkte anzupassen (zu "fitten"):
    - **y-Achsenabschnitt** $\hat{b}_0$ (engl. *intercept*)
    - **Steigung** $\hat{b}_1$ (engl. *slope*)
- Die senkrechten Abstände der Datenpunkte von der gefitteten Geraden werden [**Residuen**]{color="navy"} genannt.
- Exakt 0 wären die senkrechten Abstände nur, wenn alle Punkte auf einer perfekten Gerade liegen.

## Warum weichen die Datenpunkte überhaupt von einer Geraden ab?
:::: {.columns}
::: {.column width="60%"}
Verschiedene Gründe:

- Variablen hängen gar nicht zusammen
- Zusammenhang ist nichtlinear
- Einfluss von Störvariablen
- Messungenauigkeit
:::
::: {.column width="40%"}
::: {style="margin-top: -32px"}
![](images/regression_weight_height_annot.png)
:::
:::
::::

:::{.fragment}
In der Psychologie gibt es (bis auf triviale Fälle) keine perfekten linearen Zusammenhänge, d.h. es verbleiben immer **Residuen** $\Delta \hat{y_i}$:

$$
\text{Residuum:}\quad \Delta \hat{y_i} = \hat{\epsilon}_i = \hat{y_i} - y_i
$$
:::

::: {.merke .fragment style="margin-top: 52px"}
:::: {.columns}
::: {.column width="5%"}
::: {style="margin-top: 18px"}
![](images/merke.png){height="55px"}
:::
:::
::: {.column width="95%"}
Residuum = Differenz von vorhergesagtem Wert $\hat{y_i}$ und tatsächlichem Wert $y_i$
:::
::::
:::



<!----------------->
<!--- New slide --->
<!----------------->
<!-- :::{.content-hidden when-format="pdf"}
## [https://shiney.zoology.ubc.ca/whitlock/Guessing_correlation/]{style="font-size: 37px"}

<iframe width=100% height="100%" src="https://shiney.zoology.ubc.ca/whitlock/Guessing_correlation/"></iframe>
::: -->

<!-- https://www.guessthecorrelation.com/ -->

## Streudiagramm bei Regression

- in der Regel UV auf der x-Achse und AV auf der y-Achse

:::{.fragment}
![](images/regression_scatter.png){height=500px style="margin-top: 20px !important"}
:::


<!----------------->
<!--- New slide --->
<!----------------->
:::{.content-hidden when-format="pdf"}
## [https://phet.colorado.edu/sims/html/least-squares-regression/latest/least-squares-regression_all.html?locale=de]{style="font-size:22px"}
<iframe width=100% height=100% src="https://phet.colorado.edu/sims/html/least-squares-regression/latest/least-squares-regression_all.html?locale=de"></iframe>
:::


<!----------------->
<!--- New slide --->
<!----------------->
<!-- :::{.content-hidden when-format="pdf"}
## [https://phet.colorado.edu/sims/html/least-squares-regression/latest/least-squares-regression_all.html?locale=de]{style="font-size:22px"}
<iframe width=100% height=100% src="https://phet.colorado.edu/sims/html/least-squares-regression/latest/least-squares-regression_all.html?locale=de"></iframe>
::: -->



<!----------------->
<!--- New slide --->
<!----------------->
<!-- ## y-Achsenabschnitt $b_0$

[ToDo] Einige Beispielspielplots für b_0 -->


<!----------------->
<!--- New slide --->
<!----------------->
<!-- ## Steigung $b_1$

[ToDo] Einige Beispielspielplots für b_1 -->