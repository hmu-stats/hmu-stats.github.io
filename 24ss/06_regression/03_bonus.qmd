
# [Bonuscontent]{color="darkred"}

<!----------------->
<!--- New slide --->
<!----------------->
## [Herleitung der Regressionskoeffizienten]{color="darkred"}


- Die Methode der kleinsten Quadrate entspricht der Minimierung der quadratischen Residuen:
$$
SQR = \sum\left(\hat{y}_i-y_i\right)^2 = \sum\left(\hat{b}_0+\hat{b}_1 x_i-y_i\right)^2 \overset{!}{=}\text{min}
$$
- Um das Minimum von SQR in Abhängigkeit von $\hat{b}_0$ und $\hat{b}_1$ zu finden, setzen wir die Ableitungen von SQR nach den Parametern gleich Null (Infinitesimalrechnung@Schule &#128521;)
- Zunächst leiten wir SQR nach $\hat{b}_0$ ab (Kettenregel):
$$
\frac{dSQR}{d\hat{b}_0} = \sum2\left(\hat{b}_0+\hat{b}_1 x_i-y_i\right)=2n\hat{b}_0+2\sum\left(\hat{b}_1 x_i-y_i\right)=0
$$

::: {.fragment style="border: 1px solid darkblue"}
$$
\color{darkblue}{\longrightarrow \hat{b}_0} = \frac{1}{n}\sum\left(y_i-\hat{b}_1x_i\right) = \frac{1}{n}\sum y_i-\frac{\hat{b}_1}{n}\sum x_i = \color{darkblue}{\bar{y}-\hat{b}_1\bar{x}}
$$
:::

<div class="vspace-xlarge"></div>

- ... jetzt benötigen wir noch $\hat{b}_1$

## [Herleitung der Regressionskoeffizienten]{color="darkred"}


:::{style="margin-top: -10px !important"}
- SQR nach $\hat{b}_1$ ableiten und gleich Null setzen:
:::

:::{.fragment}
$$
\small{
\frac{dSQR}{d\hat{b}_1} = \sum2(\hat{b}_0+\hat{b}_1 x_i-y_i)x_i=2\hat{b}_0\sum x_i + 2\hat{b}_1\sum x_i^2-2\sum x_i y_i = 0
}
$$
:::

:::{.fragment style="margin-top: -10px !important"}
$$\small{
\longrightarrow \hat{b}_1 = \frac{\sum x_i y_i}{{\sum x_i^2}} - \frac{\hat{b}_0\sum x_i}{\sum x_i^2} \overset{\color{darkgreen}{(\hat{b}_0\,einsetzen)}}{=} \frac{\sum x_i y_i}{{\sum x_i^2}} - \frac{\bar{y}\sum x_i}{\sum x_i^2}+\hat{b}_1\frac{\bar{x}\sum x_i}{\sum x_i^2}
}
$$
:::

- Alle $\hat{b}_1$-Terme auf die linke Seite bringen und einige Umformungen vornehmen:

::: {.fragment style="margin-top: -12px"}
$$
\small{
\hat{b}_1 - \hat{b}_1\frac{\bar{x}\sum x_i}{\sum x_i^2} = \frac{\sum x_i y_i}{{\sum x_i^2}} - \frac{\bar{y}\sum x_i}{\sum x_i^2}
}
$$
:::


:::{.fragment}
$$
\scriptsize{
b_1 \left(1 - \frac{\bar{x}\sum x_i}{\sum x_i^2}\right) = \frac{\sum x_i y_i - \bar{y}\sum x_i}{\sum x_i^2}
}
$$ 
:::

 
:::{.fragment}
$$
\scriptsize{
b_1 \left(\frac{\sum x_i^2}{\sum x_i^2} - \frac{\bar{x}\sum x_i}{\sum x_i^2}\right) = \frac{\sum x_i y_i - \bar{y}\sum x_i}{\sum x_i^2}
}
$$
:::


:::{.fragment}
$$
\scriptsize{
\hat{b}_1 \frac{\sum x_i^2 - \bar{x}\sum x_i}{\sum x_i^2} = \frac{\sum x_i y_i - \bar{y}\sum x_i}{\sum x_i^2}
}
$$
:::


::: {.fragment style="margin-left: -100px"}
$$
\scriptsize{
\longrightarrow    
\hat{b}_1 = \frac{\color{darkred}{\sum x_i^2}}{\sum x_i^2-\bar{x}\sum x_i}\frac{\sum x_i y_i - \bar{y}\sum x_i}{\color{darkred}{\sum x_i^2}} = \frac{\sum x_i y_i - \bar{y}\sum x_i}{\sum x_i^2-\bar{x}\sum x_i}
\overset{\color{darkgreen}{\left(\sum x_i=n\bar{x}\right)}}{=} \frac{\sum x_i y_i - n\bar{x}\bar{y}}{\sum x_i^2-n\bar{x}^2} \overset{\color{darkgreen}{:n}}{=} \frac{\frac{1}{n}\sum x_i y_i - \bar{x}\bar{y}}{\frac{1}{n}\sum x_i^2-\bar{x}^2}
}
$$
:::




## [Herleitung der Regressionskoeffizienten]{color="darkred"}


- Zwischenergebnis:

::: {.fragment style="margin-top:-35px"}
$$
\hat{b}_1 = \frac{\frac{1}{n}\sum x_i y_i - \bar{x}\bar{y}}{\frac{1}{n}\sum x_i^2-\bar{x}^2}
$$
:::

- Um zu erkennen, dass der Zähler der Kovarianz und der Nenner der Varianz entspricht, betrachten wir nochmal die Formeln der (Ko)Varianz:


:::{.fragment}
$$
\scriptsize{
\begin{aligned}
Cov(X,Y) &= \frac{1}{n}\sum(x_i-\bar{x})(y_i-\bar{y}) = \frac{1}{n}\sum\left(x_iy_i-x_i\bar{y}-y_i\bar{x}+\bar{x}\bar{y}\right) = 
\frac{1}{n}\sum x_iy_i-
\bar{y}\frac{1}{n}\sum x_i-
\bar{x}\frac{1}{n}\sum y_i+
\bar{x}\bar{y} = \\
&\underset{\color{darkgreen}{\left(\frac{1}{n}\sum y_i=\bar{y}\right)}}{\overset{\color{darkgreen}{\left(\frac{1}{n}\sum x_i=\bar{x}\right)}}{=}} \frac{1}{n}\sum x_iy_i - \bar{y}\bar{x}-\bar{x}\bar{y}+\bar{x}\bar{y} = \frac{1}{n}\sum x_iy_i - \bar{x}\bar{y}
\end{aligned}
}
$$
:::


:::{.fragment}
$$
\scriptsize{
\begin{aligned}
Var(X) &= \frac{1}{n}\sum\left(x_i-\bar{x}\right)^2 = \frac{1}{n}\sum\left(x_i^2-2x_i\bar{x}+\bar{x}^2\right) = \frac{1}{n}\sum x_i^2-2\bar{x}\frac{1}{n}\sum x_i+\bar{x}^2 = \\
 &\overset{\color{darkgreen}{\left(\frac{1}{n}\sum x_i=\bar{x}\right)}}{=} \frac{1}{n}\sum x_i^2-2\bar{x}^2+\bar{x}^2 = \frac{1}{n}\sum x_i^2-\bar{x}^2 
\end{aligned}
}
$$
:::


- Es gilt also tatsächlich:

::: {.fragment style="margin-top:-20px"}
$$
\color{darkblue}{b_1 = \frac{Cov(X,Y)}{Var(X)}}
$$
:::



<!----------------->
<!--- New slide --->
<!----------------->
## [Beweis, dass $R^2=\hat{\rho}^2$ bei einfacher Regression]{color="darkred"}

- Ausgestattet mit der Formel für den Regressionskoeffizienten, lässt sich nun auch beweisen, dass bei der einfachen Regression gilt: $\quad \color{darkblue}{R^2=\hat{\rho}^2}$


:::{.fragment}
$$
\small{
\begin{aligned}
R^2 = \frac{Var(\hat{Y})}{Var(Y)} &= \frac{\frac{1}{n}\sum\left(\hat{y}_i-\bar{y}\right)^2}{Var(Y)}
 \underset{\color{darkgreen}{\left(\bar{y} = \hat{b}_0+\hat{b}_1\bar{x}\right)}}{\overset{\color{darkgreen}{\left(\hat{y}_i = \hat{b}_0+\hat{b}_1x_i\right)}}{=}} \frac{\frac{1}{n}\sum\left[(\hat{b}_0+\hat{b}_1x_i)-(\hat{b}_0+\hat{b}_1\bar{x})\right]^2}{Var(Y)} = \frac{\frac{1}{n}\sum\left(\hat{b}_1x_i-\hat{b}_1\bar{x}\right)^2}{Var(Y)} = \\
&= \hat{b}_1^2\frac{\frac{1}{n}\sum\left(x_i-\bar{x}\right)^2}{Var(Y)}= \hat{b}_1^2\frac{Var(X)}{Var(Y)}\qquad\qquad \scriptsize{\left(\text{NB sieht man, dass gilt:}\quad Var(\hat{Y}) = \hat{b}_1^2Var(X)\right)}
\end{aligned}
}
$$
:::

- Nun $\hat{b}_1 = \frac{Cov(X,Y)}{Var(X)}$ einsetzen:

:::{.fragment}
$$
\small{
\color{darkblue}{R^2} = \frac{Cov^2(X,Y)}{\color{darkred}{Var^2(X)}}\frac{\color{darkred}{Var(X)}}{Var(Y)} = \color{darkblue}{\frac{Cov^2(X,Y)}{Var(X)Var(Y)}}
}
$$
:::

- Vergleiche mit $\hat{\rho}^2$:

:::{.fragment}
$$
\small{
\color{darkblue}{\hat{\rho}^2} = \left[\frac{Cov(X,Y)}{\sigma_{\mkern-2mu\scriptscriptstyle{X}}\sigma_{\mkern-2mu\scriptscriptstyle{Y}}}\right]^2 = \frac{Cov^2(X,Y)}{\sigma_{\mkern-2mu\scriptscriptstyle{X}}^2\sigma_{\mkern-2mu\scriptscriptstyle{Y}}^2} = \frac{Cov^2(X,Y)}{Var(X)Var(Y)}
}
$$
:::

