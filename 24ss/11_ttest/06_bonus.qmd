# [Bonuscontent]{color="darkred"}



<!----------------->
<!--- New slide --->
<!----------------->
## [t-Test & Normalverteilung]{color="darkred"}{#tnormal}

![](images/kein_klausurstoff.png){.absolute bottom=0 right=0 height=110px}

:::: {.columns}
::: {.column width="43%"}

**Warum ist die Normalverteilung des Merkmals $X$ eine (theoretische) Vorausetzung für den t-Test?** 

Eigentlich hatten wir ja festgestellt, dass Stichprobenverteilungen der Kennwerte $\hat{\theta}$ *unabhängig von der Verteilung des Merkmals* $X$ normalverteilt sind (vgl. zentraler Grenzwertsatz / Vorlesung 08).

:::
::: {.column width="57%"}
![](images/central_limit_theorem_average.png){height=225px style="margin-top: 15px !important"}
:::
::::

::: {style="margin-top: 8px"}
:::{.fragment}
[Ein Grund:]{.underline} damit $t=\hat{\theta}/\hat{se}$ einer t-Verteilung folgt, müssen der Stichprobenkennwert $\hat{\theta}$ und sein Standardfehler $\hat{se}$ unabhängig sein (dürfen nicht korrelieren). Dies ist genau dann erfüllt, *wenn das gemessene Merkmal $X$ einer Normalverteilung folgt* &mdash; tatsächlich lässt sich genau mit dieser Bedingung die Normalverteilung ableiten. Insofern wirkt sich hier die Merkmalsverteilung von $X$ doch auf die Stichprobenverteilung von $\hat{\theta}$ aus.
<!-- - Der zentrale Grenzwertsatz für die Stichprobenverteilung von $\hat{\theta}$ gilt streng genommen nur für $n\rightarrow\infty$, was in empirischen Studien nie gegeben ist. Ist die Stichprobengröße $n$ begrenzt, kann die Stichprobenverteilung von $\hat{\theta}$ nur dann als normalverteilt angenommen werden, wenn auch das gemessene Merkmal $X$ in der Population normalverteilt ist ([Satz von Cramer](https://de.wikipedia.org/wiki/Satz_von_Cram%C3%A9r_(Normalverteilung))). -->
:::

:::

:::{.fragment style="margin-top: 35px"}
**Aber wie erwähnt: trotz dieses theoretischen Fallstricks ist der t-Test in der Praxis<br>auch bei nicht-normalverteilten Populationsvariablen $X$ recht robust.**
:::



<!----------------->
<!--- New slide --->
<!----------------->
## [Die t-Verteilung ist bereits standardisiert]{color="darkred"}

![](images/kein_klausurstoff.png){.absolute top=295 right=0 height=100px}

Vor der Einführung des z-Tests hatten wir zunächst die **unstandardisierte Normalverteilung** kennengelernt, die durch zwei Parameter definiert ist: Mittelwert $\mu$ und Standardabweichung $\sigma$:

$$
f(x)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}
$$

Verwenden wir statt $x$ die standardisierte Variable $\frac{x-\mu}{\sigma}$, vereinfacht sich die Nullhypothesen-Verteilung zur **Standardnormalverteilung**:

:::{style="margin-top: -10px"}
$$
f(x)=\varphi(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}
$$
:::

Die Standardnormalverteilung hat keinen Parameter mehr (nur noch die *Variable* $x$). Da es sich beim z-Wert auch um eine standardisierte Variable handelte, konnten wir auch für $z$ eine Standardnormalverteilung annehmen:

:::{style="margin-top: -10px"}
$$
f(z)=\varphi(z)=\frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}}
$$
:::

Die **t-Verteilung** hatten wir dagegen direkt auf Basis der standardisierten Prüfgröße $\frac{\hat{\theta}}{\hat{se}}$ eingeführt. **Die klassische t-Verteilung ist aus diesem Grund bereits eine standardisierte Verteilung, die nicht mehr vom Mittelwert oder Streuung abhängt.** Im Gegensatz zur Standardnormalverteilung hat sie aber noch einen Parameter: die Zahl der Freiheitsgrade $\text{df}$.


<!----------------->
<!--- New slide --->
<!----------------->
## [Unstandardisierte t-Verteilung]{color="darkred"}

![](images/kein_klausurstoff.png){.absolute top=10 right=0 height=110px}

Die Formel für die klassische (standardisierte) t-Verteilung lautet:

:::{style="margin-top: -7px"}
$$
f_t(x|\scriptsize\text{df}\normalsize) = \frac{\Gamma\left(\frac{\scriptsize\text{df}\normalsize+1}{2}\right)} {\sqrt{\scriptsize\text{df}\normalsize\,\pi}~\Gamma\left(\frac{\scriptsize\text{df}\normalsize}{2}\right)}\left(1+\frac{1}{\scriptsize\text{df}\normalsize}x^{2}\right)^{-\frac{\text{df}+1}{2}}\qquad \left(\Gamma\text{ ist die Gamma-Funktion}\right)
$$
:::

Wie die Standardnormalverteilung hat die t-Verteilung Mittelwert 0 (daher kann sie als Nullhypothesenverteilung fungieren). Ihre Streuung ist jedoch nicht exakt 1, sondern hängt von der Zahl der Freiheitsgrade ab: $\sigma^2=\frac{\scriptsize\text{df}\normalsize}{\scriptsize\text{df}\normalsize-2}$.

:::{style="margin-top: -3px"}
Auch zur t-Verteilung gibt es ein unstandardisiertes Pendant, die **nicht-standardisierte t-Verteilung**, die von Mittelwert $\color{green}{\mu}$ und Streuung $\color{blue}{\sigma}$ abhängt:
:::

$$
f_t(x|\color{green}{\mu},\color{blue}{\sigma},\scriptsize\text{df}\normalsize) = \frac{\Gamma\left(\frac{\scriptsize\text{df}\normalsize+1}{2}\right)} {\color{blue}{\sigma}\sqrt{\scriptsize\text{df}\normalsize\,\pi}~\Gamma\left(\frac{\scriptsize\text{df}\normalsize}{2}\right)}\left(1+\frac{1}{\scriptsize\text{df}\normalsize}\left(\frac{x-\color{green}{\mu}}{\color{blue}{\sigma}}\right)^2\right)^{-\frac{\text{df}+1}{2}}
$$

Ähnlich wie beim z-Test hat es sich in der Praxis aber durchgesetzt immer mit standardisierten Prüfgrößen (wie $z$ oder $t$) zu arbeiten. Daher finden die unstandardisierten Normal- und t-Verteilungen im Kontext der Hypothesentestung seltener Anwendung.

:::{style="margin-top: -5px"}
Der **Vorteil standardisierter Prüfgrößen** ist, dass diese vergleichbar zwischen Studien sind. Ein bestimmter z- oder t-Wert hat eine Aussagekraft, ohne den Standardfehler einer Studie zu kennen.
:::


<!----------------->
<!--- New slide --->
<!----------------->
## [Intuition: verdickte Flanken der t-Verteilung]{style="color: darkred"}

![](images/kein_klausurstoff.png){.absolute bottom=0 right=0 height=110px}

:::{.nonincremental}
- Warum sind die Flanken der Verteilung der Testgröße $t=\frac{\hat{\theta}}{\hat{se}}$ stärker ausgeprägt, wenn $\hat{se}$ auf Basis der Stichprobe geschätzt werden muss?
- Der Grund liegt in der (Chi-)Verteilung der Zufallsvariable $\hat{se}$ im Nenner: 
    - Bei kleinen Stichprobengrößen (kleines df), gibt es eine Assymmetrie der Verteilung hin zu Werten kleiner dem Mittelwert (welcher die korrekte Schätzung von $se$ repräsentiert &mdash; in der Abbildung gestrichelte Linien)
    - D.h. wir *teilen* $\hat{\theta}$ *überproportional häufig durch zu kleine Werte*, wodurch die *Teststatistik* $t=\frac{\hat{\theta}}{\hat{se}}$ *überproportional häufig zu extreme (negative oder positive) Werte* liefert.
    - Dies führt zu den stärkeren Flanken der t-Verteilung!
:::

![](images/chi.png){style="margin-top: -20px !important"}

<!----------------->
<!--- New slide --->
<!----------------->
## [Funktionen der t-Verteilung]{color="darkred"}

![](images/kein_klausurstoff.png){.absolute top=10 right=0 height=110px}

![](images/tdist_cdf.png){height=300px style="margin-left:40px !important"}

![](images/tdist_cdfinv.png){height=400px}




<!----------------->
<!--- New slide --->
<!----------------->
## [Herleitung des gepoolten Standardfehlers]{color="darkred"}{#pooledvariance}

![](images/kein_klausurstoff.png){.absolute bottom=0 right=0 height=110px}

:::{style="margin-top: -10px"}
:::{.nonincremental}
- Im Fall unabhängiger Messungen und ähnlicher Varianzen wird eine **gepoolte Streuung** $\hat{\sigma}_\text{pooled}$ herangezogen:
:::
:::

:::{style="margin-top: -50px"}
$$
\hat{se} = \hat{\sigma}_\text{pooled}\sqrt{\frac{1}{n_A}+\frac{1}{n_B}}
$$
:::

:::{style="margin-top: -15px"}
:::{.nonincremental}
- Zur Herleitung dieser Formel starten wir mit der allgemeinen Varianzsummenformel: wir wissen, dass sich die Varianzen der beiden Stichprobenmittelwerte (=quadrierte Standardfehler $\hat{se}_A^2$ und $\hat{se}_B^2$) aufaddieren:
:::
:::

:::{style="margin-top: -54px"}
$$
\hat{se}^2 = \hat{se}_A^2 + \hat{se}_B^2\quad\rightarrow\quad \hat{se} = \sqrt{\hat{se}_A^2 + \hat{se}_B^2} = \sqrt{\frac{\hat{\sigma}_A^2}{n_A} + \frac{\hat{\sigma}_B^2}{n_B}}
$$
:::

:::{style="margin-top: -15px"}
:::{.nonincremental}
- Können die Varianzen $\hat{\sigma}_A^2$ und $\hat{\sigma}_B^2$ als ähnlich angenommen werden, gilt folgende Überlegung: statt beide Varianzen einzeln zu schätzen, wird eine einheitliche gepoolte Varianz $\hat{\sigma}^2_\text{pooled}$ auf Basis beider Stichproben geschätzt.
  - Vorteil: diese Varianz kann präziser geschätzt werden als die Einzelvarianzen, da die kombinierte Stichprobenzahl $n_A+n_B$ zugrundegelegt wird.
- Wir ersetzen also $\hat{\sigma}_A^2$ und $\hat{\sigma}_B^2$ jeweils durch $\hat{\sigma}_\text{pooled}^2$:
:::
:::

:::{style="margin-top: -20px"}
$$
\hat{se} = \sqrt{\frac{\hat{\sigma}_\text{pooled}^2}{n_A} + \frac{\hat{\sigma}_\text{pooled}^2}{n_B}} = \hat{\sigma}_\text{pooled}\sqrt{\frac{1}{n_A}+\frac{1}{n_B}}\qquad\text{q.e.d.}
$$
:::



<!----------------->
<!--- New slide --->
<!----------------->
## [Intuition zur Welch–Satterthwaite-Gleichung]{color="darkred"}{#welchsatterthwaiteintuition}

![](images/kein_klausurstoff.png){.absolute bottom=0 right=0 height=110px}

Auch wenn die Welch–Satterthwaite-Gleichung für die Freiheitsgrade beim Zweistichproben-t-Test mit unähnlichen Varianzen recht kompliziert aussieht, ist eine nähere Betrachtung aufschlussreich.

Für $n_A=n_B=n$ gilt:

$$
\text{df}=(n-1)\Big(1+\frac{2}{\left(\frac{\hat{\sigma}_A}{\hat{\sigma}_B}\right)^2+\left(\frac{\hat{\sigma}_B}{\hat{\sigma}_A}\right)^2}\Big)
$$

Zwei Fälle sind interessant:

<div class="vspace-small"></div>

[Fall 1: Sind die Varianzen gleich]{.underline} (entgegen der Annahme), d.h. $\hat{\sigma}_A=\hat{\sigma}_B$, so vereinfacht sich die Formel im Grenzfall zu

$$
\text{df} = (n-1)(1+\frac{2}{2}) = 2n -2,
$$

**also exakt die Formel des klassichen Zweistichprobentests.**


<!----------------->
<!--- New slide --->
<!----------------->
## [Intuition zur Welch–Satterthwaite-Gleichung]{color="darkred"}

![](images/kein_klausurstoff.png){.absolute bottom=0 right=0 height=110px}

[Fall 2: Sind die Varianzen extrem unterschiedlich]{.underline}, so wird entweder $\left(\frac{\hat{\sigma}_A}{\hat{\sigma}_B}\right)^2$ oder $\left(\frac{\hat{\sigma}_B}{\hat{\sigma}_A}\right)^2$ extrem groß und die Formel vereinfacht sich zu

$$
\text{df} = (n-1)(1+\frac{2}{\infty}) = (n-1)(1+0) = n-1
$$

**also exakt die Formel des Einstichprobentests.**

Für extrem ungleiche Varianzen reduzieren sich also die Freiheitsgrade &mdash; und damit die *effektive Stichprobengröße* &mdash; auf die Größe einer der beiden verglichenen Gruppen. 

Das ist durchaus intuitiv: ist z.B. $\hat{\sigma}_A$ extrem viel kleiner als $\hat{\sigma}_B$, so spielt die Varianz der Gruppe A nahezu keine Rolle mehr. Die Versuchspersonen dieser Gruppe gehen also für die Berechnung des Standardfehlers "verloren", was sich entsprechend auf die Freiheitsgrade auswirkt.

*Im Grenzfall* spielt die Varianz dieser Gruppe überhaupt keine Rolle mehr, sondern nur noch ihr Mittelwert. Die Gruppe hat damit die gleiche Funktion wie der Referenzwert $\mu_0$ beim Einstichproben-t-Test.






<!----------------->
<!--- New slide --->
<!----------------->
## [Inferenzstatistik für Regressionskoeffizienten]{color="darkred"}{#treg}

![](images/kein_klausurstoff.png){.absolute bottom=0 right=0 height=110px}

:::: {.columns}
::: {.column width="64%"}

:::{.nonincremental}
- Die Hypothesentestung für den Regressionskoeffizienten der einfachen Regression (d.h. 1 UV) unterscheidet sich nicht von der Korrelation.
:::

:::{style="margin-top: -15px"}
- Auch für den Regressionskoeffizienten können wir einen Standardfehler definieren:
:::

:::{.fragment}
$$
\hat{se}(\hat{b}_1)= \frac{\hat{\sigma}_Y}{\hat{\sigma}_X}\sqrt{\frac{1-\hat{\rho}^2}{n-2}}
$$
:::

:::
::: {.column width="36%"}
![](images/regression_basics2_v2.png)
:::
::::

- .. und einen darauf basierenden t-Wert:

:::{.fragment}
$$
t = \frac{\hat{b}_1}{\hat{se}(\hat{b}_1)} \overset{\text{(s. nächste Folie)}}{=} \hat{\rho}\sqrt{\frac{n-2}{1-\hat{\rho}^2}}
$$
:::

- Die Zahl der Freiheitsgrade ist wie bei der Korrelation $\text{df}=n-2$, da auch hier für die Berechnung des Standardfehlers die beiden Mittelwerte $\bar{x}$ und $\bar{y}$ bestimmt werden müssen.
- Der Rest ist bekannt.

<!----------------->
<!--- New slide --->
<!----------------->
## [Inferenzstatistik für Regressionskoeffizienten]{color="darkred"}

![](images/kein_klausurstoff.png){.absolute bottom=0 right=0 height=110px}

:::{.nonincremental}
- Während die Regressionssteigung abhängt davon, welche Variable als UV (bzw. $x$) und welche als AV (bzw. $y$) definiert wird, sind die t-Werte (und damit auch die p-Werte) unabhängig von der Rollenverteilung der Variablen.
- Dies ergibt sich direkt aus dem Zusammenhang von $\hat{\rho}$ und $\hat{b}_1$ (vgl. Vorlesung 06):
:::

$$
\hat{b}_1 = \frac{\hat{\sigma}_Y}{\hat{\sigma}_X}\hat{\rho}
$$

:::{.nonincremental}
- Damit ist die Prüfgröße $t$:
:::

:::{style="margin-top: -12px"}
$$
t = \frac{\hat{b}_1}{\hat{se}(\hat{b}_1)} = \frac{\frac{\hat{\sigma}_Y}{\hat{\sigma}_X}\hat{\rho}}{\frac{\hat{\sigma}_Y}{\hat{\sigma}_X}\sqrt{\frac{1-\hat{\rho}^2}{n-2}}} = \frac{\hat{\rho}}{\sqrt{\frac{1-\hat{\rho}^2}{n-2}}} = \hat{\rho}\sqrt{\frac{n-2}{1-\hat{\rho}^2}}
$$
:::

:::{.nonincremental}
- Der t-Wert bei der einfachen Regression ist also identisch zum t-Wert der Korrelation und insbesondere nur noch abhängigg von $\hat{\rho}$ und nicht mehr $\hat{b}_1$
- Da für die Korrelation $\hat{\rho}$ die Rollenverteilung der beiden Zusammenhangsvariablen<br>unerheblich ist, folgt, dass der p-Wert bei der Regression ebenso wenig von der<br>Zuordnung der Variablen als UV und AV abhängt.
:::




<!----------------->
<!--- New slide --->
<!----------------->
## [Testung des y-Achsenabschnitts bei der Regression]{color="darkred"}

![](images/kein_klausurstoff.png){.absolute bottom=0 right=0 height=110px}

:::{.nonincremental}
- Wir haben bislang den Achsenabschnitt $\hat{b}_0$ aus der Diskussion außen vor gelassen.
- Im Kontext der Regression wird der Achsenabschnitt $\hat{b}_0$ idR nicht getestet. 
- Grund: die Frage, ob Variable $Y$ bei $X=0$ einen y-Achsenabschnitt aufweist, der signifikant von Null verschieden ist, ist im Kontext der Regression selten interessant &mdash; schließlich ist es ja der ganze Zweck der Regression die systematische Verändeurng von $Y$ in $X$ zu analysieren und dabei gerade nicht nur einen bestimmten Wert von $X$ zu betrachten.
- Nichts desto trotz ist auch die Schätzung von $\hat{b}_0$ mit Unsicherheit verbunden, die durch folgenden Standardfehler definiert ist: 
:::
$$
\hat{se}(\hat{b}_0) = \underbrace{\sqrt{\frac{\sum\left(y_i-\hat{y_i}\right)^2}{n-2}}}_\text{Standardabweichung der Residuen}\sqrt{\frac{1}{n}+\frac{\bar{x}^2}{\sum\left(x_i-\bar{x}\right)^2}}
$$

<!----------------->
<!--- New slide --->
<!----------------->
## [Testung des y-Achsenabschnitts bei der Regression]{color="darkred"}

![](images/kein_klausurstoff.png){.absolute bottom=0 right=0 height=110px}

:::{.nonincremental}
- Ein Spezialfall ist das lineare Modell ohne Steigung (bzw. ohne x!):
:::

$$
\hat{y} = \hat{b}_0
$$

:::{.nonincremental}
- In diesem Modell kommt dem Regressionskoeffizienten $\hat{b}_0$ und dessen Unsicherheit eine interessantere Bedeutung zu: die Frage ob $\hat{b}_0$ signifikant verschieden von 0 ist, ist hier gleichbedeutend mit der Frage ob die Zufallsvariable $Y$ **signifikant verschieden von Null ist**.
- Mit anderen Worten: dieses Modell ist nichts anderes als ein Einstichproben-t-Test!
- Dies beinhaltet auch den Vergleich zweier abhängiger Messungen A und B, wenn wir zuvor $Y$ als Differenzvariable definieren: 
$Y=Y_A-Y_B$
:::
:::{.nonincremental}
- .. dann testet 
:::

$$
\hat{y} = \hat{b}_0
$$

:::{.nonincremental}
- die Frage, ob die Mittelwertsdifferenz von A und B signifikant verschieden von Null ist.
:::




<!----------------->
<!--- New slide --->
<!----------------->
## ["Common statistical tests are linear models"]{color="darkred"}

![](images/kein_klausurstoff.png){.absolute bottom=0 right=0 height=110px}


Die Parallele von bekannten statistischen Tests und (generalisierter) linearer Regression lässt sich auf alle Tests erweitern, die wir in Statistik 1 und Statistik 2 kennenlernen.

Hier eine Auswahl der Tests aus Statistik 1:

<!---  Table --->
| Test | Lineares Modell | Spezifierung | Nullhypothese |
|-|-|-|-|
| Pearson-Korrelation | $\hat{y}=\hat{\beta}_0 + \hat{\beta}_1 x$ | - | $\mathcal{H}_0: \beta_1 = 0$|
| Spearman-Korrelation | $\text{Rang}(\hat{y})=\hat{\beta}_0 + \hat{\beta}_1 \text{Rang}(x)$ | - | $\mathcal{H}_0: \beta_1 = 0$|
| Einstichproben-t-Test | $\hat{y}=\hat{\beta}_0$ | - | $\mathcal{H}_0: \beta_0 = 0$|
| Differenzen-t-Test | $\hat{y}_A-\hat{y}_B=\hat{\beta}_0$ | - | $\mathcal{H}_0: \beta_0 = 0$|
| Zweistichproben-t-Test | $\hat{y}=\hat{\beta}_0 + \hat{\beta}_1 x$ | $x=0$ für alle Datenpunkte von Gruppe A und $x=1$ für alle Datenpunkte von Gruppe B &rarr; Regression auf binäre x-Variable! | $\mathcal{H}_0: \beta_1 = 0$|

: {tbl-colwidths="[20, 25, 40, 15]"}

::: {.caption-style}
Die Analogie von bekannten statistischen Tests und linearem Modell ist lange bekannt, ging aber 2021 durch Beiträge von Jonas Lindeløv viral^[https://stats.stackexchange.com/questions/303269/common-statistical-tests-as-linear-models] ^[https://lindeloev.github.io/tests-as-linear/] ^[https://twitter.com/jonaslindeloev/status/1110907133833502721].
:::