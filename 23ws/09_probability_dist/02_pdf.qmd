
<!----------------->
<!--- New slide --->
<!----------------->
## Wahrscheinlichkeitsdichte

::: {.colorbox}
Geht die Kategorienbreite gegen 0, wird die Wahrscheinlichkeit(sverteilung) zur [**Wahrscheinlichkeitsdichte(verteilung)**]{color="navy"}.
:::

Wie kann man sich "Wahrscheinlichkeitsdichte" vorstellen?
    
- Wir kennen das Konzept der "Dichte" bei Stoffen: z.B. ist die Dichte von Eis ist ca. $0{,}918\stackrel{g}{}\!\!\unicode{x2215}_{\!\unicode{x202f}cm^3}$, d.h. dass sich in einem $1cm^3$ Würfel ein knappes Gramm Eis befindet.
- Eine Dichte ist also immer eine bestimmte Masse *pro* Maßeinheit.

:::{.fragment}
Wir können daher Wahrscheinlichkeitsdichte wie folgt definieren:
:::

::: {.definition .fragment}
<!---  Definition--->
|||
|:-:|-|
|||
| ![](images/definition.svg){height=70px} | $\text{Wahrscheinlichkeitsdichte} = \text{Wahrscheinlichkeits(masse) }pro\text{ Maßeinheit}$ | 
|||
: {tbl-colwidths="[10, 90]"}
:::

- Die Maßeinheit ist durch die Skala unseres Merkmals gegeben:<br>Wahrscheinlichkeit *pro* Zentimeter Nasenlänge, Wahrscheinlichkeit *pro* IQ-Punkt, Wahrscheinlichkeit *pro* Fragebogenpunkt




<!----------------->
<!--- New slide --->
<!----------------->
## Wahrscheinlichkeitsdichte

Da die Funktion der Normalverteilung
$$
f(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}
$$

für beliebig "feine" $x$ definiert ist, handelt es sich bei ihr in der Tat um eine **Wahrscheinlichkeitsdichtefunktion**.

<div class="vspace-medium"></div>

:::: {.columns}
::: {.column width="65%"}
- Wichtig: Die Fläche unter Dichtefunktionen ist gleich 1:
$$
\int_{-\infty}^{\infty} f(x) dx = 1
$$
:::
::: {.column width="35%"}
:::{.fragment}
![](images/integral_of_pdf_is_1.png){height=222px style="margin-top:-40px !important"}
:::
:::
::::

:::{.fragment}
.. im Fall der Normalverteilung sorgt dafür der Normalisierungsfaktor $\frac{1}{\sigma\sqrt{2\pi}}$ .
:::

<!-- ```{python}

``` -->
<!-- ```{python}
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import norm

np.random.seed(0)

fontsize = 12

mu = 5
std = 1.5
x = np.linspace(0, 10, 100)
y = norm.pdf(x, mu, std)

plt.figure(figsize=(4, 2.5))
plt.plot(x, y, color='#783c00', lw=2)
plt.fill_between(x,y, where= (2 < x)&(x < 4), color="b", alpha=0.2)
plt.annotate('Wahrscheinlichkeit,\n dass sich das\n Merkmal im Bereich\n [2cm;4cm] befindet', xy=(3, 0.05), xytext=(7.5,0.12), arrowprops=dict(arrowstyle='->'))
plt.xlabel('Nasenlänge (cm)', fontsize=fontsize)
plt.ylabel('Wahrscheinlichkeitsdichte', fontsize=fontsize)
plt.xticks(bins[::2], fontsize=fontsize-2)
plt.yticks(fontsize=fontsize-2)
plt.ylim(0, 0.28)
plt.xlim(-0.5, 15)
plt.savefig('images/density_integral.png', bbox_inches='tight')
``` -->


<!----------------->
<!--- New slide --->
<!----------------->
## Wahrscheinlichkeitsdichte
Um aus einer Wahrscheinlichkeits*dichte* eine Wahrscheinlichkeit zu erhalten, muss die Dichte über einen bestimmten Wertebereich des Merkmals summiert (integriert) werden.

<div class="vspace-small"></div>

<!---  Example --->
::: {.example}
:::: {.columns}
::: {.column width="10%"}
::: {style="margin-top: 10px"}
![](images/example.png){height=70px}
:::
:::
::: {.column width="90%"}
Wäre die Wahrscheinlichkeitsdichte für Nasenlängen im Intervall $[2cm; 4cm]$ konstant gleich $0{,}1$, so wäre die Wahrscheinlichkeit einer Nasenlänge in diesem Intervall gleich: 

$$
Wahrscheinlichkeit=Intervallgröße \cdot Wahrscheinlichkeitsdichte = 2cm \cdot 0{,}1cm^{-1} = 0{,}2
$$
:::
::::
:::

<!-- Analogie Dichte von Eis: bei eine Dichte von Eis $0{,}918\frac{g}{cm^3}$ können wir ausrechnen, dass sich in einem 10x10x10cm Eiswürfel $918g$ Eis befinden: $Gewicht = Volumen \cdot Dichte = 1000cm^3 \cdot 0{,}918\frac{g}{cm^3}=918g$ -->

<div class="vspace-small"></div>

:::: {.columns .fragment}
::: {.column width="62%"}
:::{.nonincremental}
- Mathematisch beschreiben wir diese Operation als ein Integral:
:::

$$
P(x_0<x<x_1) = \int_{x_0}^{x_1} f(x) dx
$$

- $P$ ist die Wahrscheinlichkeit, dass das Merkmal einen Wert zwischen $x_0$ (Untergrenze) und $x_1$ (Obergrenze) aufweist.
- Das Integral setzt die *Wahrscheinlichkeit* $P(x_0<x<x_1)$ mit der *Wahrscheinlichkeitsdichte* $f(x)$ in Verbindung.
:::
::: {.column width="38%"}
<div class="vspace-large"></div>
![Beachte, dass im Bild die Wahrscheinlichkeitsdichte (anders als im Beispiel oben) nicht konstant zwischen 2cm und 4cm ist.](images/density_integral.png)
:::
::::    

<!----------------->
<!--- New slide --->
<!----------------->
## Wahrscheinlichkeitsdichte: Beispiel

:::: {.columns}
::: {.column width="63%"}

Lassen wir die Vereinfachung im vorherigen Beispiel fallen, dass die Wahrscheinlichkeitsdichte im Bereich $[2cm; 4cm]$ konstant ist. Stattdessen nehmen wir an, dass Nasenlängen in der Population normalverteilt sind, mit Mittelwert $\mu=5$ und Standardabweichung $\sigma=1{,}5$. 

**Frage: wie hoch ist die Wahrscheinlichkeit, dass eine zufällig gezogene Nase aus der Population eine Länge zwischen $2cm$ und $4cm$ hat?**
:::
::: {.column width="37%"}
![](images/density_integral.png)
:::
::::



:::{.fragment}
$$
P(2\le x\le 4) = \int_2^4 f(x)dx = \frac{1}{\sigma\sqrt{2\pi}}\int_2^4\text{exp}\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)dx = \\
= \frac{1}{1{,}5\sqrt{2\pi}}\int_2^4\text{exp}\left(-\frac{(x-5)^2}{2\cdot1{,}5^2}\right)dx \overset{(Computer!)}{\approx} 0{,}23
$$
:::

<!----------------->
<!--- New slide --->
<!----------------->
<!-- ## Theoretische Wahrscheinlichkeitsverteilung
- Je nach Forschungsfrage kann das Ziel einer Stichprobe sein
    1. Rückschlüsse auf die Population zu ziehen (wie gerade gesehen), oder
    2. Rückschlüsse auf die [**theoretische Wahrscheinlichkeitsverteilung**]{color="navy"} zu ziehen.
- Die theoretische Wahrscheinlichkeitsverteilung ist von Interesse, wenn wir die Ursachen oder Mechanismen eines psychologischen Phänomens beschreiben und verstehen wollen.
    - In diesem Fall ist die spezifische Population nicht das primäre Interesse &mdash; selbst die Population ist im Prinzip nur eine (großen) Stichprobe aus der Wahrscheinlichkeitsverteilung.
    - Stattdessen wollen wir mechanistisch und quantitativ verstehen, wie naturwissenschaftliche Prozesse (biologisch, psychisch, sozial) bestimmte psychologische Phänomene "generieren".
- In der Psychologie ist das primäre Interesse häufig der Mechanismus und damit die theoretische Wahrscheinlichkeitsverteilung eines Merkmals; in der Politologie, Soziologie, Ökonomie ist dagegen häufig die Verteilung in der Population relevant (z.B. "Sonntagsfrage").
- Auch wenn diese Unterscheidung konzeptionell wichtig ist, ist sie in der Praxis häufig nicht relevant, da Populationen i.d.R. so groß sind, dass sie nahezu perfekt mit der theoretischen Wahrscheinlichkeitsverteilung übereinstimmen. -->


