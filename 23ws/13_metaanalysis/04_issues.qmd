

<!----------------->
<!--- New slide --->
<!----------------->
## Probleme bei einer Metaanalyse

**Problem 1:** Qualität der einfließenden Studien ist nicht zufriedenstellend (in der Praxis ein sehr häufiges Problem) &mdash; „garbage-in-garbage-out“.

- Lässt sich prinzipiell durch gut gewählte Ein- und Ausschlusskriterien vermeiden &mdash; allerdings bleiben dann oft nur sehr wenige Studien übrig.
- Alternative: methodische Qualität codieren und als Gewichtung einfügen.

**Problem 2:** die einfließenden Studien messen nicht genau dasselbe Konstrukt &mdash; „Äpfel-Birnen-Problem“.

- Unterschiedliche abhängige Variablen (d.h. Outcomes).
- Effekte sind dann nicht oder nur begrenzt vergleichbar.
- Diagnose durch die **psychometrische Metaanalyse**: 
    - Einteilung der Studien in Subgruppen mit jeweils einheitlichen abhängigen Variablen.
    - Ist die Varianz innerhalb der Subgruppen deutlich kleiner als die Varianz aller Studien kombiniert ("wenn alles in einem Topf“ liegt"): Hinweis auf unzulässige Vermischung der abhängigen Variablen.<br>&rArr; „Äpfel“ und „Birnen“ sollten dann in getrennten Analysen untersucht werden.


<!----------------->
<!--- New slide --->
<!----------------->
## Probleme bei einer Metaanalyse

**Problem 3:** Mehrere Studienergebnisse pro Artikel – Abhängigkeitsproblem

- Mehrere Effekte, die in einem einzelnen Artikel berichtet werden, sind typischerweise nicht unabhängig (insbesondere Überlapp der Versuchspersonen).
- Lösung: zunächst den mittleren (aggregierten) Effekt pro Artikel bestimmen und diesen in die Metaanalyse aufnehmen.

**Problem 4:** die einfließenden Studien bilden kein repräsentatives Abbild aller Studien zur Fragestellung &mdash; Publikationsbias

- Der gemittelte Effekt als Hauptergebnis ist dann verzerrt und möglicherweise unbrauchbar.
- Diagnose: Funnel Plot, p-curve, p-uniform
- Lösung: im Prinzip keine (es gibt Vorschläge – z. B. trim-and-fill, p-curve, p-uniform – aber diese schneiden in Simulationsstudien unzureichend ab).



<!----------------->
<!--- New slide --->
<!----------------->
## Funnel plot

[**Funnel Plot**]{color="navy"}: visuelle Methode zur Diagnose eines Publikationsbias'

- Effekte aller Studien werden gegen ihre jeweilige Präzision abgetragen (z. B. Stichprobengröße $n$).
- *Wenn kein* Publikationsbias vorliegt, sollte ein umgekehrter Trichter (engl. *funnel*) entstehen, da die Varianz der Schätzungen bei größeren Stichproben systematisch kleiner werden sollte (Gesetz der großen Zahl).
- *Wenn* ein Publikationsbias vorliegt, wird der Funnel asymmetrisch,

![](images/metaanalysis12_annot.png)


<!----------------->
<!--- New slide --->
<!----------------->
## Funnel plot

[**Funnel Plot**]{color="navy"}: visuelle Methode zur Diagnose eines Publikationsbias'

- **Beispiel:** Meta-Analyse zum Zusammenhang von Habituationsfähigkeit im Kindesalter und späterem IQ^[Bakker M, Van Dijk A, Wicherts JM (2012) The Rules of the Game Called Psychological Science. Perspect Psychol Sci 7:543–554.]

:::: {.columns}
::: {.column width="55%"}
![](images/metaanalysis13.png)

:::
::: {.column width="45%"}
::: {style="font-size: 23px !important"}
> If science were a game, a dominant rule would probably be to collect results that are statistically significant. Several reviews of the psychological literature have shown that <mark>around 96% of papers involving the use of null hypothesis significance testing report significant outcomes</mark> for their main results but that the typical studies are <mark>insufficiently powerful for such a track record</mark>. We explain this paradox by showing that the use of several small underpowered samples often represents a more efficient research strategy (in terms of finding p < .05) than does the use of one larger (more powerful) sample.
:::
[Bakker et al (2012): The Rules of the Game Called Psychological Science.]{.quote-citation}
:::
::::


<!----------------->
<!--- New slide --->
<!----------------->
##
:::: {.columns}
::: {.column width="9%"}
::: {style="margin-top:-15px"}
![](images/summary.png){width=60px}
:::
:::
::: {.column width="91%"}
::: {.summary style="margin-top: 30px !important"}
- Eine Metaanalyse **aggregiert standardisierte oder unstandardisierte Effekte** aus verschiedenen Studien.
    - Sie basiert also auf einer **empirischen Stichprobenverteilung**.
- Die Bestimmung des mittleren Effektes erfolgt in der Regel mit einem **gewichteten Mittelwert**.
    - **Gewichtung anhand der Präzision** der einzelnen Studien (die Präzision hängt stark von der Stichprobengröße ab).
- Metaanalysen liefern **bessere Schätzungen für den wahren Populationseffekt** als einzelne Studien.
- Ergebnisse von Metaanalysen sind häufig **durch einen Publikationsbias verzerrt**, da vorrangig große/signifikante Effekte publiziert wurden – der wahre Effekt wird dann überschätzt.
:::
:::
::::

## {.center}

![[xkcd#1477](https://xkcd.com/1447/)](images/metaanalysis1447.png){height=450px}