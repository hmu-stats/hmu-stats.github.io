{
  "hash": "61664eb6edc14d2bdbdbd312ec7ddc6d",
  "result": {
    "markdown": "---\ntitle: \"Vorlesung 04: Zusammenhänge\"\n---\n\n## Was sind Zusammenhänge?\n\n:::: {.columns}\n::: {.column width=\"70%\"}\n- Ein [**Zusammenhang**]{color=\"navy\"} beschreibt zu welchem Grad zwei Variablen (Merkmale) systematisch miteinander in Verbindung stehen\n- Zusammenhänge bilden die Essenz der psychologischen Forschung&mdash;durch sie versuchen wir die Mechanik der menschlichen Psyche zu verstehen:\n    - Fördert **Ausdauersport** das **psychische Wohlbefinden**?\n    - Helfen **Psychotherapiestunden** bei der Überwindung einer **Depression**?\n    - Wirkt sich **Bildschirmzeit** nachteilig auf die **Schlafqualität** aus?\n    - Steigt durch **kindliche Frühförderung** die Wahrscheinlichkeit für einen **akadamischen Bildungsabschluss**?\n- Fast jede wissenschaftliche Hypothese lässt sich als Zusammenhang formuliern &ndash; selbst Unterschiede!\n    - [Formulierung als Unterschied]{.underline}: unterscheiden sich Männer und Frauen in ihren verbalen Fähigkeiten?\n    - [Formulierung als Zusammenhang]{.underline}: steht die kategorische Variable **Geschlecht** in Zusammenhang mit der metrischen Variable **verbale Fähigkeiten**?\n:::\n<!-- Begin second column -->\n::: {.column width=\"30%\"}\n![](images/relationship.png){width=200px}\n:::\n::::\n\n## Zusammenhänge im engeren Sinn\n\n- Um Unterschiede und Zusammenhänge voneinander abzugrenzen, verstehen wir nachfolgend Zusammenhänge in einem engeren Sinn:\n\n<!---  Definition--->\n::: {.definition}\n|||\n|:-:|-|\n|||\n| ![](images/definition.svg){height=70px} | Ein [**Zusammenhang**]{color=\"navy\"} (im engeren Sinn) beschreibt zu welchem Grad die **Variation zweier metrischer Variablen** miteinander in Verbindung steht. | \n|||\n: {tbl-colwidths=\"[10, 90]\"}\n:::\n\n- Durch die Eingrenzung auf **metrische Variablen** (diskret oder kontinuerlich) stellt etwa die Verbindung von Geschlecht und verbalen Fähigkeiten keinen Zusammenhang im engeren Sinn dar, da Geschlecht eine kategorische Variable ist.\n- In den meisten Fällen sind Zusammenhänge das \"schärfere statistische Schwert\" als Unterschiede und es lohnt sich oft, Forschungsfragen entsprechend anzupassen:\n    - Unterscheidet sich die akademische Leistung von Rauchern und Nichtrauchern? &rArr; Zusammenhang **Zahl der Zigaretten pro Tag** und **akademische Leistung**\n    - Unterscheidet sich der Medienkonsum von Depressiven und Kontrollen? &rArr; Zusammenhang **Depressivität** und **Medienkonsum**\n    - Unterscheidet sich das Depressionsrisiko zwischen Nord- und Süddeutschland? &rArr; Zusammenhang **geographischer Breitengrad** und **Depressionsrisiko**\n\n\n## Kovarianz\n\n- Ziel: mathematische Größe, die zum Ausdruck bringt, wie stark die Variation zweier Variablen miteinander in Zusammenhang steht\n- Wir haben bereits eine Größe für die Variation *einer* Variable &mdash; die Varianz:\n\n$$\nVar(X) = \\frac{1}{N}\\sum_{i=1}^N\\big(x_i-\\bar{X}\\big)^2 = \\frac{1}{N}\\sum_{i=1}^N(x_i-\\bar{X})\\color{darkred}{(x_i-\\bar{X})}\n$$\n\n\n- Die Varianz gibt an, wie stark eine Variable $X$ um ihren Mittelwert $\\bar{X}$ schwankt\n- Analog berechnet die [**Kovarianz**]{color=\"navy\"}, wie stark die *gemeinsame Schwankung zweier Variablen um ihren jeweiligen Mittelwert* ist:\n$$\n\\text{Kovarianz:}\\quad Cov(X,Y) = \\frac{1}{N}\\sum_{i=1}^N\\big(x_i-\\bar{X}\\big)^2 = \\frac{1}{N}\\sum_{i=1}^N(x_i-\\bar{X})\\color{darkred}{(y_i-\\bar{Y})}\n$$\n\n- Im Zentrum der Kovarianz steht die Erkenntnis, dass das **mathematische Produkt** zweier Abweichungsvariablen &mdash; hier die Abweichungen $(x_i-\\bar{X})$ und $(y_i-\\bar{Y})$ vom Mittelwert &mdash; angibt, wie stark die beiden Abweichungen gleichsinnig variieren (ko-variieren).\n\n## Kovarianz\n\nIntuition:\n\n- Sind zwei zusammengehörige Datenpunkte $x_i$ und $y_i$ **größer als der Mittelwert**, sind sowohl $(x_i-\\bar{X})$ als auch $(y_i-\\bar{Y})$ **positiv**, und damit auch das Produkt $(x_i-\\bar{X})(y_i-\\bar{Y})$ **positiv**.\n\n- Sind zwei zusammengehörige Datenpunkte $x_i$ und $y_i$ **geringer als der Mittelwert**, sind sowohl $(x_i-\\bar{X})$ als auch $(y_i-\\bar{Y})$ **negativ**, und damit das Produkt $(x_i-\\bar{X})(y_i-\\bar{Y})$ wieder **positiv**.\n\n<div class=\"vspace-medium\"></div>\n\n![](images/covariance-examples.png)\n\n## Kovarianz: Größe-Gewicht-Beispiel\n\n<div class=\"vspace-small\"></div>\n\n:::: {.columns}\n::: {.column width=\"34%\"}\n![](images/covariance_iq.png)\n:::\n::: {.column width=\"66%\"}\n$$\n\\scriptsize{\n\\begin{align}\n&Cov(X, Y) = \\frac{1}{N}\\sum_{i=1}^N(x_i-\\bar{X})(y_i-\\bar{Y}) = \\\\\n          &= \\frac{1}{3}\\big[(160-170)(60-70)+(170-170)(70-70)+(180-170)(80-70)\\big] = \\\\\n          &= \\frac{1}{3}\\big[(-10)\\cdot (-10)+0\\cdot 0+10\\cdot 10\\big] = \\\\\n          &= \\frac{1}{3}\\cdot 200 = 66{,}67\n\\end{align}}\n$$\n:::\n::::\n\n- Problem: die Kovarianz hängt von den Einheiten ab!\n- Wird im Beispiel die Körpergröße $X$ in der Einheit *Meter* angegeben, so lautet die Kovarianz:\n\n$$\n\\scriptsize{\n\\begin{align}\nCov(X, Y) &= \\frac{1}{3}\\big[(1{,}60-1{,}70)(60-70)+(1{,}70-1{,}70)(70-70)+(1{,}80-1{,}70)(80-70)\\big] = \\\\\n          &= \\frac{1}{3}\\big[(-0{,}10)\\cdot (-10)+0\\cdot 0+0{,}10\\cdot 10\\big] = \\frac{1}{3}\\cdot 2 = 0{,}667\n\\end{align}}\n$$\n\n- Kovarianzen sind also **nicht vergleichbar wenn sich Einheiten unterscheiden**, und erst recht nicht, wenn sich die Variablen unterscheiden.\n\n\n\n## Pearson-Korrelation\n\n:::: {.columns}\n::: {.column width=\"70%\"}\n- Die [**Pearson-Korrelation**]{color=\"navy\"} schafft Abhilfe für das Problem der mangelnden Vergleichbarkeit\n- Der Schlüssel: die Kovarianz wird mit der Standardabweichung beider Variablen normalisiert:\n\n$$\n\\small{\n\\text{Korrelation:}\\;\\; r_{XY} = \\frac{Cov(X,Y)}{s_X s_Y} = \\frac{1}{N s_X s_Y}\\sum_{i=1}^N(x_i-\\bar{X})(y_i-\\bar{Y})\n}\n$$\n\n- Durch das Teilen durch die Standardabweichungen $s_X$ und $s_Y$ werden die Einheiten herausgekürzt &mdash; die Korrelation ist also eine **einheitslose Größe**.\n- Die Korrelation kann Werte zwischen $-1$ (perfekter negativer Zusammenhang) und $+1$ (perfekter positiver Zusammenhang) annehmen.\n\n:::\n<!-- Begin second column -->\n::: {.column width=\"30%\"}\n\n![Der britische Mathematiker Karl Pearson in seinem Büro im Jahr 1910. Neben dem Korrelationskoeffizienten verdanken wir Pearson viele andere statistische Konzepte wie die Hauptkomponentenanalyse oder den p-Wert. Später wurden seine Ansichten zu Eugenik kritisch hinterfragt. Bildnachweis^[http://www.learn-stat.com/life-of-karl-pearson/]](images/karl_pearson.png)\n\n:::\n::::\n\n\n\n\n<!----------------->\n<!--- New slide --->\n<!----------------->\n## [Warum ist die Korrelation auf &minus;1 bis 1 beschränkt?]{color=\"darkred\"}\n\n![](images/kein_klausurstoff.png){.absolute bottom=0 right=0 height=130px}\n\nDarstellung der Korrelation nur mit (Ko)Varianzen:\n\n$$\nr_{XY} = \\frac{Cov(X,Y)}{s_X s_Y} = \\frac{Cov(X,Y)}{\\sqrt{Var(X)} \\sqrt{Var(Y)}}\n$$\n\nDie Korrelation sollte maximal ($r=1$) sein, wenn $X$ mit sich selbst korreliert wird ($Y=X$). Zu berücksichtigen ist, dass die Kovarianz *von X mit sich selbst* gleich der Varianz ist:\n\n$$\nr_{XX} = \\frac{Cov(X,X)}{\\sqrt{Var(X)} \\sqrt{Var(X)}} = \\frac{Var(X)}{\\sqrt{Var(X)} \\sqrt{Var(X)}} = \\frac{Var(X)}{Var(X)} = 1\n$$\n\n\nUmgekehrt sollte die Korrelation maximal negativ sein ($r=-1$), wenn $Y$ genau das Inverse von $X$ ist, also $Y=-X$. Unter Berücksichtigung von $Var(X) = Var(-X)$ gilt:\n$$\nr_{X(-X)} = \\frac{Cov(X,-X)}{\\sqrt{Var(X)} \\sqrt{Var(-X)}} = \\frac{-Cov(X,X)}{\\sqrt{Var(X)} \\sqrt{Var(X)}} = \\frac{-Var(X)}{Var(X)} = -1\n$$\n\n<!-- \n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\n\nnp.random.seed(0)\ndata = np.random.multivariate_normal([0, 0], [[1, 0.5], [0.5, 1]], 30) + 3\nx, y = data[:, 0], data[:, 1]\nfontsize = 15\n\nr, p = pearsonr(x, y)\n\nplt.style.use('dark_background')\nplt.figure()\nplt.gca().tick_params(color='white')\nplt.scatter(x, y)\nplt.xlabel('Stunden auf TikTok', fontsize=fontsize)\nplt.ylabel('Entzündungswerte', fontsize=fontsize)\nplt.xticks(fontsize=fontsize-2)\nplt.yticks(fontsize=fontsize-2)\nplt.text(3.1, 0.7, fr'$r={r:.3f}$ ($p={p:.4f}$)', color='white', fontsize=fontsize-2)\nplt.savefig('images/streudiagramm.png', bbox_inches=\"tight\")\n``` -->\n\n<!----------------->\n<!--- New slide --->\n<!----------------->\n## Das Streudiagramm {.blackslide}\n\n- Von einer kleinen Stichprobe im TikTok-Datensatz des CCC sind auch Entzündungswerte bekannt\n- In der Task Force fragen Sie sich, ob es unter den Paradoxikern einen Zusammenhang zwischen der Zahl der Stunden auf TikTok und den Entzündungswerten gibt\n\n:::: {.columns}\n::: {.column width=\"45%\"}\nSie fassen Ihr Ergebnis in einem [**Streudiagramm**]{color=\"lightblue\"} zusammen.\n\n- Das Streudiagramm (engl. *scatter plot*) ist die häufigste Visualisierung eins biavariaten Zusammenhangs\n- Bei der Korrelation ist es dabei willkürlich, welche Variable auf der X- und Y-Achse liegt\n- Häufig wird zusätzlich zur \"Punktwolke\" auch eine **Regressionsgerade** angegeben, sowie die Stärke des Zusammenhangs (r=.., p=..)\n:::\n::: {.column width=\"55%\"}\n![](images/streudiagramm.png)\n:::\n::::\n\n\n## Pearson-Korrelation: Größe-Gewicht-Beispiel\n\n<div class=\"vspace-small\"></div>\n\n:::: {.columns}\n::: {.column width=\"44%\"}\n![](images/covariance_iq.png)\n:::\n::: {.column width=\"56%\"}\nWir hatten:\n$$\n\\small{\n\\begin{align}\n&Cov(X, Y) = \\frac{1}{N}\\sum_{i=1}^N(x_i-\\bar{X})(y_i-\\bar{Y}) = \\frac{200}{3}\n\\end{align}}\n$$\nUnd berechnen nun die Korrelation $\\;\\;r_{XY} = \\frac{Cov(X,Y)}{s_X s_Y}$\n\n$\\scriptsize{s_X=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N\\big(x_i-\\bar{X}\\big)^2}=\\sqrt{\\frac{1}{3}[(-10)^2+0^2+10^2]}=\\sqrt{\\frac{200}{3}}}$\n\n$\\scriptsize{s_Y=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N\\big(y_i-\\bar{Y}\\big)^2}=\\sqrt{\\frac{1}{3}[(-10)^2+0^2+10^2]}=\\sqrt{\\frac{200}{3}}}$\n\n$$\n\\small{\nr_{XY} = \\frac{\\frac{200}{3}}{\\sqrt{\\frac{200}{3}}\\cdot\\sqrt{\\frac{200}{3}}}=1\n}\n$$\n\n:::\n::::\n\n- Wäre die Körpergröße in der Einheit *Meter* angeben, so kürzt sich der Faktor 100 (1m = 100cm) nicht nur im Zähler, sondern auch im Nenner:\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n$$\n\\small{\nr_{XY} = \\frac{\\frac{2\\color{darkred}{{,}}00}{3}}{\\sqrt{\\frac{2\\color{darkred}{{,}}00}{3}}\\cdot\\sqrt{\\frac{2\\color{darkred}{{,}}00}{3}}}=1\n}\n$$\n:::\n::: {.column width=\"60%\"}\nDie Normalisierung mit der Standardabweichung sorgt somit dafür, dass die willkürliche Einheit keine Rolle spielt.\n:::\n::::\n\n\n\n\n<!----------------->\n<!--- New slide --->\n<!----------------->\n## Interpretation der Pearson-Korrelation\n\n- Die Pearson-Korrelation zeigt an, **wie linear** der Zusammenhang zweier Variablen ausgeprägt ist\n- Die Pearson-Korrelation ist dabei [nicht]{.underline} von der Steigung einer gedachten Gerade abhängig.\n\n![Korrelationskoeffizienten für verschiedene hypothetische Streudiagramme](images/correlation_examples.png){width=1030px}\n\n- Ein Zusammenhang zweier Variablen kann extrem schwach sein (z.B. so, dass eine Verdopplung von X nur einer 0.1%-Steigerung von Y entspricht) und dennoch kann die Korrelation stark sein (= nah an $\\pm 1$), wenn die Punkte exakt auf einer Geraden liegen.\n- Auf einen Satz gemünzt kann man sagen: \n\n::: {style=\"font-size: 26px; margin-top: -10px\"}\n> Die Pearson-Korrelation misst, wie gut bivariate Daten durch eine Gerade abgebildet werden können.\n:::\n\n\n## Voraussetzungen für das Berechnen der Pearson-Korrelation\n\n- Die Pearson-Korrelation *kann* *immer* berechnet werden, solange beide Variablen aus Zahlenwerten bestehen.\n- Es gibt jedoch weitere Kriterien, die für die Sinnhaftigkeit und Interpretierbarkeit der Pearson-Korrelation wichtig sind:\n\n<!---  Table --->\n|Kriterium|Falls Kriterium nicht erfüllt? | Beispiel | Mögliche Abhilfe? |\n|-|-|-|-|\n| Daten haben mindestens Intervallskalenniveau | ▪&numsp;Keine Aussage über Linearität des untersuchten Zusammenhangs möglich \\\n▪&numsp;Korrelation nicht interpretierbar | ![](images/example_ordinal.png) | Rangkorrelation |\n| Keine Ausreißer | Korrelationskoeffizient kann massiv verzerrt sein | ![](images/outlier_example.png) | Ausreißer entfernen oder Rangkorrelation | \n| Zusammenhang der Daten wird nicht durch *nicht-linearen* Anteil dominiert | ▪&numsp;Pearson-Korrelation falsches Modell \\\n▪&numsp;Linearität der Daten wird verzerrt wiedergegeben, da die Korrelation vom nicht-linearen Teil beeinflusst wird  | ![](images/nonlinear.png) | Komplexeres Modell, das den nicht-linearen Anteil berücksichtigt |\n\n: {tbl-colwidths=\"[24, 36, 20, 20]\"}\n\n## Mythen zur Pearson-Korrelation\n\n- In vielen Quellen finden sich darüber hinaus **unzutreffende Behauptungen** zur Verwendung der Pearson-Korrelation:\n\n<!---  Table --->\n||Behauptung| Fact |\n|-|-|-|\n| **Mythos 1** | Die Variablen müssen kontinuierlich sein | **Pearson-Korrelation ist valide für diskrete Daten**, solange diese mindestens Intervallskalenniveau aufweisen. Tatsächlich gibt es sogar eine Variante der Pearson-Korrelation, bei der beide Variablen binär sind (Phi-Koeffizient). |\n| **Mythos 2** | Die beiden Variablen müssen normalverteilt sein. | Die beiden Variablen müssen nicht normalverteilt sein. Korrekt ist, dass die Daten für die Berechnung eines p-Wertes **bivariat normalverteilt** sein sollten.\n| **Mythos 3** | Die Variablen müssen einen linearen Zusammenhang aufweisen | Gegenbeispiel: wenn Daten nur aus zufälligem Rauschen bestehen, sind sie hochgradig nicht-linear, aber dennoch gibt der Pearson-Koeffizient idR korrekterweise an, dass die Korrelation ungefähr 0 ist.<br>Zusammenhänge in der Psychologie sind *sehr selten* eindeutig linear, dennoch kann es sinnvoll sein, die Pearson-Korrelation anzuwenden. Besser ist es daher zu sagen (wie in der Folie zuvor), dass der **Zusammenhang nicht zu stark durch einen nicht-linearen Anteil dominiert** werden sollten. |\n| **Mythos 4** | Die Variablen müssen varianzhomogen sein | Varianzhomogenität (auch Homoskedastizität) meint, dass Y-Werte ähnliche Varianz in verschiedenen Abschnitten der X-Achse haben und umgekehrt. Jedoch ist Varianzhomogenität keine zwingende Voraussetzung für die Anwendung der Pearson-Korrelation. Korrekt ist, dass der erhaltene Korrelationskoeffizient **ungenauer** und der **p-Wert nicht mehr korrekt** ist.\n\n: {tbl-colwidths=\"[10, 25, 65]\"}\n\n\n<!-- ```{python}\nfrom string import ascii_letters\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_theme(style=\"white\")\nfontsize = 36\n\ndata = [\n    [1, 0.13, 0.35, 0.23, -0.2],\n    [0, 1, 0.37, 0.1, -0.4],\n    [0, 0, 1, 0.01, -0.26],\n    [0, 0, 0, 1, -0.1],\n    [0, 0, 0, 0, 1]\n]\n\nannot = [\n    ['1', '0.13', '0.35', '0.23', '-0.2'],\n    ['', '1', '0.37', '0.1', '-0.4'],\n    ['', '', '1', '0.01', '-0.26'],\n    ['', '', '', '1', '-0.1'],\n    ['', '', '', '', '1']\n]\n\ncolumns = ['O', 'C', 'E', 'A', 'N']\n\ncorr = pd.DataFrame(data, columns=columns, index=pd.Index(columns))\n\n# Generate a mask for the upper triangle\n# mask = np.tril(np.ones_like(corr, dtype=bool))\nmask = np.zeros_like(corr, dtype=bool)\nnp.fill_diagonal(mask, True)\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nhm = sns.heatmap(corr, annot=annot, fmt='', mask=mask, cmap=cmap, vmax=.3, center=0,\n                 square=True, linewidths=.5, cbar_kws=dict(shrink=1, label='Korrelation'), annot_kws=dict(size=fontsize))\nplt.gca().collections[0].colorbar.ax.tick_params(labelsize=28)\nhm.figure.axes[-1].set_ylabel('Korrelation', size=fontsize)\nhm.tick_params(labelsize=fontsize)\nplt.yticks(rotation=0) \nhm.patch.set_facecolor('#777')\nplt.tight_layout()\nplt.savefig(f'images/corrmatrix.png', bbox_inches=\"tight\")\n```-->\n\n\n<!----------------->\n<!--- New slide --->\n<!----------------->\n## Korrelationsmatrix\n\n- Eine **Korrelation** bestimmt immer den Zusammenhang zwischen **zwei Variablen**\n- Gibt es mehr als zwei Variablen (z.B. die \"Big Five\"), bietet sich eine Darstellung aller paarweisen Korrelationen an &ndash; die [**Korrelationsmatrix**]{color=\"navy\"}\n\n:::: {.columns .vcenter-column style=\"margin-top:-15px\"}\n::: {.column width=\"60%\"}\n![Tabellarische Korrelationsmatrix. Sterne kennzeichnen häufig das Signifikanzniveau.](images/corrmatrix_table.png)\n:::\n::: {.column width=\"40%\"}\n![Korrelationsmatrix als \"Heatmap\" &ndash; die Einfärbung ist ein visuelles Hilfsmittel zur intuitiven und schnellen Erfassung der Korrelationsstruktur eines Variablen-Sets.](images/corrmatrix.png)\n:::\n::::\n- Die [Nebendiagonalelemente]{color=\"darkgreen\"} sind der interessante Teil der Korrelationsmatrix, sie geben die Korrelationen verschiedener Variablen an\n- Die [Diagonalelemente]{color=\"darkred\"}, also die Korrelationen von Variablen mit sich selbst, sind immer 1\n\n\n\n<!----------------->\n<!--- New slide --->\n<!----------------->\n## Kovarianzmatrix\n\n:::: {.columns}\n::: {.column width=\"75%\"}\n- Eine analoge Matrix-Darstellung gibt es auch für die **Kovarianz**\n- Im Unterschied zur Korrelationsmatrix sind die Diagonalelemente der Kovarianzmatrix nicht 1, sondern geben die **Varianz** der Variable an.\n- Sei $\\mathbf{X}$ (beachte Fettschrift) ein Vektor von $n$ Variablen $X_1 .. X_n$, so ist die zugehörige Kovarianzematrix $\\operatorname{Cov}(\\mathbf{X})$:\n\n$$\n\\small{\n\\begin{align}\n\\operatorname{Cov}(\\mathbf{X}) & = \n\\begin{pmatrix}\\operatorname{Var}(X_1) & \\operatorname{Cov}(X_1,X_2) & \\cdots & \\operatorname{Cov}(X_1,X_n) \\\\ \\\\\n \\operatorname{Cov}(X_2,X_1)  & \\operatorname{Var}(X_2) & \\cdots & \\operatorname{Cov}(X_2,X_n) \\\\ \\\\\n \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\\\\n\\operatorname{Cov}(X_n,X_1) & \\operatorname{Cov}(X_n,X_2) & \\cdots & \\operatorname{Var}(X_n)\n\\end{pmatrix}\n\\end{align}\n}\n$$\n\n:::\n::: {.column width=\"25%\"}\n![](images/covariance_matrix.png)\n:::\n::::\n\n- Im Gegensatz zur Korrelationsmatrix ist die Kovarianzmatrix selten das \"Endprodukt\" einer Analyse, sondern meist ein Zwischenschritt in fortgeschritteneren statistischen Analysen wie der **Hauptkomponentenanalyse**\n\n# Rangkorrelationen\n\n## Was ist eine Rangkorrelation?\n\n:::: {.columns}\n::: {.column width=\"72%\"}\n\n- [**Rangkorrelationsmaße**]{color=\"navy\"} ermöglichen es, Zusammenhänge von ordinalskalierten und nicht-linearen Variablem zu untersuchen.\n    - Dies ist mit der Pearson-Korrelation nicht möglich\n- \"Rang\" bezieht sich auf das Ordinalskalenniveau, d.h. dass die Daten nur als \"Rang\" oder \"Reihenfolge\" interpretiert werden können.\n- Im Gegensatz zur Pearson-Korrelation bewerten Rangkorrelationen nicht die Linearität eines Zusammenhangs, sondern die **Monotonie des Zusammmenhangs**.\n- Ein weiterer Anwendungsfall sind intervallskalierte Daten, die Ausreißer aufweisen und daher die Pearson-Korrelation verzerren.\n    - Rangkorrelationen sind **robust gegenüber Ausreißern**\n    - In diesem Fall werden die Variablen künstlich in Ränge umgewandelt.\n- Hier behandeln wir zwei Maße für die Rangkorrelation:\n    - [**Spearman-Korrelation**]{color=\"navy\"}\n    - [**Kendall'sches Tau**]{color=\"navy\"}\n:::\n::: {.column width=\"28%\"}\n![](images/rankorder_example.png)\n\n<div class=\"vspace-medium\"></div>\n\n![](images/rankorder_example2.png)\n:::\n::::\n\n\n## Berechnung von Rängen\n\n- Liegen die Variablen nicht als Ränge vor, müssen sie zunächst in Ränge umgewandelt werden:\n    1. Werte der Variable sortieren\n    2. (Unnormierte) Ränge zuordnen\n    3. Normierung: Gleiche Werte erhalten den Mittelwert ihrer Ränge\n    4. (Optional) Variablen in ihre ursprüngliche Reihenfolge bringen\n\n![](images/rangbildung.png){heigh=375px}\n\n- Die Tabelle gibt die Rangberechnung *einer* Variablen an (z.B. X) &ndash; für die andere Variable (Y) muss das analoge Prozedere durchgeführt werden.\n\n\n<!----------------->\n<!--- New slide --->\n<!----------------->\n## Spearman-Korrelation\n- Die Spearman-Korrelation ist identisch zur Pearson-Korrelation, wenn die Variablen $X$ und $Y$ als Ränge $R(X)$ und $R(Y)$ vorliegen (Spearman wird häufig mit dem Buchstaben $\\rho$ bezeichnet):\n\n$$\n\\rho_{XY} = \\frac{cov(R(X),R(Y))}{s_{R(X)} s_{R(Y)}}\n$$\n&numsp;&numsp;wobei $s_{R(X)}$ und $s_{R(Y)}$ die Standardabweichungen der Ränge von $X$ und $Y$ sind.\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n- Wie die Pearson-Korrelation nimmt die Spearman-Korrelation Werte zwischen $-1$ und $+1$ an:\n    - Ein positiver Wert impliziert eine positiven monotonen Zusammenhang\n    - Ein negativer Wert impliziert eine negativen monotonen Zusammenhang\n    - Ein Wert nahe bei 0 impliziert einen schwachen (oder keinen) monotonen Zusammenhang\n:::\n::: {.column width=\"40%\"}\n![](images/monotonie.png)\n:::\n::::\n\n\n\n<!----------------->\n<!--- New slide --->\n<!----------------->\n## Kendalls Tau\n\n:::: {.columns}\n::: {.column width=\"65%\"}\n- Eine Alternative Rangkorrelation zu Spearman ist [**Kendalls Tau**]{color=\"navy\"}\n- Kendalls Tau vergleicht inwieweit die Rangfolge *aller* Paare $x_i$, $x_j$ mit der Rangfolge *aller* Paare $y_i$, $y_j$ übereinstimmt\n- Dazu wird die Zahl der konkordanten (übereinstimmenden) und diskordanten (nicht übereinstimmenden) Paare gezählt\n:::\n::: {.column width=\"35%\"}\n<div class=\"vspace-large\"></div>\n![](images/Kendalls_Tau_Gleichung.png)\n:::\n::::\n\n\n:::: {.columns}\n::: {.column width=\"30%\"}\n::: {.greybox}\n**Beispiel**\n:::\n\n$$\n\\small{\nX=\\begin{pmatrix} 9\\\\ 3\\\\ 7\\\\ 5 \\end{pmatrix}\nY=\\begin{pmatrix} 18\\\\ 7\\\\ 8\\\\ 21 \\end{pmatrix}\n}\n$$\n:::\n::: {.column width=\"70%\"}\n- Die Paare von X sind: $\\small{(9, 3), (9, 7), (9, 5), (3, 7), (3, 5), (7, 5)}$\n- Die Paare von Y sind: $\\small{(18, 7), (18, 8), (18, 21), (7, 8), (7, 21), (8, 21)}$\n- Die Paare $(x_1=9, x_3=7)$ und $(y_1=18, y_3=2)$ wären konkordant, da die Rangfolge des X-Paares ($x_1 > x_3$) gleich der Rangfolge des entsprechenden Y-Paares ($y_1 > y_3$) ist\n- Die Paare$(x_1=9, x_4=5)$ und $(y_1=18, y_4=21)$ wären diskonkordant, da die Rangfolge des X-Paares ($x_1 > x_4$) ungleich der Rangfolge des entsprechenden Y-Paares ($y_1 < y_4$) ist\n:::\n::::\n\n[Insgesamt gibt es im Beispiel 4 konkordante Paare und 2 diskordante Paare (prüfe nach!), daher gilt:]{style=\"margin-top:-12px; display: block\"}\n\n$$\n\\tau = \\frac{K-D}{K+D} = \\frac{4-2}{3+3} = \\frac{2}{6} = 0.333..\n$$\n\n\n\n<!-- ```{python}\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import pearsonr, spearmanr, linregress\nnp.random.seed(13)\nfontsize=15\n\ndata = np.random.multivariate_normal([0, 0], [[1, 0.1], [0.1, 1]], 20) + 3\ndata = np.vstack((data, [5, 12]))\nx, y = data[:, 0], data[:, 1]\n\nreg = linregress(x, y)\nreg2 = linregress(x[:-1], y[:-1])\nr, p = pearsonr(x, y)\nr2, p2 = pearsonr(x[:-1], y[:-1])\nxrange = np.array([1.1, 5.2])\n\nplt.figure(figsize=(11, 4))\nplt.subplot(121)\nplt.scatter(x[:-1], y[:-1], c='#888')\nplt.scatter(x[-1], y[-1], c='#d80000')\nplt.plot(xrange, reg.slope*xrange+reg.intercept, color='#d80000')\nplt.plot(xrange, reg2.slope*xrange+reg2.intercept, color='#888')\nplt.xticks(fontsize=fontsize-2)\nplt.yticks(fontsize=fontsize-2)\nplt.xlabel('X', fontsize=fontsize)\nplt.ylabel('Y', fontsize=fontsize)\nplt.text(1, 10.25, f'Pearson (mit Ausreißer): $r = {r:.3f}$', fontsize=fontsize-2, color='#d80000')\nplt.text(1, 11.4, f'Pearson (ohne Ausreißer): $r = {r2:.3f}$', fontsize=fontsize-2, color='#888')\nplt.text(4.53, 11, 'Ausreißer', fontsize=fontsize-2, color='#d80000')\n\nxr, yr = data[:, 0].argsort().argsort()+1, data[:, 1].argsort().argsort()+1\nprint(data[:, 0], data[:, 1])\nprint(xr, yr)\nrr, pr = spearmanr(xr, yr)\nrr2, pr2 = spearmanr(xr[:-1], yr[:-1])\nregr = linregress(xr, yr)\nregr2 = linregress(xr[:-1], yr[:-1])\nxrange = np.array([1, 22])\n\nplt.subplot(122)\nplt.scatter(xr[:-1], yr[:-1], c='#888')\nplt.scatter(xr[-1], yr[-1], c='#d80000')\nplt.plot(xrange, regr.slope*xrange+regr.intercept, color='#d80000')\nplt.plot(xrange, regr2.slope*xrange+regr2.intercept, color='#888')\nplt.xticks(range(0, 21, 5), fontsize=fontsize-2)\nplt.yticks(fontsize=fontsize-2)\nplt.xlabel('Rang(X)', fontsize=fontsize)\nplt.ylabel('Rang(Y)', fontsize=fontsize)\nplt.text(0.5, 23.5, fr'Spearman (mit Ausreißer): $\\rho = {rr:.3f}$', fontsize=fontsize-2, color='#d80000')\nplt.text(0.5, 26, fr'Spearman (ohne Ausreißer): $\\rho = {rr2:.3f}$', fontsize=fontsize-2, color='#888')\nplt.ylim(0, 29)\n\nplt.tight_layout()\n\nplt.savefig('images/pearson_outlier.png', bbox_inches=\"tight\")\n\n``` -->\n\n\n<!----------------->\n<!--- New slide --->\n<!----------------->\n## Rangkorrelationen: Robust gegen Ausreißer\n\n- Beispiel Pearson vs. Spearman:\n\n![](images/pearson_outlier.png)\n\n- Ein einziger Ausreißer verändert die Pearson-Korrelation im Beispiel von $r=-0.046$ nach $r=0.410$\n- Demgegnüber ist die Spearman-Korrelation \"robuster\" gegenüber dem Ausreißer &ndash; sie verändert sich \"lediglich\" von $\\rho=-0.030$ nach $\\rho=0.110$.\n- Intution: während der *Wert* es Ausreißers deutlich über dem zweithöchsten Y-Wert liegt, ist der *Rang* nur um 1 höher.\n\n<!----------------->\n<!--- New slide --->\n<!----------------->\n## [Wann Spearman und wann Kendall?]{color=\"darkred\"}\n\n![](images/kein_klausurstoff.png){.absolute bottom=0 right=0 height=130px}\n\n- Beide Rangkorrelationskoeffizienten bestimmen die Monotonie eines Zusammenhangs\n- **Kendall ist robuster bei kleinen Stichproben** und ist in diesen Fällen bevorzugt\n- **Spearman ist etwas weniger sensitiv gegenüber Rangbindungen** (also wenn zwei Werte den gleichen Rang haben) und ist daher bevorzugt, wenn es viele Rangbindungen gibt^[Puth M-T, Neuhäuser M, Ruxton GD (2015) Effective use of Spearman’s and Kendall’s correlation coefficients for association between two measured traits. Animal Behaviour 102:77–84.  ]\n    - Beachte: auch bei den Kendall'schen Paaren gibt es Rangbindungen, und zwar dann, wenn die verglichenen *Paare* zwischen $X$ und $Y$ genau identisch sind\n    - Diese Paare sind weder konkordant noch diskordant und es gibt verschiedene Algorithmen diese Fälle zu berücksichtigen (hier nicht behandelt)\n- **In Abwesenheit von Rangbindungen liefert Kendall präzisere Schätzungen** und ist Kendall zu bevorzugen.^[Puth M-T, Neuhäuser M, Ruxton GD (2015) Effective use of Spearman’s and Kendall’s correlation coefficients for association between two measured traits. Animal Behaviour 102:77–84.]\n- In der Statistik wird Kendall häufig als \"Default\"-Rangkorrelation empfohlen^[DC Howell (2012). Statistical Methods for Psychology. Wadsworth.]:\n    - idR präzisere Schätzung des Populationsparameters\n    - Standardfehler ist bekannt (für Spearman gibt es lediglich Approximationen^[https://stats.stackexchange.com/questions/18887/how-to-calculate-a-confidence-interval-for-spearmans-rank-correlation])\n- Faktisch ist aber Spearman der weitaus verbreitetere Korrelationskoeffizient &ndash;<br> womöglich weil er idR größer als der Kendall-Koeffizient ist &#128521;\n\n<!-- https://www.researchgate.net/post/Does-Spearmans-rho-have-any-advantage-over-Kendalls-tau     -->\n\n\n\n<!----------------->\n<!--- New slide --->\n<!----------------->\n## Wann ist ein Korrelationskoeffizient groß oder klein?\n- Pauschal schwer zu beantworten\n- In der psychologischen Literatur hat sich folgende Nomenklatur nach Jacob Cohen^[Cohen J. (1988). Statistical Power Analysis for the Behavioral Sciences. New York, NY: Routledge Academic] eingebürgert:\n\n<!---  Table --->\n| Korrelation (r) | Nomenklatur |\n|-|-|\n| 0.1-0.29  | \"kleiner\" Effekt |\n| 0.3-0.49 | \"mittlerer\" Effekt |\n| 0.5-1 | \"großer\" Effekt |\n\n: {tbl-colwidths=\"[20, 20]\"}\n\n- Allerdings fügt Cohen im gleichen Artikel hinzu:\n\n> These proposed conventions were set forth throughout with much diffidence [Zurückhaltung], qualifications [Bedingungen], and invitations **not to employ** them if possible.\n\n- Bei der Beurteilung sollte der Korrelationskoeffizient, wie jede Effektgröße, immer in Relation zu typischen Werten im jeweiligen Forschungsfeld gesetzt werden.\n\n<!----------------->\n<!--- New slide --->\n<!----------------->\n## [Wann ist ein Korrelationskoeffizient groß oder klein?]{color=\"darkred\"}\n\n![](images/kein_klausurstoff.png){.absolute bottom=0 right=0 height=120px}\n\n- In einer kürzlichen Metaanalyse von Lovakov Agadullina (2021)^[Lovakov A, Agadullina ER (2021) Empirically derived guidelines for effect size interpretation in social psychology. Eur J Soc Psychol 51:485–504.] wurden typische Effektstärken in der Sozialpsychologie untersucht:\n\n![](images/lovakov_2021.png){height=400px}\n\n- Erkenntnis: \"mittlere\" und \"starke\" Korrelationen (definiert als das 50%- und 75%- Quantil)<br> sind in der Realität kleiner als von Cohen angenommen\n\n<!-- \n- Ob der resultierende Korrelationskoeffizient eine sinnvolle Charakterisierung der Daten ist, hängt von weiteren Faktoren ab:\n\n1. Daten sollten Intervallskalenniveau haben (aber diskrete Daten möglich!)\n    - Sonst: bei unklaren Abständen zwischen Ausprägungen der Variable ist keine Einschätzung der Linearität möglich\n    - Dies ist das wichtigste Kriterium\n\n\n2. Keine (zu starken) Ausreißer\n    - Sonst: Die Korrelation kann bereits durch einzelne Ausreißer massiv verzerrt werden\n\n\n\n3. Varianzhomogenität (Homoskedastizität): die Abweichungen der Punkte um die gedachte Gerade\n    - Sonst:\n\nHomoskedastizität -->\n\n## \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}