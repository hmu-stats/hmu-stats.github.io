---
title: "Vorlesung 08: Inferenzstatistik"
---


## Der Forschungsprozess {.hcenter-slide}

```yaml { .animate src="images/scientific_process.svg"}
setup:
    - element: "#inference"
      modifier: function() { this.node.style.fill = 'green'; }
    - element: "#inferencebg"
      modifier: function() { this.node.style.fill = '#d8ffe2';}
```

## Was ist Inferenzstatistik?

:::: {.columns}
::: {.column width="45%"}
<div class="vspace-small"></div>
![Die statistische Prozess in der Psychologie auf einen Blick: eine Teilmenge von Versuchspersonen wird aus der Population gezogen &mdash; die Stichprobe. In der Stichprobe werden Daten $(x_1, x_2, ..., x_n)$ eines Merkmals gemessen. Mit Methoden der deskriptiven Statistik werden **Kennwerte** in der Stichprobe beschrieben $(\bar{X}, s^2, s)$. Letztendlich ist das Ziel ein Rückschluss auf die wahren **Parameter** der Population $(\mu, \sigma^2, \sigma)$ und das Testen von Hypothesen über die Population. Bildnachweis^[https://stats.libretexts.org/Bookshelves/Introductory_Statistics/Introductory_Statistics_(Shafer_and_Zhang)/01%3A_Introduction_to_Statistics/1.01%3A_Basic_Definitions_and_Concepts]](images/grand_picture_of_statistics_update.png)
:::
::: {.column width="55%"}
- In den vergangenen Vorlesungen haben wir kennengelernt, wie Merkmale in Stichproben quantitativ beschrieben werden können &mdash; wir haben **deskriptive Statistik** betrieben.
- In den meisten Fällen ist das Ziel in der Psychologie allerdings nicht die Beschreibung der spezifischen Stichprobe, sondern ein Rückschluss &ndash; eine Inferenz, eine Verallgemeinerung &ndash; auf die Population.
- Die Mathematik hinter diesem Inferenzprozess ist Gegenstand der [**Inferenzstatistik**]{color="navy"}.
:::
::::



<!----------------->
<!--- New slide --->
<!----------------->
## Was ist Inferenzstatistik?

<div class="vspace-medium"></div>

![](images/population_sample_schaefer.png)

**Zentrale Frage:** Ist das Ergebnis meiner Studie eine gute Schätzung für die wahren Verhältnisse in der Population?


# Das Gesetz der großen Zahlen

<div class="vspace-medium"></div>

Was passiert wenn meine Stichprobe immer größer wird ($n\longrightarrow\infty$)?&nbsp;&nbsp;&nbsp;

## Das Gesetz der großen Zahlen

:::: {.columns}
::: {.column width="73%"}

**Beispiel: gezinkter Würfel**

- Sie wollen überprüfen, ob ein Würfel zu Ihrem Nachteil gezinkt ist, so dass die "Eins" überproportional häufig kommt. Dazu führen Sie eine Reihe von Probewürfen durch.
- Sollte der Würfel *nicht gezinkt* sein, sollte die Wahrscheinlichkeit einer "Eins" genau $\frac{1}{6}$ ist.
- Sie zählen die Häufigkeit einer "Eins" nach 10, 100, 1000 und 10000 Würfen.
:::
::: {.column width="27%"}
![](images/gezinkter_wuerfel.png)
:::
::::

<!-- <div class="vspace-small"></div> -->

:::{.fragment}
:::{style="margin-top: -10px"}
[Ergebnis:]{.underline}
:::

<!---  Table --->
||||||
|-|:-:|:-:|:-:|:-:|
|  | 10 Würfe | 100 Würfe | 1000 Würfe | 10000 Würfe |
| Anzahl Einsen | 3 | 20 | 172 | 1669 |
| Relative Häufigkeit | $\frac{3}{10}=0.3$ | $\frac{20}{100}=0.2$ | $\frac{172}{1000}=0.172$ | $\frac{1669}{10000}=0.1669$ |
: {tbl-colwidths="[20, 20, 20, 20, 20]"}
:::

<div class="vspace-small"></div>

- Mit zunehmender Zahl der Würfe nähert sich also Ihre gemessene Wahrscheinlichkeit der erwarteten Wahrscheinlichkeit $\frac{1}{6}=0{,}1666..\;$ an &mdash; dies ist das [**Gesetz der großen Zahlen**]{color="navy"}!

:::{.fragment style="margin-top: -15px"}
(&rArr; ... und offenkundig ist der Würfel nicht gezinkt!)
:::



<!----------------->
<!--- New slide --->
<!----------------->
## Das Gesetz der großen Zahlen

::: {.vcenter}


::: {.definition .fragment}
<!---  Definition--->
|||
|:-:|-|
|||
| ![](images/definition.svg){height=70px} | **Gesetz der großen Zahlen:** Die gemessene relative Häufigkeit eines Zufallsereignisses nähert sich immer weiter an die tatsächliche Wahrscheinlichkeit dieses Ereignisses an, je häufiger das Zufallsexperiment durchgeführt wird. | 
|||
: {tbl-colwidths="[10, 90]"}
:::

<div class="vspace-medium"></div>

:::{.fragment}
Das Gesetz der großen Zahlen ist ein mathematischer **Grenzwertsatz**, weil er das Verhalten einer Größe in einem Grenzfall ($\text{Anzahl Zufallsexperimente }\rightarrow \infty$) beschreibt.
:::

:::
<!----------------->
<!--- New slide --->
<!----------------->
## Das Gesetz der großen Zahlen

Im nächsten Schritt übertragen wir das ganze auf **Verteilungen**:

- Im Würfelbeispiel haben wir bislang die relative Häufigkeit einer *einzelnen* Merkmalsausprägung (Würfelzahl 1) betrachtet
- Natürlich gilt das Gesetz der großen Zahlen aber für alle Merkmalsausprägungen &ndash; im Würfelbeispiel also auch für die Zahlen 2 bis 5.
- Die gemessene relative Häufigkeit *aller* Merkmalsausprägungen einer Zufallsvariablen $X$ wird auch als die **empirische Häufigkeitsverteilung** oder **empirische Stichprobenverteilung** von $X$ bezeichnet.

:::: {.columns}
::: {.column width="60%"}
- Im Würfelbeispiel ist die Zufallsvariable die gewürfelte Zahl. Wir erwarten, dass alle Zahlen 1-6 gleich wahrscheinlich sind, oder mit anderen Worten: wir erwarten dass sich die empirische Häufigkeitsverteilung einer **uniformen Verteilung** annähert:
:::
::: {.column width="40%"}
:::{.fragment}
![](images/dices_uniform.png){height=250px}
:::
:::
::::


## Das Gesetz der großen Zahlen

- Analog nähert sich hier mit steigender Zahl von Würfen die gemessene Häufigkeitsverteilung der erwarteten Häufigkeitsverteilung an:

:::{.fragment}
![](images/dices_experiment.png){height=320px}
:::

<div class="vspace-medium"></div>

:::{.fragment}
Das Gesetz der großen Zahlen kann deshalb auch auf Verteilungen bezogen werden:
:::

::: {.fragment .definition}
<!---  Definition--->
|||
|:-:|-|
|||
| ![](images/definition.svg){height=70px} | **Gesetz der großen Zahlen (2):** Die gemessene relative Häufigkeits*verteilung* (=Wahrscheinlichkeitsverteilung) einer Zufallsvariablen nähert sich immer weiter an die tatsächliche Wahrscheinlichkeitverteilung der Variable an, je häufiger das Zufallsexperiment durchgeführt wird.  | 
|||
: {tbl-colwidths="[10, 90]"}
:::

<!-- ```{python}
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import norm

np.random.seed(0)

fontsize = 14

mu = 5
std = 1.5
x = np.linspace(0, 10, 100)

N = 8000
s = np.random.normal(mu, std, N)
bins = np.arange(0, 10.1, 1)
counts = np.histogram(s, bins=bins)[0]

plt.figure(figsize=(6.5, 2.5))
plt.subplot(121)
plt.bar(bins[:-1]+0.5, 8e7*norm.pdf(bins[:-1]+0.5, mu, std), width=1, ec='w', fc='#999')
plt.xlabel('Nasenlänge (cm)', fontsize=fontsize)
plt.ylabel('Absolute Häufigkeit', fontsize=fontsize)
plt.xticks(bins[::2], fontsize=fontsize-2)
plt.yticks(plt.yticks()[0], [f"{np.round(y/1e6):.0f} Mio." for y in plt.yticks()[0]], fontsize=fontsize-2)

plt.subplot(122)
plt.bar(bins[:-1]+0.5, norm.pdf(bins[:-1]+0.5, mu, std), width=1, ec='w', fc='#999')
plt.xlabel('Nasenlänge (cm)', fontsize=fontsize)
plt.ylabel('Relative Häufigkeit', fontsize=fontsize)
plt.xticks(bins[::2], fontsize=fontsize-2)
plt.yticks(fontsize=fontsize-2)
plt.ylim(0, 0.3)

plt.tight_layout()
plt.savefig('images/noselength_normal.png', bbox_inches='tight')

plt.figure(figsize=(3, 2.5))
plt.bar(bins[:-1]+0.5, counts/N, width=1, ec='w', fc='#783c00')
for i, (x, y) in enumerate(zip(bins[:-1], counts/N)):
    plt.annotate(text='', xy=(x-0.15,y+0.005), xytext=(x+1.15,y+0.005), arrowprops=dict(arrowstyle='<->'))
plt.annotate(text='Kategorien-\nbreite $d$', xy=(6.3, 0.165), xytext=(6.5,0.21), arrowprops=dict(arrowstyle='->'))
plt.xlabel('Nasenlänge (cm)', fontsize=fontsize)
plt.ylabel('Relative Häufigkeit', fontsize=fontsize)
plt.xticks(bins[::2], fontsize=fontsize-2)
plt.yticks(fontsize=fontsize-2)
plt.savefig('images/noselength_normal_relative.png', bbox_inches='tight')
``` -->


<!-- ```{python}

``` -->
<!-- ```{python}
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import norm

np.random.seed(0)

fontsize = 14

mu = 5
std = 1.5
fig = plt.figure(figsize=(10, 2.3))

for i, w in enumerate([2, 1, 0.5, 0.2]): 
    x = np.arange(w/2, 10, w)
    y = norm.pdf(x, mu, std)
    y /= np.sum(y)
    plt.subplot(1, 4, i+1)
    plt.bar(x, y, width=w, ec='w', fc='#783c00')
    plt.plot(x, y, 'k-')
    plt.plot(x, y, 'ko', markersize=4)
    if i == 0:
        plt.ylabel('Relative Häufigkeit', fontsize=fontsize)
    plt.xticks(np.arange(0,10.1,2), fontsize=fontsize-2)
    plt.yticks(fontsize=fontsize-2)
    plt.title(f'Kategorienbreite $d={w}cm$', fontsize=fontsize-2)

fig.text(0.5, -0.03, 'Nasenlänge (cm)', fontsize=fontsize, ha='center')

plt.tight_layout()
plt.savefig('images/probability_density_demo.png', bbox_inches='tight')

``` -->

<!-- ```{python}

``` -->

<!-- ```{python}
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import norm

np.random.seed(0)

fontsize = 13

mu = 5
std = 1.5
fig = plt.figure(figsize=(3, 2.7))
y = norm.pdf(x, mu, std)
y /= np.sum(y)
plt.plot(x, y, 'k-', color='#777', lw=2)
plt.xlabel('Nasenlänge (cm)', fontsize=fontsize)
plt.ylabel('Wahrscheinlichkeitsdichte', fontsize=fontsize)
plt.xticks(np.arange(0,10.1,2), fontsize=fontsize-2)
plt.yticks(fontsize=fontsize-2)
plt.ylim(0, 0.029)

plt.savefig('images/noselength_probability_density.png', bbox_inches='tight')

``` -->

<!-- ```{python}

``` -->

<!-- ```{python}
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import norm

np.random.seed(0)

fontsize = 14

mu = 5
std = 1.5
x = np.linspace(0, 10, 100)

N = 40
s = np.random.normal(mu, std, N)
bins = np.arange(0, 10.1, 1)
counts = np.histogram(s, bins=bins)[0]

plt.figure(figsize=(6.5, 2.5))
plt.subplot(121)
plt.bar(bins[:-1]+0.5, counts, width=1, ec='w', fc='#783c00')
plt.xlabel('Nasenlänge (cm)', fontsize=fontsize)
plt.ylabel('Absolute Häufigkeit', fontsize=fontsize)
plt.xticks(bins[::2], fontsize=fontsize-2)
plt.yticks(fontsize=fontsize-2)
# plt.title("$n=40$")
plt.subplot(122)
plt.bar(bins[:-1]+0.5, counts/N, width=1, ec='w', fc='#783c00')
plt.xlabel('Nasenlänge (cm)', fontsize=fontsize)
plt.ylabel('Relative Häufigkeit', fontsize=fontsize)
plt.xticks(bins[::2], fontsize=fontsize-2)
plt.yticks(fontsize=fontsize-2)
# plt.title("$n=40$")
plt.tight_layout()
plt.savefig('images/noselength_sample.png', bbox_inches='tight')
``` -->

## Das Gesetz der großen Zahlen in der Psychologie

- Im Kontext der Psychologie begegnet uns das Gesetz der großen Zahlen, wenn wir die **relative Häufigkeitsverteilung eines Merkmals in der Stichprobe** betrachten.

<!---  Example --->
::: {.fragment}
::: {.example}
|||
|:-:|-|
| ![](images/example.png){height=70px} | Sie messen die Nasenlänge in ihrer Kohorte. Von jeder Person notieren Sie die \
Nasenlänge und tragen diese in einem Histogramm auf. ![](images/noselength.png){height=75px style="float:right; margin-top:-33px !important"} |
: {tbl-colwidths="[10, 90]"}
:::


<!---  Table --->
||||
|-|-|-|
| | Ihr Ergebnis in Ihrer Stichprobe sieht folgendermaßen aus: | ![](images/noselength_sample.png) |
|  ![](images/tengumask.png) | Der Gott der Nasen hat uns zum Vergleich die **tatsächliche** Häufigkeitssverteilung von Nasenlängen in Deutschland(=Population) anvertraut:| ![](images/noselength_normal.png) |
|  |  | |

: {tbl-colwidths="[12, 44, 44]"}

::: {style="margin-top: -2px"} 
- Auch hier würden wir erwarten, dass sich die Verteilung des Merkmals in der Stichprobe an die tatsächliche Verteilung in der Population annähert, je größer die Stichprobe ist.
:::

:::
 
<!-- ```{python}
 
``` -->

<!-- ```{python}
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import norm

np.random.seed(0)

fontsize = 14

mu = 5
std = 1.5
x = np.linspace(0, 10, 100)

N = [40, 400, 4000, 40000]
bins = np.arange(0, 10.1, 1)

plt.figure(figsize=(11, 2.5))

for i, n in enumerate(N):
    plt.subplot(1,4,i+1)
    s = np.random.normal(mu, std, n)
    counts = np.histogram(s, bins=bins)[0]
    plt.bar(bins[:-1]+0.5, norm.pdf(bins[:-1]+0.5, mu, std), width=1, ec='#999', fc='#999')
    plt.bar(bins[:-1]+0.5, counts/n, width=1, ec='w', fc='#783c00', alpha=0.33)
    plt.xlabel('Nasenlänge (cm)', fontsize=fontsize)
    if i == 0:
        plt.ylabel('Relative Häufigkeit', fontsize=fontsize)
    plt.xticks(bins[::2], fontsize=fontsize-2)
    plt.yticks(fontsize=fontsize-2)
    plt.title(rf"$n={n}$")
    plt.ylim(0, 0.31)
plt.tight_layout()
plt.savefig('images/noselength_limit.png', bbox_inches='tight')
``` -->

<!----------------->
<!--- New slide --->
<!----------------->
## Gesetz der großen Zahlen in der Psychologie

- Jede zusätzliche Versuchsperson entspricht einem neuen **Zufallsexperiment** ("Würfelwurf").
    - Annahme: Versuchspersonen werden *zufällig* aus der Population gezogen.
- Wir können die Annäherung der Stichprobenverteilung an die Populationverteilung mit $n\rightarrow N$ simulieren ($N$ = Umfang der Population, 80 Mio. Deutsche in diesem Beispiel):

:::{.fragment}
![Die Stichprobenverteilungen sind in braun schattiert, die (gleichbleibende) Häufigkeitsverteilung der Population in grau, die in diesem Fall alle 80 Mio. Deutsche umfasst. Beachte, dass in diesem Fall die *relative* Häufigkeit dargestellt ist.](images/noselength_limit.png)

:::
- Ähnlich wie beim Würfelwurf nähert sich die Verteilung des Merkmals in der Stichprobe an die tatsächliche Verteilung in der Population an.
