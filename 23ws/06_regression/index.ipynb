{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Vorlesung 06: Regression\"\n",
        "---"
      ],
      "id": "47ee3d52"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## {.blackslide .center}\n",
        "\n",
        "<div class=\"vspace-medium\"></div>\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"65%\"}\n",
        "Kurze Erinnerung: beim letzten Mal fanden wir einen Zusammenhang von TikTok-Online-Zeit und Entzündungsparametern:\n",
        ":::\n",
        "::: {.column width=\"35%\"}\n",
        "::: {.content-hidden when-format=\"pdf\"}\n",
        "![](images/paradoxia_group_of_scientists.png){.hcenter-image height=200px style=\"margin-top:-100px !important\"}\n",
        "<!-- Source: Midjourney -->\n",
        ":::\n",
        ":::\n",
        "::::\n",
        "\n",
        "<div class=\"vspace-large\"></div>\n",
        "![](images/paradoxia_histogram_correlation_paradoxiker.png){height=300px}\n",
        "\n",
        "Bei der Interpretation stellt sich einerseits die Kausalitätsfrage, andererseits, wie stark der Zusammenhang tatsächlich ist. Da die Pearson-Korrelation lediglich den **Grad der Linearität** beurteilt, fragen Sie sich: um wie viel erhöhen sich die Entzündungsparameter pro Stunde zusätzliche Zeit auf TikTok? Oder umgekehrt: um wie viel erhöht sich die Zeit auf TikTok, wenn die Entzündungswerte um einen Wert x ansteigen?\n",
        "\n",
        "\n",
        "<!-- \n",
        "## Messwiederholungen: Liniendiagramme{.blackslide}\n",
        "\n",
        "Gerade als sich die Hinweise auf die Hypothese des viralen Ursprungs von Paradoxia verdichten, wird ein Blogpost des Chaos Computer Club (CCC) in der Öffentlichkeit bekannt. Anonymen Hackern gelang es, auf die letzten 12 Monate Tik-Tok-Historie von 1800 Personen zuzugreifen &ndash; darunter viele Paradoxiker!\n",
        "\n",
        "<div class=\"vspace-medium\"></div>\n",
        "\n",
        "![Die Abbildung zeigt ein Liniendiagramm mit Fehlerbalken. Bei wiederholten Messungen Personen bietet es sich an, die einzelnen Messzeitpunkte mit Linien zu verbinden, um den zeitlichen Zusammenhang zu unterstreichen. Überlegen Sie sich: welche Aussage macht ein einzelner Fehlerbalken in dieser Abbildung?](images/paradoxia_lineplot.png) -->\n",
        "\n",
        "<!-- ```{python}\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "fontsize=15\n",
        "np.random.seed(0)\n",
        "\n",
        "plt.style.use('dark_background')\n",
        "plt.figure()\n",
        "\n",
        "x = np.arange(12)\n",
        "y_paradoxia = 0.2 * x - 0.3 + 1*np.random.rand(12)\n",
        "yerr_paradoxia = 0.1*np.random.rand(12)+0.1\n",
        "y_control = 0.03 * x + 0.15 + 0.2*np.random.rand(12)\n",
        "yerr_control = 0.1*np.random.rand(12)+0.1\n",
        "\n",
        "plt.plot(x, y_paradoxia, color='#beddff', lw=2)\n",
        "plt.errorbar(x, y_paradoxia, yerr=yerr_paradoxia, fmt='o', markersize=5, capsize=10, capthick=2, elinewidth=2, ecolor='#beddff', mfc='#beddff', mec='#beddff', label='Paradoxia')\n",
        "plt.plot(x, y_control, color='#f5f5b9', lw=2)\n",
        "plt.errorbar(x, y_control, yerr=yerr_control, fmt='o', markersize=5, capsize=10, capthick=2, elinewidth=2, ecolor='#f5f5b9', mfc='#f5f5b9', mec='#f5f5b9', label='Kontroll')\n",
        "plt.xlabel('Monate', fontsize=fontsize)\n",
        "plt.xticks(x, x+1, fontsize=fontsize-2)\n",
        "plt.yticks(fontsize=fontsize-2)\n",
        "plt.ylabel('Stunden Tik Tok / 24h', fontsize=fontsize)\n",
        "# plt.xlim(xlim)\n",
        "# plt.ylim(ylim)\n",
        "plt.legend(fontsize=fontsize)\n",
        "plt.savefig('images/paradoxia_lineplot.png', bbox_inches=\"tight\",\n",
        "            pad_inches=0)\n",
        "``` -->\n",
        "\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "<!-- ## {.blackslide .center}\n",
        "\n",
        "::: {.content-hidden when-format=\"pdf\"}\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/paradoxia_group_of_scientists.png){.hcenter-image height=250px}\n",
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/paradoxia_lineplot.png)\n",
        ":::\n",
        "::::\n",
        ":::\n",
        "<div class='vspace-small'></div>\n",
        "\n",
        "Die Daten des CCC geben der alternativen Hypothese neue Nahrung, nach der es sich bei Paradoxia um ein soziales Tik-Tok-Phänomen handelt. Leider ist nicht bekannt, zu welchem Zeitpunkt die Paradoxiker in dem Datensatz erkrankt sind. \n",
        "Die erste Frage an Sie als Task Force lautet: welche Möglichkeiten von Ursache und Wirkung könnten plausibel sein? -->\n",
        "\n",
        "## Woher kommt der Ausdruck \"Regression\"?\n",
        "\n",
        "- Lateinisch »regredi« = „umkehren, zurückgehen“\n",
        "- Psychoanalyse: Regression = Zurückfallen in kindliche Verhaltensmuster\n",
        "\n",
        "> Wir heißen es **Regression**, wenn sich im Traum die Vorstellung in das sinnliche Bild zurückverwandelt, aus dem sie irgend einmal hervorgegangen ist.\n",
        "\n",
        "[Sigmund Freud (1900). \"Traumdeutung\".]{.quote-citation}\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"70%\"}\n",
        "- In die Statistik wird der Ausdruck \"Regression\" klassischerweise auf **Francis Galton** (Cousin von Charles Darwin) attribuiert, der bereits 1885 ein Phänomen beschrieb, das er *regression toward mediocrity* (**Regression zur Mitte**) taufte\n",
        "- Das Phänomen bestand darin, dass Nachfahren großer Eltern dazu tendieren, selbst nur durchschnittlich groß zu werden\n",
        "- Neuere Forschung zeigt allerdings, dass sich Galton selbst wohl noch nicht des statistischen Ursprungs dieses Phänomens bewusst war und eine biologische Erklärung favorisierte^[Krashniak A, Lamm E (2021) Francis Galton’s regression towards mediocrity and the stability of types. Studies in History and Philosophy of Science Part A 86:6–19.].\n",
        ":::\n",
        "::: {.column width=\"30%\"}\n",
        "![](images/galton.png)\n",
        ":::\n",
        "::::\n",
        "\n",
        "## Regression\n",
        "\n",
        "- Dem Wortsinn nach ist Ziel der [**Regression**]{color=\"navy\"} eine abhängige Variable auf eine oder mehrere unabhängige Variablen zurückzuführen (auf diese zu *regredieren*)\n",
        "- Eingängiger ist aber die umgekehrte Formulierung: Ziel der Regression ist es, auf Basis der unabhängigen Variablen die eine abhängige Variable **vorherzusagen** oder **zu erklären**\n",
        "    - **Unabhängige Variable(n) = vorher*gesagte* oder erklär*te* Variable(n)** (\"**U**rsache\")\n",
        "    - **Abhängige Variable = vorher*sagende* oder erklär*ende* Variable** (\"**A**uswirkung\")\n",
        "\n",
        "![](images/regression_basics.png)\n",
        "\n",
        "::: {style=\"margin-top: -5px\"}\n",
        "- Beispiel: eine Studie untersucht, ob sich Lebenszufriedenheit auf Basis von sportlicher Aktivität vorhersagen/erklären lässt.\n",
        "    - [Lebenszufriedenheit: unabhängige/vorhersagende/erklärende Variable; &mdash; sportliche Aktivität: abhängige/vorhergesagte/erklärte Variable]{style=\"font-size: 18px !important\"}\n",
        ":::\n",
        "\n",
        "## Regression\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"70%\"}\n",
        "- Im Gegensatz zur Korrelation bestimmt die Regression nicht die Linearität des Zusammenhangs (vielmehr wird dies vorausgesetzt), sondern die **Steigung** des Zusammenhangs\n",
        "- Aus diesem Grund ist die Regression (wieder im Gegensatz zur Korrelation) nicht symmetrisch &ndash; die Steigung ist abhängig davon welche Variable als abhängig und unabhängig deklariert wird.\n",
        "    - Wie wir noch sehen werden, ist es auch nicht gestattet, die Regressionsgleichung zu invertieren ( $\\hat{x}_i = \\frac{1}{b_1}y_i-\\frac{b_0}{b_1}$) &mdash; im Allgemeinen ist $\\frac{1}{b_1}$ *nicht* die Steigung, wenn die Rollen von X und Y vertauscht werden.\n",
        ":::\n",
        "::: {.column width=\"30%\"}\n",
        "![](images/regression_basics_plot.png)\n",
        ":::\n",
        "::::\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"60%\"}\n",
        "- Die Vorhersage/Erklärung von X durch Y geschieht durch eine Gleichung &ndash; die **Regressionsgleichung** &ndash; die im Streudiagramm als Gerade eingezeichnet werden kann.\n",
        ":::\n",
        "::: {.column width=\"40%\"}\n",
        "::: {style=\"margin-top: -25px\"}\n",
        "![](images/regression_sports.png)\n",
        ":::\n",
        ":::\n",
        "::::\n",
        "\n",
        "## [Bestimmung der Regressionsgerade: Methode der kleinsten Quadrate]{style=\"font-size: 37px\"}\n",
        ":::: {.columns}\n",
        "::: {.column width=\"58%\"}\n",
        "- Ziel der Regression ist es, die Gerade zu finden, die die Datenpunkte möglichst gut abbildet &mdash; es gibt jedoch verschiedene Definitionen dessen, was  \"möglichst gut\" heißt\n",
        "- Die häufigste Variante ist die [**Methode der kleinsten Quadrate**]{color=\"navy\"}, bei der die Gerade so gewählt wird, dass die **Summe der quadrierten [senkrechten Abstände]{color=\"darkmagenta\"} jedes Datenpunktes zur Geraden** minimal ist\n",
        "    - Engl. *ordinary least square*\n",
        ":::\n",
        "::: {.column width=\"42%\"}\n",
        "![](images/regression_sports_distances.png)\n",
        ":::\n",
        "::::\n",
        "- Die **einfache Regression** mit nur einer unabhängigen Variablen hat zwei freie Parameter, um die Gerade an die Datenpunkte anzupassen (zu \"fitten\"):\n",
        "    - **y-Achsenabschnitt** $b_0$ (engl. *intercept*)\n",
        "    - **Steigung** $b_1$ (engl. *slope*)\n",
        "- Exakt 0 wären die senkrechten Abstände nur, wenn alle Punkte auf einer perfekten Gerade liegen. Dies ist eigentlich nie der Fall. Die verbleibenden senkrechten Abstände der Datenpunkte von der gefitteten Geraden werden [**Residuen**]{color=\"navy\"} genannt.\n",
        "\n",
        "## Warum weichen die Datenpunkte überhaupt von einer Geraden ab?\n",
        ":::: {.columns}\n",
        "::: {.column width=\"60%\"}\n",
        "Verschiedene Gründe:\n",
        "\n",
        "- Variablen korrelieren überhaupt nicht\n",
        "<!-- - Zusammenhang ist nichtlinear -->\n",
        "- Einfluss von Störvariablen\n",
        "- Messungenauigkeit\n",
        ":::\n",
        "::: {.column width=\"40%\"}\n",
        "![](images/regression_weight_height_annot.png)\n",
        ":::\n",
        "::::\n",
        "\n",
        "In der Psychologie gibt es (bis auf triviale Fälle) keine perfekten linearen Zusammenhänge, d.h. es verbleiben immer **Residuen** $\\Delta \\hat{y_i}$:\n",
        "\n",
        "$$\n",
        "\\text{Residuum:}\\quad \\Delta \\hat{y_i} = \\hat{\\epsilon}_i = \\hat{y_i} - y_i\n",
        "$$\n",
        "\n",
        "::: {.merke style=\"margin-top: 32px\"}\n",
        ":::: {.columns}\n",
        "::: {.column width=\"5%\"}\n",
        "::: {style=\"margin-top: 18px\"}\n",
        "![](images/merke.png){height=\"55px\"}\n",
        ":::\n",
        ":::\n",
        "::: {.column width=\"95%\"}\n",
        "Residuum = Differenz von vorhergesagtem Wert $\\hat{y_i}$ und tatsächlichem Wert $y_i$\n",
        ":::\n",
        "::::\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        ":::{.content-hidden when-format=\"pdf\"}\n",
        "## [https://shiney.zoology.ubc.ca/whitlock/Guessing_correlation/]{style=\"font-size: 37px\"}\n",
        "\n",
        "<iframe width=100% height=\"100%\" src=\"https://shiney.zoology.ubc.ca/whitlock/Guessing_correlation/\"></iframe>\n",
        ":::\n",
        "\n",
        "<!-- https://www.guessthecorrelation.com/ -->\n",
        "\n",
        "## Streudiagramm bei Regression\n",
        "\n",
        "- in der Regel UV auf der x-Achse und AV auf der y-Achse\n",
        "\n",
        "![Bildnachweis^[https://flowingdata.com/2014/06/25/duck-vs-rabbit-plot/]](images/rabbit_duck.jpg)\n",
        "\n",
        "\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        ":::{.content-hidden when-format=\"pdf\"}\n",
        "## [https://phet.colorado.edu/sims/html/graphing-slope-intercept/latest/graphing-slope-intercept_all.html?locale=de]{style=\"font-size:22px\"}\n",
        "<iframe width=100% height=100% src=\"https://phet.colorado.edu/sims/html/least-squares-regression/latest/least-squares-regression_all.html?locale=de\"></iframe>\n",
        ":::\n",
        "\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        ":::{.content-hidden when-format=\"pdf\"}\n",
        "## [https://phet.colorado.edu/sims/html/least-squares-regression/latest/least-squares-regression_all.html?locale=de]{style=\"font-size:22px\"}\n",
        "<iframe width=100% height=100% src=\"https://phet.colorado.edu/sims/html/least-squares-regression/latest/least-squares-regression_all.html?locale=de\"></iframe>\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "## y-Achsenabschnitt $b_0$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "## Steigung $b_1$\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "## Totale, erklärte und Residuenquadratsumme\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"49%\"}\n",
        "- Die Methode der kleinsten Quadrate minimiert die **Residuenquadratsumme (SQR)**:\n",
        "$$\n",
        "SQR = \\sum\\left(\\hat{y}_i-y_i\\right)^2 = \\sum\\hat{\\epsilon}_i^2\n",
        "$$\n",
        "- Diese wiederum lässt sich in Bezug setzen zur **totalen Quadratsumme (SQT)** und zur **erklärten Quadratsumme (SQE)**:\n",
        "\n",
        "$$\n",
        "SQR = SQT - SQE\\qquad\\text{bzw.}\n",
        "$$\n",
        "$$\n",
        "SQT = SQT + SQR\n",
        "$$\n",
        "\n",
        "- Mit\n",
        "\n",
        "$$\n",
        "SQE = \\sum\\left(\\hat{y}_i-\\bar{Y}\\right)^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "SQT = \\sum\\left(y_i-\\bar{Y}\\right)^2\n",
        "$$\n",
        ":::\n",
        "::: {.column width=\"51%\"}\n",
        "<div class=\"vspace-medium\"></div>\n",
        "![](images/quadratsummen2.png)\n",
        ":::\n",
        "::::\n",
        "\n",
        "- Falls alle Punkte exakt auf der Regressionsgeraden liegen ($\\hat{y}_i = y_i$) ist die erklärte Quadratsumme identisch der totalen Quadratsumme ($SQE = SQT$) und die Residuenquadratsumme ist 0\n",
        "\n",
        "\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "## Bestimmtheitsmaß\n",
        "\n",
        "- Das [**Bestimmtheitsmaß**]{color=\"navy\"} $R^2$ gibt an, wie gut die Datenpunkte durch die Regressionsgerade gefittet werden (\"Anpassungsgüte\")\n",
        "- Es gibt an, welcher Anteil der Datenvarianz $Var(Y)$ durch die Varianz der Vorhersage $Var(\\hat{Y})$ erklärt wird..\n",
        "\n",
        "$$\n",
        "R^2 = \\frac{Var(\\hat{Y})}{Var(Y)} = \\frac{\\sum\\left(\\hat{y_i}-\\bar{Y}\\right)^2}{\\sum\\left(y_i-\\bar{Y}\\right)^2} = \\frac{SQE}{SQT}\n",
        "$$\n",
        "\n",
        "- .. oder äquivalent, den Anteil der erklärten Quadratsumme an der totalen Quadratsumme.\n",
        "\n",
        "::: {.merke}\n",
        ":::: {.columns}\n",
        "::: {.column width=\"5%\"}\n",
        "::: {style=\"margin-top: 18px\"}\n",
        "![](images/merke.png){height=\"55px\"}\n",
        ":::\n",
        ":::\n",
        "::: {.column width=\"95%\"}\n",
        "Bei einer einfachen Regression gilt: $R^2 = r_{XY}^2$<br>\n",
        "Das Bestimmtheitsmaß ist bei einer einfachen Regression also identisch dem quadrierten Korrelationskoeffizienten zwischen den Variablen X und Y!\n",
        ":::\n",
        "::::\n",
        ":::\n",
        "\n",
        "<div class=\"vspace-medium\"></div>\n",
        "\n",
        "<!---  Example --->\n",
        "::: {.example}\n",
        "|||\n",
        "|:-:|-|\n",
        "|||\n",
        "| ![](images/example.png){height=70px} | Lebenszufriedenheit und sportliche Aktivität haben eine Korrelation von $r_{XY}=0{,}8$.<br>&rArr; Sportliche Aktivität erklärt $\\;r_{XY}^2=0{,}64\\;\\widehat{=}\\;64\\%$ der Varianz von Lebenszufriedenheit (und umgekehrt). |\n",
        "|||\n",
        ": {tbl-colwidths=\"[10, 90]\"}\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "## Bestimmtheitsmaß\n",
        "\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "![Beispiele für zwei Regressionen mit Bestimmtheitsmaß $R^2=98{,}92\\%$ und $R^2=57{,}13\\%$. Selbst das schwächere Beispiel mit $57{,}13$ wäre für typische Effekte in der Psychologie noch ein außerordentlich hoher Wert.^[https://de.wikipedia.org/wiki/Datei:R2values.svg]](images/coefficient_of_determination_examples.png)\n",
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "- Das Bestimmtheitsmaß $R^2$ gibt an, wie gut sich die Variable $Y$ mit einer linearen Gleichung basierend auf $X$ vorhersagen lässt.\n",
        "- Der Maximal wert von $R^2$ ist 1. In diesem Fall erklärt die lineare Gleichung in $X$ die Daten $Y$ perfekt.\n",
        "- Da das Bestimmtheitsmaß $R^2$ angibt, welcher Anteil der Varianz in den Daten durch die lineare Gleichung erklärt wird, wird es manchmal in Prozent ausgedrückt (d.h. mit 100 multipliziert; wie links). Der Maximalwert von $R^2$ ist dann 100%.\n",
        ":::\n",
        "::::\n",
        "\n",
        "\n",
        "\n",
        "## [Analytische Form der Regressionskoeffizienten (einfache Regression)]{style=\"font-size:38px\"}\n",
        "\n",
        "Die optimalen [**Regressionskoeffizienten**]{color=\"navy\"} $b_0$ (Achsenabschnitt) und $b_1$ (Steigung) lassen sich analytisch herleiten:\n",
        "\n",
        "![](images/regression_derivation3.png){height=150px}\n",
        "\n",
        "- Aus den Mittelwerten und der Kovarianz von $X$ und $Y$, sowie der Varianz von $X$, lassen sich also die Regressionskoeffizienten vollständig bestimmen.\n",
        "- Auch hier zeigt sich wieder die Assymmetrie der Regression: während bei der Formel für die Pearson-Korrelation $Var(X)Var(Y)$ im Nenner steht, ist es beim Regressionskoeffizienten lediglich die Varianz der unabhäbgigen Variable $Var(X)$\n",
        "- Wäre stattdessen $Y$ die unabhängige Variable, stünde $Var(Y)$ im Nenner, und der Regressionskoeffizient hätte i.d.R. einen anderen Wert.\n",
        "    - Dies ist auch der Grund, weshalb die Regressionsgleichung nicht einfach invertiert werden darf: \n",
        "    \n",
        "::: {style=\"margin-top: -23px\"}\n",
        "$$\n",
        "\\require{cancel}\n",
        "\\scriptsize{\n",
        "\\cancel{\n",
        "\\hat{x}_i = \\frac{1}{b_1}y_i-\\frac{b_0}{b_1}\n",
        "}\\\\\n",
        "\\left(\\text{.. und }\\frac{1}{b_1}\\text{ im Allgemeinen }\\textbf{nicht}\\text{ der Regressionskoeffizient für $Y$ als unabhängige Variable ist.}\\right)\n",
        "}\n",
        "$$\n",
        ":::\n",
        "\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "## Zusammenhang Regression &harr; Korrelation\n",
        "\n",
        "- Folgendener Zusammenhang gilt zwischen der Steigung $b_1$ und dem Korrelationskoeffizienten $r_{XY}$:\n",
        "$$\n",
        "\\scriptsize{\n",
        "b_1 = \\frac{Cov(X,Y)}{Var(X)} = \\frac{Cov(X,Y)}{s_X^2} = \\frac{s_Y}{s_Y}\\frac{Cov(X,Y)}{s_Xs_X} = \\frac{s_Y}{s_X}\\underbrace{\\frac{Cov(X,Y)}{s_Xs_Y}}_{r_{XY}} = \\frac{s_Y}{s_X}r_{XY}\n",
        "}\n",
        "$$\n",
        "\n",
        "- Nach $r_{XY}$ auflösen: \n",
        "\n",
        "::: {style=\"margin-top: -15px\"}\n",
        "$$\n",
        "\\quad r_{XY} = \\frac{s_X}{s_Y}b_1\n",
        "$$\n",
        ":::\n",
        "\n",
        "::: {.merke}\n",
        ":::: {.columns}\n",
        "::: {.column width=\"5%\"}\n",
        "::: {style=\"margin-top: 18px\"}\n",
        "![](images/merke.png){height=\"55px\"}\n",
        ":::\n",
        ":::\n",
        "::: {.column width=\"95%\"}\n",
        "Sind die Standardabweichungen $s_X$ und $s_Y$ bekannt, kann aus der Steigung $b_1$ der Regression immer auch der Korrelationskoeffizient $r_{XY}$ bestimmt werden.\n",
        ":::\n",
        "::::\n",
        ":::\n",
        "\n",
        "<div class=\"vspace-medium\"></div>\n",
        "\n",
        "- Der Ausruck $\\frac{s_X}{s_Y}b_1$ wird auch [**standardisierter Regressionskoeffizient**]{color=\"navy\"}  $\\beta_1$ genannt: \n",
        "\n",
        "::: {style=\"margin-top:-15px\"}\n",
        "$$\n",
        "    \\beta_1 = \\frac{s_X}{s_Y}b_1\n",
        "$$\n",
        ":::\n",
        "\n",
        "- Bei der einfachen Regression ist der standardisierte Regressionskoeffizient identisch mit dem Korrelationskoeffizienten: $\\quad\\beta_1 = r_{XY}$\n",
        "\n",
        "<!-- https://stats.stackexchange.com/questions/435699/how-to-derive-the-formula-for-coefficient-slope-of-a-simple-linear-regression -->\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "## Standardisierter Regressionskoeffizient\n",
        "- Wie gesehen erhält man den **standardisierten Regressionskoeffizienten** $\\beta$ durch die Transformation $\\beta_1 = \\frac{s_X}{s_Y}b_1$ \n",
        "- Im Gegensatz zu $b_1$ ist $\\beta_1$ unabhängig von der Skalierung von $X$ und $Y$ (also z.B. ob die Einheit als $cm$ oder $m$ gewählt wurde) &rArr; $\\beta$-Koeffizienten lassen sich besser zwischen verschiedenen Regressionen vergleichen\n",
        "- Analog gilt $\\beta_1=b_1$, wenn die Variablen $X$ und $Y$ vorher **standardisiert** wurden.\n",
        "\n",
        "::: {.definition}\n",
        "<!---  Definition--->\n",
        "|||\n",
        "|:-:|-|\n",
        "|||\n",
        "| ![](images/definition.svg){height=70px} | **Standardisierung einer Variable $X$**&#x2254; Variable $X$ durch ihre Standardabweichung $s_X$ teilen<br>Die Variable $X$ hat nach der Standardisierung die Standardabweichung $s_X=1$. | \n",
        "|||\n",
        ": {tbl-colwidths=\"[9, 91]\"}\n",
        ":::\n",
        "- Wurden sowohl $X$ als auch $Y$ vor der Regression standardisiert, also $s_X=s_Y=1$, so sind die Regressionskoeffizienten automatisch standardisiert:\n",
        "\n",
        "::: {style=\"margin-top: -10px\"}\n",
        "$$\n",
        "\\beta_1 = \\frac{s_X}{s_Y}b_1 = \\frac{1}{1}b_1 = b_1\n",
        "$$\n",
        ":::\n",
        "\n",
        "<!---  Table --->\n",
        "|||\n",
        "|-|-|\n",
        "| $b_1$ | Um welchen Wert ändert sich $Y$ bei einer Änderung von $X$ um den Wert $1$? |\n",
        "| $\\beta_1$ | Um welchen Wert ändert sich $Y$ bei einer Änderung von $X$ um *eine* Standardabweichung $s_X$? |\n",
        "\n",
        ": Interpretation im Kontext der Regressionsgleichungen $\\quad\\hat{Y}=b_0+b_1X\\quad$ bzw. $\\quad\\hat{Y}=\\beta_0+\\beta_1X$ {tbl-colwidths=\"[10, 90]\"}\n",
        "\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "## [Herleitung der Regressionskoeffizienten]{color=\"darkred\"}\n",
        "\n",
        "![](images/kein_klausurstoff.png){.absolute bottom=0 right=0 height=110px}\n",
        "\n",
        "- Die Methode der kleinsten Quadrate entspricht der Minimierung der quadratischen Residuen:\n",
        "$$\n",
        "SQR = \\sum\\left(\\hat{y}_i-y_i\\right)^2 = \\sum\\left(b_0+b_1 x_i-y_i\\right)^2 \\overset{!}{=}\\text{min}\n",
        "$$\n",
        "- Um das Minimum von SQR in Abhängigkeit von $b_0$ und $b_1$ zu finden, setzen wir die Ableitungen von SQR nach den Parametern gleich Null (Infinitesimalrechnung@Schule &#128521;)\n",
        "- Zunächst leiten wir SQR nach $b_0$ ab:\n",
        "$$\n",
        "\\frac{dSQR}{db_0} = \\sum2\\left(b_0+b_1 x_i-y_i\\right)=2nb_0+2\\sum\\left(b_1 x_i-y_i\\right)=0\n",
        "$$\n",
        "\n",
        "::: {style=\"border: 1px solid darkblue\"}\n",
        "$$\n",
        "\\color{darkblue}{\\longrightarrow b_0} = \\frac{1}{n}\\sum\\left(y_i-b_1x_i\\right) = \\frac{1}{n}\\sum y_i-\\frac{b_1}{n}\\sum x_i = \\color{darkblue}{\\bar{Y}-b_1\\bar{X}}\n",
        "$$\n",
        ":::\n",
        "\n",
        "<div class=\"vspace-xlarge\"></div>\n",
        "\n",
        "- ... jetzt benötigen wir noch $b_1$\n",
        "\n",
        "## [Herleitung der Regressionskoeffizienten]{color=\"darkred\"}\n",
        "\n",
        "![](images/kein_klausurstoff.png){.absolute bottom=0 right=0 height=110px}\n",
        "\n",
        "- SQR nach $b_1$ ableiten und gleich Null setzen:\n",
        "$$\n",
        "\\frac{dSQR}{db_1} = \\sum2(b_0+b_1 x_i-y_i)x_i=2b_0\\sum x_i + 2b_1\\sum x_i^2-2\\sum x_i y_i = 0\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\longrightarrow b_1 = \\frac{\\sum x_i y_i}{{\\sum x_i^2}} - \\frac{b_0\\sum x_i}{\\sum x_i^2} \\overset{\\color{darkgreen}{(b_0\\,einsetzen)}}{=} \\frac{\\sum x_i y_i}{{\\sum x_i^2}} - \\frac{\\bar{Y}\\sum x_i}{\\sum x_i^2}+b_1\\frac{\\bar{X}\\sum x_i}{\\sum x_i^2}\n",
        "$$\n",
        "\n",
        "- Alle $b_1$-Terme auf die linke Seite bringen und einige Umformungen vornehmen:\n",
        "\n",
        "::: {style=\"margin-top: -12px\"}\n",
        "$$\n",
        "\\scriptsize{\n",
        "b_1 - b_1\\frac{\\bar{X}\\sum x_i}{\\sum x_i^2} = \\frac{\\sum x_i y_i}{{\\sum x_i^2}} - \\frac{\\bar{Y}\\sum x_i}{\\sum x_i^2}\n",
        "}\n",
        "$$\n",
        ":::\n",
        "\n",
        "\n",
        "$$\n",
        "\\scriptsize{\n",
        "b_1 \\left(1 - \\frac{\\bar{X}\\sum x_i}{\\sum x_i^2}\\right) = \\frac{\\sum x_i y_i - \\bar{Y}\\sum x_i}{\\sum x_i^2}\n",
        "}\n",
        "$$ \n",
        " \n",
        "$$\n",
        "\\scriptsize{\n",
        "b_1 \\left(\\frac{\\sum x_i^2}{\\sum x_i^2} - \\frac{\\bar{X}\\sum x_i}{\\sum x_i^2}\\right) = \\frac{\\sum x_i y_i - \\bar{Y}\\sum x_i}{\\sum x_i^2}\n",
        "}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\scriptsize{\n",
        "b_1 \\frac{\\sum x_i^2 - \\bar{X}\\sum x_i}{\\sum x_i^2} = \\frac{\\sum x_i y_i - \\bar{Y}\\sum x_i}{\\sum x_i^2}\n",
        "}\n",
        "$$\n",
        "\n",
        "::: {style=\"margin-left: -100px\"}\n",
        "$$\n",
        "\\scriptsize{\n",
        "\\longrightarrow    \n",
        "b_1 = \\frac{\\color{darkred}{\\sum x_i^2}}{\\sum x_i^2-\\bar{X}\\sum x_i}\\frac{\\sum x_i y_i - \\bar{Y}\\sum x_i}{\\color{darkred}{\\sum x_i^2}} = \\frac{\\sum x_i y_i - \\bar{Y}\\sum x_i}{\\sum x_i^2-\\bar{X}\\sum x_i}\n",
        "\\overset{\\color{darkgreen}{\\left(\\sum x_i=n\\bar{X}\\right)}}{=} \\frac{\\sum x_i y_i - n\\bar{X}\\bar{Y}}{\\sum x_i^2-n\\bar{X}^2} \\overset{\\color{darkgreen}{:n}}{=} \\frac{\\frac{1}{n}\\sum x_i y_i - \\bar{X}\\bar{Y}}{\\frac{1}{n}\\sum x_i^2-\\bar{X}^2}\n",
        "}\n",
        "$$\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## [Herleitung der Regressionskoeffizienten]{color=\"darkred\"}\n",
        "\n",
        "![](images/kein_klausurstoff.png){.absolute bottom=0 right=0 height=110px}\n",
        "\n",
        "- Zwischenergebnis:\n",
        "\n",
        "::: {style=\"margin-top:-20px\"}\n",
        "$$\n",
        "b_1 = \\frac{\\frac{1}{n}\\sum x_i y_i - \\bar{X}\\bar{Y}}{\\frac{1}{n}\\sum x_i^2-\\bar{X}^2}\n",
        "$$\n",
        ":::\n",
        "\n",
        "- Um zu erkennen, dass der Zähler der Kovarianz und der Nenner der Varianz entspricht, betrachten wir nochmal die Formeln der (Ko)Varianz:\n",
        "\n",
        "\n",
        "$$\n",
        "\\scriptsize{\n",
        "\\begin{aligned}\n",
        "Cov(X,Y) &= \\frac{1}{n}\\sum(x_i-\\bar{X})(y_i-\\bar{Y}) = \\frac{1}{n}\\sum\\left(x_iy_i-x_i\\bar{Y}-y_i\\bar{X}+\\bar{X}\\bar{Y}\\right) = \n",
        "\\frac{1}{n}\\sum x_iy_i-\n",
        "\\bar{Y}\\frac{1}{n}\\sum x_i-\n",
        "\\bar{X}\\frac{1}{n}\\sum y_i+\n",
        "\\bar{X}\\bar{Y} = \\\\\n",
        "&\\underset{\\color{darkgreen}{\\left(\\frac{1}{n}\\sum y_i=\\bar{Y}\\right)}}{\\overset{\\color{darkgreen}{\\left(\\frac{1}{n}\\sum x_i=\\bar{X}\\right)}}{=}} \\frac{1}{n}\\sum x_iy_i - \\bar{Y}\\bar{X}-\\bar{X}\\bar{Y}+\\bar{X}\\bar{Y} = \\frac{1}{n}\\sum x_iy_i - \\bar{X}\\bar{Y}\n",
        "\\end{aligned}\n",
        "}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\scriptsize{\n",
        "\\begin{aligned}\n",
        "Var(X) &= \\frac{1}{n}\\sum\\left(x_i-\\bar{X}\\right)^2 = \\frac{1}{n}\\sum\\left(x_i^2-2x_i\\bar{X}+\\bar{X}^2\\right) = \\frac{1}{n}\\sum x_i^2-2\\bar{X}\\frac{1}{n}\\sum x_i+\\bar{X}^2 = \\\\\n",
        " &\\overset{\\color{darkgreen}{\\left(\\frac{1}{n}\\sum x_i=\\bar{X}\\right)}}{=} \\frac{1}{n}\\sum x_i^2-2\\bar{X}^2+\\bar{X}^2 = \\frac{1}{n}\\sum x_i^2-\\bar{X}^2 \n",
        "\\end{aligned}\n",
        "}\n",
        "$$\n",
        "\n",
        "\n",
        "- Es gilt also tatsächlich:\n",
        "\n",
        "::: {style=\"margin-top:-20px\"}\n",
        "$$\n",
        "\\color{darkblue}{b_1 = \\frac{Cov(X,Y)}{Var(X)}}\n",
        "$$\n",
        ":::\n",
        "\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "## [Beweis, dass $R^2=r_{XY}^2$ bei einfacher Regression]{color=\"darkred\"}\n",
        "\n",
        "![](images/kein_klausurstoff.png){.absolute bottom=0 right=0 height=130px}\n",
        "\n",
        "- Ausgestattet mit der Formel für den Regressionskoeffizienten, lässt sich nun auch beweisen, dass bei der einfachen Regression gilt: $\\quad \\color{darkblue}{R^2=r_{XY}^2}$\n",
        "\n",
        "\n",
        "$$\n",
        "\\scriptsize{\n",
        "\\begin{aligned}\n",
        "R^2 = \\frac{Var(\\hat{Y})}{Var(Y)} &= \\frac{\\frac{1}{n}\\sum\\left(\\hat{y}_i-\\bar{Y}\\right)^2}{Var(Y)}\n",
        " \\underset{\\color{darkgreen}{\\left(\\bar{Y} = b_0+b_1\\bar{X}\\right)}}{\\overset{\\color{darkgreen}{\\left(\\hat{y}_i = b_0+b_1x_i\\right)}}{=}} \\frac{\\frac{1}{n}\\sum\\left[(b_0+b_1x_i)-(b_0+b_1\\bar{X})\\right]^2}{Var(Y)} = \\frac{\\frac{1}{n}\\sum\\left(b_1x_i-b_1\\bar{X}\\right)^2}{Var(Y)} = \\\\\n",
        "&= b_1^2\\frac{\\frac{1}{n}\\sum\\left(x_i-\\bar{X}\\right)^2}{Var(Y)}= b_1^2\\frac{Var(X)}{Var(Y)}\\qquad\\qquad \\left(\\text{Man sieht also, dass gilt:}\\quad Var(\\hat{Y}) = b_1^2Var(X)\\right)\n",
        "\\end{aligned}\n",
        "}\n",
        "$$\n",
        "\n",
        "- Nun $b_1 = \\frac{Cov(X,Y)}{Var(X)}$ einsetzen:\n",
        "\n",
        "$$\n",
        "\\scriptsize{\n",
        "\\color{darkblue}{R^2} = \\frac{Cov^2(X,Y)}{\\color{darkred}{Var^2(X)}}\\frac{\\color{darkred}{Var(X)}}{Var(Y)} = \\color{darkblue}{\\frac{Cov^2(X,Y)}{Var(X)Var(Y)}}\n",
        "}\n",
        "$$\n",
        "\n",
        "- Vergleiche mit $r_{XY}^2$:\n",
        "\n",
        "$$\n",
        "\\scriptsize{\n",
        "\\color{darkblue}{r_{XY}^2} = \\left[\\frac{Cov(X,Y)}{s_Xs_Y}\\right]^2 = \\frac{Cov^2(X,Y)}{s_X^2s_Y^2} = \\frac{Cov^2(X,Y)}{Var(X)Var(Y)}  \\qquad Q.E.D.\n",
        "}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "## Ausblick: Multiple Regression\n",
        "\n",
        "- Gibt es mehr als eine **unabhängige Variable** (auch **Prädiktoren** genannt), so handelt es sich nicht mehr um eine einfache Regression, sondern um eine [**multiple Regression**]{color=\"navy\"}:\n",
        "\n",
        "<div class=\"vspace-medium\"></div>\n",
        "\n",
        "![](images/ausblick_multiple_regression.png){height=\"300px\"}\n",
        "\n",
        "<div class=\"vspace-medium\"></div>\n",
        "\n",
        "- Jeder Prädiktor $X_1, X_2, ... X_n$ hat einen eigenen Regressionskoeffizienten $b_1, b_2, ... b_n$\n",
        "- Multiple Regression wird ausführlich in Statistik 2 behandelt.\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "## [\"Formula Notation\": Formalisierung von Regressionsmodellen]{style=\"font-size: 40px\"}\n",
        "\n",
        "- Da Regressionen heute auschließlich mit dem Computer berechnet werden, hat sich eine eigene Sprache etabliert, um Regressionsmodelle zu definieren (bekannt als *Formula Notation*):\n",
        "\n",
        "<div class=\"vspace-medium\"></div>\n",
        "![](images/patsy_simple.png)\n",
        "<div class=\"vspace-medium\"></div>\n",
        "\n",
        "- Der Ausdruck [\"DV &Tilde; 1 + IV1 + IV2\"]{style=\"font-family:monospace\"} kann der Statistiksoftware als *String* übergeben werden; so wird definiert, welches Regressionsmodell gerechnet werden soll.\n",
        "- [DV]{style=\"font-family:monospace\"}, [IV1]{style=\"font-family:monospace\"}, [IV2]{style=\"font-family:monospace\"} sind dabei die gewählten Variablennamen &mdash; beliebige Ausdrücke sind möglich\n",
        "\n",
        "<!---  Example --->\n",
        "::: {.example}\n",
        "|||\n",
        "|:-:|-|\n",
        "|||\n",
        "| ![](images/example.png){height=70px} | [\"satisfaction &Tilde;  1 + physical_activity\"]{style=\"font-family:monospace\"} \\\n",
        "[Dies wäre eine mögliche Definition unserer einfachen Regression mit sportlicher Aktivität als unabhängiger und Lebenszufriedenheit als abhängiger Variable.]{style=\"display:block;margin-top:12px;line-height:1.1\"} |\n",
        "|||\n",
        ": {tbl-colwidths=\"[10, 90]\"}\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "## Regression: Erklärung versus Vorhersage\n",
        "\n",
        "<!---  Table --->\n",
        "||[Erklärung]{color=\"navy\"}|[Vorhersage]{color=\"navy\"}|\n",
        "|-|:-:|:-:|\n",
        "| **Ziel** | Zusammenhänge zwischen Variablen untersuchen:\\\n",
        "- Hängen die Variablen $X$ und $Y$ zusammen? \\\n",
        "- Ist der Zusammenhang positiv oder negativ? \\\n",
        "- Wie stark ist der Zusammenhang? | Wie gut kann Variable $Y$ durch Variable $X$ vorhergesagt werden? |\n",
        "| **Interessante Größe** | Steigung $b_1$ | Bestimmungsmaß $R^2$ |\n",
        "|**Visuelle Hervorhebung der interessanten Größe**|![](images/regression_explanation.png){.hcenter-image height=200px}|![](images/regression_prediction.png){.hcenter-image height=200px}|\n",
        "| **Beispiel** | Regression von Lebenszufriedenheit auf sportliche Aktivität. Der Regressionskoeffizient sei $b_1=0{,}5$.\\\n",
        "- Der Zusammenhang ist positiv. \\\n",
        "- Eine Erhöhung von sportlicher Aktivität um den Wert 1 führt im Schnitt zu einer Erhöhung der Lebenszufriedenheit um den Wert $0{,}5$.  | Regression von Lebenszufriedenheit auf sportliche Aktivität. Das Bestimmtheitsmaß sei $R^2=0{,}4$.\\\n",
        "- Sportliche Aktivität hat eine gute Vorhersagekraft für Lebenszufriedenheit. \\\n",
        "- Sportliche Aktivität erklärt 40% der Varianz von interindividueller Lebenszufriedenheit. |\n",
        "\n",
        ": {tbl-colwidths=\"[20, 40, 40]\"}\n",
        "\n",
        "\n",
        "<!----------------->\n",
        "<!--- New slide --->\n",
        "<!----------------->\n",
        "##\n",
        ":::: {.columns}\n",
        "::: {.column width=\"9%\"}\n",
        "::: {style=\"margin-top:-15px\"}\n",
        "![](images/summary.png)\n",
        ":::\n",
        ":::\n",
        "::: {.column width=\"91%\"}\n",
        "::: {.summary style=\"font-size: 28px\"}\n",
        "- Die lineare Regression erweitert die Korrelation zu einer [**Vorhersageanalyse**]{color=\"navy\"}: wenn Variablen korrelieren, lässt sich eine Variable aus der anderen vorhersagen.\n",
        "- Die Vorhersage basiert auf einer [**Regressionsgerade**]{color=\"navy\"}, die alle Datenpunkte so gut wie möglich repräsentiert.\n",
        "- Die Regressionsgerade wird durch den [**Achsenabschnitt**]{color=\"navy\"} $b_0$ und die [**Steigung**]{color=\"navy\"} $b_1$  beschrieben.\n",
        "- Die standardisierte Form des Steigungs-Koeffizienten wird [**Beta**]{color=\"navy\"} oder [**Beta-Gewicht**]{color=\"navy\"} genannt und ist identisch dem Korrelationskoeffizienzen (bei einfacher Regression).\n",
        "- Das [**Bestimmtheitsmaß**]{color=\"navy\"} $R^2$ bemisst die Vorhersagegenauigkeit der Regression.\n",
        "- Die Regression kann sowohl der [**Vorhersage**]{color=\"navy\"} einer Variable $Y$ auf Basis einer Variable $X$ dienen, als auch der [**Erklärung**]{color=\"navy\"} bzw. Beschreibung eines Zusammehangs von $X$ und $Y$.\n",
        ":::\n",
        ":::\n",
        "::::\n",
        "\n",
        ":::\n",
        ":::\n",
        "::::\n",
        "\n",
        "\n",
        "<!---"
      ],
      "id": "142a6ab0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``` \n",
        "--->\n",
        "<!---"
      ],
      "id": "a436ef80"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from scipy.stats import linregress, pearsonr\n",
        "\n",
        "np.random.seed(0)\n",
        "root = '/home/matteo/OneDrive/lehre/Statistik/stats1_lecture/book/'\n",
        "df = pd.read_csv(os.path.join(root, 'data', 'paradoxia.csv'))\n",
        "data_tiktok_paradoxia = df[df.group == 2]['hours_tiktok_per_day'].values\n",
        "data_inflam_paradoxia = df[df.group == 2]['inflammation'].values\n",
        "\n",
        "r, p = pearsonr(data_tiktok_paradoxia, data_inflam_paradoxia)\n",
        "\n",
        "fontsize = 15\n",
        "\n",
        "plt.style.use('dark_background')\n",
        "plt.figure(figsize=(6, 2.8))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.scatter(data_tiktok_paradoxia, data_inflam_paradoxia)\n",
        "linrg = linregress(data_tiktok_paradoxia, data_inflam_paradoxia)\n",
        "plt.plot([0.1, 3.1], linrg.intercept + linrg.slope * np.array([0.1, 3.1]), color='#ff00ff', lw=2)\n",
        "plt.text(0.2, 0.3075, r'$\\bf{slope=' + f'{linrg.slope:.3f}' + r'}$', color='#ff00ff', fontsize=fontsize-2)\n",
        "plt.xticks(fontsize=fontsize-2)\n",
        "plt.yticks(fontsize=fontsize-2)\n",
        "plt.xlabel('Stunden TikTok / 24h', fontsize=fontsize)\n",
        "plt.ylabel('Entzündungswerte', fontsize=fontsize)\n",
        "plt.xlim(0, 3.2)\n",
        "plt.ylim(0, 0.34)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.scatter(data_inflam_paradoxia, data_tiktok_paradoxia)\n",
        "linrg = linregress(data_inflam_paradoxia, data_tiktok_paradoxia)\n",
        "plt.plot([0.01, 0.31], linrg.intercept + linrg.slope * np.array([0.01, 0.31]), color='#ff00ff', lw=2)\n",
        "plt.text(0.015, 3.15, r'$\\bf{slope=' + f'{linrg.slope:.2f}' + r'}$', color='#ff00ff', fontsize=fontsize-2)\n",
        "plt.xticks(fontsize=fontsize-2)\n",
        "plt.yticks(fontsize=fontsize-2)\n",
        "plt.xlabel('Entzündungswerte', fontsize=fontsize)\n",
        "plt.ylabel('Stunden TikTok / 24h', fontsize=fontsize)\n",
        "plt.xlim(0, 0.34)\n",
        "plt.ylim(0, 3.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig('images/paradoxia_histogram_slope_paradoxiker.png', bbox_inches='tight')\n",
        "``` \n",
        "--->\n",
        "\n",
        "## {.blackslide .center}\n",
        "\n",
        "<div class=\"vspace-medium\"></div>\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"65%\"}\n",
        "Sie führen nun eine Regressionsanalyse bezüglich des Zusammenhangs von TikTok-Online-Zeit und Entzündungswerten durch. Einmal mit TikTok-Online-Zeit und einmal mit Entzündungswerten als unabhängiger Variable:\n",
        ":::\n",
        "::: {.column width=\"35%\"}\n",
        "::: {.content-hidden when-format=\"pdf\"}\n",
        "![](images/paradoxia_group_of_scientists.png){.hcenter-image height=200px style=\"margin-top:-100px !important\"}\n",
        "<!-- Source: Midjourney -->\n",
        ":::\n",
        ":::\n",
        "::::\n",
        "\n",
        "<div class=\"vspace-large\"></div>\n",
        "![](images/paradoxia_histogram_slope__paradoxiker.png){height=300px}\n",
        "\n",
        "Faszinierend. Es gibt tatsächlich einen Zusammenhang beider Variablen. Kann das Ergebnis so interpretiert werden, dass TikTok sich auf physiologische Entzündungswerte auswirkt?\n"
      ],
      "id": "8dd06c87"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}