
<!----------------->
<!--- New slide --->
<!----------------->
## Einstichprobentest versus Zweistichprobentest

Statistische Tests, die sich auf eine Stichprobe beziehen (Einzelmessung oder Vergleich zweier abhängiger Messungen) werden auch als [**Einstichprobentest**]{color="navy"} bezeichnet.

:::{.fragment}
Beispielhypothesen für den Einstichprobentest:
:::

- Einzelmessung: Psychologiestudierende sind überdurchschnittlich intelligent &numsp;($\overline{IQ} > 100$)
    - Nullhypothese: $\overline{IQ} = 100$
    - Beachte: $\mu_0$ ist in diesem Fall 100 und nicht 0!
- Vergleich zweier abhängiger Messungen: Psychologiestudierende sind morgens intelligenter als abends &numsp;($\overline{IQ}_\text{Morgen} > \overline{IQ}_\text{Abend}$)
    - Nullhypothese: $\overline{IQ}_\text{Morgen} - \overline{IQ}_\text{Abend} = \Delta \overline{IQ} = 0$

:::{.fragment}
Demgegenüber werden statistische Tests, die sich auf zwei unabhängige Stichproben beziehen, als [**Zweistichprobentest**]{color="navy"} bezeichnet.
:::

:::{.fragment}
Beispielhypothese für den Zweistichprobentest:
:::

- Studierende der HMU sind intelligenter als Studierende der MSB &numsp;($\overline{IQ}_\text{HMU} > \overline{IQ}_\text{MSB}$)
    - Nullhypothese: $\overline{IQ}_\text{HMU} - \overline{IQ}_\text{MSB} = \Delta \overline{IQ} = 0$




<!----------------->
<!--- New slide --->
<!----------------->
## Signifikanzniveau ($p < \alpha$ ?)

Bislang haben wir die Berechnung des p-Wertes besprochen, aber nicht, wie klein der p-Wert sein sollte, um die Nullhypothese abzulehnen (und im Umkehrschluss den Effekt signifikant zu werten).

:::{.fragment}
Um diese Entscheidung zu treffen, legen wir ein [**Signifikanzniveau**]{color="navy"} $\alpha$ fest. **Unterschreitet der p-Wert das Signifikanzniveau $\alpha$**, **lehnen wir die Nullhypothese ab** und werten den **Effekt als statistisch signifikant**.
:::

:::{.fragment}
Der Wert $\alpha$ wird auch als [**Irrtumswahrscheinlichkeit**]{color="navy"} bezeichnet. Ist beispielsweise $\alpha=0{,}1$ so wären wir bereit einen p-Wert von $p<0.1$ als signifikant zu werten, gehen dabei aber ein 10%-iges Risiko ein, dass die Nullhypothese in Wahrheit doch korrekt ist. D.h. wir nehmen in Kauf, dass wir uns mit 10% Wahrscheinlichkeit irren und fälschlicherweise die Nullhypothese ablehnen.
:::

:::: {.columns .fragment}
::: {.column width="50%"}

Auf welchen Wert das Signifikanzniveau festgelegt werden sollte ist seit langer Zeit Gegenstand von kontroversen Debatten in der Psychologie. Als de facto Standard-Signifikanzniveau hat sich jedoch der [**Wert $\color{navy}{\alpha=0{,}05}$**]{color="navy"} eingebürgert.


:::
::: {.column width="50%"}
![](images/significance_level.png){style="margin-top:0px !important"}
:::
::::


<!----------------->
<!--- New slide --->
<!----------------->
## Ein- und zweiseitiges Testen

- Im Nasenlängenbeispiel haben wir bislang eine [**gerichtete Hypothese betrachtet**]{color="navy"}, indem wir bei der Hypothese festgelegt haben, welche der beiden Gruppen durchschnittlich eine längere Nase hat (Med-Nasenlänge > Psych-Nasenlänge oder $\Delta\bar{x}>0$).
- Wir hätten auch die **umgekehrte gerichtete Hypothese** betrachten können:<br>Med-Nasenlänge < Psych-Nasenlänge oder $\Delta\bar{x}<0$
- Eine [**ungerichtete Hypothese**]{color="navy"} gibt dagegen keine Richtung vor, d.h. die Hypothese würde schlicht lauten: Med-Nasenlängen und Psych-Nasenlängen sind **unterschiedlich** ($\Delta\bar{x}\ne 0$)


:::: {.columns .fragment}
::: {.column width="55%"}
Dies wirkt sich auf die Flächen unter der Nullverteilung aus, die wir betrachten.
:::
::: {.column width="45%"}

![](images/nulldist2g.png)

:::
::::




<!----------------->
<!--- New slide --->
<!----------------->
## Ein- und zweiseitiges Testen

<div class="vspace-small"></div>

![](images/nulldist2g.png){height=330px}

Wie sehen folgende Zusammenhänge:

:::{.nonincremental}
- Die **p-Werte der beiden gerichteten Hypothesen** ("größer als" oder "kleiner als") sind genau **invers**:
:::

:::{style="margin-top: -40px"}
$$
p(\bar{x}_\text{med} > \bar{x}_\text{psych}) = 1 - p(\bar{x}_\text{med} < \bar{x}_\text{psych})
$$
:::

:::{.nonincremental}
- Der **p-Wert der ungerichteten Hypothese** ("unterschiedlich") ist gleich **2 x der kleinere p-Wert der beiden gerichteten Hypothesen**:
:::

:::{style="margin-top: -20px"}
$$
p(\bar{x}_\text{med} \neq \bar{x}_\text{psych}) = 2 \times \text{min}\left(p(\bar{x}_\text{med} > \bar{x}_\text{psych}), p(\bar{x}_\text{med} < \bar{x}_\text{psych})\right)
$$

:::
<!----------------->
<!--- New slide --->
<!----------------->
## Ein- und zweiseitiges Testen

<div class="vspace-small"></div>

![](images/nulldist2g.png){height=310px}

Der p-Wert der ungerichteten Hypothese ist damit immer größer (doppelt so groß) wie der kleinere p-Wert der gerichteten Hypothesen.

**Intuition:** bei der ungericheteten Hypothese legen wir uns weniger fest, denn unsere Hypothese wäre sowohl erfüllt wenn $\bar{x}_\text{med}$ signifikant **größer** ist als $\bar{x}_\text{psych}$, als auch wenn $\bar{x}_\text{med}$ signifikant **kleiner** ist als $\bar{x}_\text{psych}$. Wir machen uns das Leben (bzw. unsere Vorhesagen) also "einfacher". 

Genauer gesagt verdoppeln wir unsere Chance, mit der Hypothese richtig zu liegen, da a priori beide ungerichtete Hypothesen gleich wahrscheinlich sind. Die Verdopplung des p-Wertes kompensiert dies (man könnte auch sagen "bestraft unsere schwammige Vorhersage") &mdash; nun wird es schwieriger für p das Signifikanzniveau zu unterschreiten.



<!----------------->
<!--- New slide --->
<!----------------->
## Ein- und zweiseitiges Testen

Es ist zusätzlich gewinnbringend, sich den Vergleich von gerichteten und ungerichteten Hypothesen aus der Perspektive des Signifikanzniveaus $\alpha$ zu betrachten:

![](images/significance_level2.png){height=420px}

Beim zweiseitigen Test verteilt sich die Irrtumswahrscheinlichkeit (im Bild 5%) *auf beide Flanken* der Nullverteilung. Es wird damit auf beiden Seiten schwieriger für unseren Effekt einen noch extremeren Wert aufzuweisen, als durch die Irrtumswahrscheinlichkeit vorgegeben.



<!----------------->
<!--- New slide --->
<!----------------->
## p-Wert versus Stichprobengröße

<div class="vspace-small"></div>

![Getestet wird, ob der IQ der betrachteten Population (hier $102{,}5$) größer ist, als der Referenzwert $\mu_0=100$, der idealerweise den durchschnittlichen IQ aller Menschen darstellt. Im Bild ist zu sehen, dass die betrachtete Population (z.B. alle Brandenburger:innen) tatsächlich einen etwas höheren Mittelwert als 100 aufweisen, was sich auch im Stichprobenmittelwert niederschlägt. **Wie verändert sich der p-Wert abhängig davon, ob die Stichprobe $n=10$ oder $n=100$ umfasst?** **Schritt 1:** der Standardfehler $se$ ist bei der höheren Stichprobenzahl wesentlich kleiner (aufgrund von $\hat{\sigma}/\sqrt{n}$) und damit die Stichprobenverteilung bei $n=100$ wesentlich schmaler. **Schritt 2:** der kleinere Standardfehler wirkt sich auf den z-Wert $z=\frac{102{,}5}{se}$ aus: kleinerer Standardfehler bedeutet größerer z-Wert. **Schritt 3:** größerer z-Wert bedeutet kleinerer p-Wert, da kleinere Fläche rechts von z.](images/pwert_fallzahl_annot.png){height=500px}

<!----------------->
<!--- New slide --->
<!----------------->
## p-Wert versus Stichprobengröße/Populationsstreuung

<div class="vspace-medium"></div>

**Gibt es den untersuchten Effekt in der Population tatsächlich (Alternativhypothese wahr), gilt:**

- Größere Stichprobengrößen führen im Schnitt zu kleineren p-Werten.
- Kleinere Populationsstreuungen führen zu kleineren p-Werten.

:::{.fragment}
**Gibt es den untersuchten Effekt in der Population nicht (Nullhypothese wahr), gilt:**
:::

- Größere Stichprobengrößen haben keine Auswirkung auf den p-Wert.
- Kleinere Populationsstreuungen haben keine Auswirkung auf den p-Wert.
- Alle p-Werte sind gleich wahrscheinlich (d.h. p-Werte haben eine uniforme Verteilung auf dem Intervall [0; 1])


